<?xml version="1.0"?>
<kb_documents>
<kb_document>
<kb_title>Bar Graphs in Stata</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p>Bar graphs are simple but powerful (or rather, powerful because they are simple) tools for conveying information. They can be understood at a glance by both technical and non-technical audiences, and often tell you much more than summary statistics will. This article will show you how to make a variety of useful bar graphs using Stata.</p>
<h2>The Example Data Set</h2>
<p>SSCC's statistical consultants have been asked to analyze several workplace surveys in recent years, so the example data we'll use has that theme (much of this article came out of our efforts to find ways to present our results to very busy leaders). You can obtain the data by typing, or more likely copying and pasting, the following in a do file:</p>
<p class="InputCode">use http://ssc.wisc.edu/sscc/pubs/bargraphs/bar_example.dta</p>
<p>It contains fictional data with 1,000 observations and four variables:</p>
<ul>
<li><span class="InputCode">sat</span>: responses to the question "In general, how satisfied are you with your job?" on a five-point scale ranging from "Very Dissatisfied" to "Very Satisfied."</li>
<li><span class="InputCode">eng</span>: a numeric measure of employee engagement from 1 to 100.</li>
<li><span class="InputCode">leave</span>: responses to the question "How likely are you to leave your job in the next year?" on a five-point scale ranging from "Very Likely" to "Very Unlikely." </li>
<li><span class="InputCode">stay</span>: a binary variable based on <span class="InputCode">leave</span>. It is 0 if the respondent said they were likely to leave and 1 otherwise.</li>
<li><span class="InputCode">female</span>: a binary variable which is 1 if the respondent is female and 0 if the respondent is male.                </li>
</ul>
<h2>Distribution of a Single Variable</h2>
<p>The most basic task of a bar graph is to help you understand the distribution of a single categorical variable. Begin with the <span class="InputCode">sat</span> variable (job satisfaction) and the most basic bar graph:</p>
<p class="InputCode">graph bar, over(sat)</p>
<p>                The <span class="InputCode">graph bar</span> command tell Stata you want to make a bar graph, and the <span class="InputCode">over()</span> option tells it which variable defines the categories to be described. By default it will tell you the percentage of observations that fall in each category. Unfortunately, the result is not very satisfactory:</p>
<img alt="Basic bar graph" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b1.png" width="550"/>
<p>The categories are labeled using the value labels of the <span class="InputCode">sat</span> variable, but they're unreadable because they overlap. You can fix this problem easily and naturally by making the whole graph horizontal rather than vertical. Just change <span class="InputCode">graph bar</span> to <span class="InputCode">graph hbar</span>.</p>
<p>The y axis title "percent" is vague. Make it more clear with a <span class="InputCode">ytitle()</span> option. Note that this axis will be horizontal since you're now making a horizontal graph, but it's still referred to as the y axis.</p>
<p>This graph is also in dire need of an overall title, which can be added using the <span class="InputCode">title()</span> option. For graphs describing surveys, the question text is often a useful title. The title text doesn't always need to go in quotes, but this one does because it contains a comma. Without quotes, Stata will think you're trying to set title options.</p>
<p>Stata graph commands often get long;  you can make them more readable by splitting them across multiple lines if you use <span class="InputCode">///</span> to tell Stata the command continues on the next line. For this article, we'll put just one option per line, though some options will soon take more than one line. We'll also bold the new or changed parts of each command.</p>
<p class="InputCode">graph <span class="BoldCode">hbar</span>, ///<br/>
                  over(sat) ///<br/>
<span class="BoldCode">ytitle("Percent of Respondents")</span> ///<br/>
<span class="BoldCode">title("In general, how satisfied are you with your job?")</span><br/>
</p>
<img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b2.png" width="550"/>
<p>Now the problem is that the text doesn't fit in the graph. You can do several things to fix that:</p>
<ul>
<li>Reduce the size of the category labels using the <span class="InputCode">label(labsize(small))</span> option</li>
<li>Reduce the size of the y axis title using the <span class="InputCode">size(small)</span> option</li>
<li>Allow the title to use the space above the axis labels (and be centered across the entire space)  using the <span class="InputCode">span</span> option</li>
<li>Reduce the size of the title using the <span class="InputCode">size(medium)</span> option</li>
</ul>
<p>You could also split the title into multiple lines by putting each line in its own set of quotes, but that won't be necessary here.</p>
<p>Each of the new options goes inside the option for the thing it controls. They are options for options!</p>
<p class="InputCode">graph hbar, ///<br/>
                  over(sat<span class="BoldCode">, label(labsize(small))</span>) ///<br/>
                  ytitle("Percent of Respondents"<span class="BoldCode">, size(small)</span>) ///<br/>
                  title("In general, how satisfied are you with your job?" ///<br/>
<span class="BoldCode">, span size(medium)) </span><br/>
</p>
<img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b3.png" width="550"/>
<p>We're getting much closer, but the label "Neither Satisfied nor Dissatisfied" is still being truncated. One good solution would be to  use a shorter label. On the other hand, an elegant command called <span class="InputCode">splitvallabels</span> by Nick Winter and Ben Jann will take value labels, split them into multiple lines, and make them available as an <span class="InputCode">`r(relabel)'</span> macro in a form the <span class="InputCode">relabel()</span> option can understand. (The <span class="InputCode">relabel()</span> option allows you to set category labels to whatever you want without setting value labels for the variable, but using value labels is a good practice for many reasons.) You can get <span class="InputCode">splitvallabels</span>   by running:</p>
<p class="InputCode">ssc install splitvallabels</p>
<p>You only need to run this once—don't put it in a research do file that you'll run over and over.</p>
<p class="InputCode"> <span class="BoldCode">splitvallabels sat</span><br/>
                  graph hbar, ///<br/>
                  over(sat, label(labsize(small)) <span class="BoldCode">relabel(`r(relabel)')</span>) ///<br/>
                  ytitle("Percent of Respondents", size(small)) ///<br/>
                  title("In general, how satisfied are you with your job?" ///<br/>
                  , span size(medium))<br/>
</p>
<p>If you are new to macros, note that the character at the left of <span class="InputCode">`r(relabel)'</span> is the left single quote, found on the left side of your keyboard under the tilde, and the character at the right of it is the right single quote, found on the right side of your keyboard under the double quote.</p>
<p></p>
<img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b4.png" width="550"/>
<p>This  fixes the truncated label and reduces the amount of space taken up by the labels in general, leaving more space for the graph.</p>
<p>Because the <span class="InputCode">splitvallabels</span> command puts its results in the r() vector, they'll be replaced by any other command that stores results in the r() vector, including <span class="InputCode">graph</span>. If you'll be using a set of labels repeatedly, you could store them in a separate macro:</p>
<p class="InputCode">local relabel `r(relabel)'</p>
<p></p>
<p>and then use <span class="InputCode">`relabel'</span> in subsequent commands rather than <span class="InputCode">`r(relabel)'</span>. For this article we'll instead run the <span class="InputCode">splitvallabels</span> command before each graph, so each graph can be run separately.</p>
<p>This is now a usable graph, but some might complain that it does not have the precision of a table giving the percentages as numbers. No problem: you can have the numbers too by adding a <span class="InputCode">blabel(bar)</span> option, meaning Stata should label each bar with the height of the bar. You'll almost certainly want to control the number of decimal places displayed with a <span class="InputCode">format()</span> option like <span class="InputCode">format(%4.1f)</span>. This means the number on each bar should take up four total spaces, including the decimal point, with one number after the decimal point.</p>
<p class="InputCode">splitvallabels sat<br/>
                  graph hbar, ///<br/>
                  over(sat, label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("Percent of Respondents", size(small)) ///<br/>
                  title("In general, how satisfied are you with your job?" ///<br/>
                  , span size(medium)) ///<br/>
<span class="BoldCode">blabel(bar, format(%4.1f))</span></p>
<p><img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b5.png" width="550"/></p>
<p>One final tweak: if someone prints this graph, the bars will use a lot of toner and, depending on the printer, the ink may streak. You can avoid that by reducing the "intensity" of the colors with the <span class="InputCode">intensity()</span> option:</p>
<p class="InputCode">splitvallabels sat<br/>
                  graph hbar, ///<br/>
                  over(sat, label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("Percent of Respondents", size(small)) ///<br/>
                  title("In general, how satisfied are you with your job?" ///<br/>
                  , span size(medium)) ///<br/>
                  blabel(bar, format(%4.1f)) ///<br/>
<span class="BoldCode">intensity(25)</span></p>
<p><img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b6.png" width="550"/></p>
<p>If you want frequencies rather than percentages, tell <span class="InputCode">graph hbar</span> that the thing you want to plot is the <span class="InputCode">(count)</span>. You'll also want to change the <span class="InputCode">ytitle()</span> and the format of the bar labels—with integers the default will do so you can just remove the <span class="InputCode">format()</span> option entirely.</p>
<p class="InputCode">splitvallabels sat<br/>
                  graph hbar <span class="BoldCode">(count)</span>, ///<br/>
                  over(sat, label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle(<span class="BoldCode">"Number of Respondents</span>", size(small)) ///<br/>
                  title("In general, how satisfied are you with your job?" ///<br/>
                  , span size(medium)) ///<br/>
<span class="BoldCode">blabel(bar)</span> ///<br/>
                intensity(25)</p>
<p><img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b7.png" width="550"/></p>
<div style="border: medium ridge; margin: 10px 50px 15px 100px">
<p style="margin-left: 35px">Tip for advanced users: When working with survey data, you may find it useful to put the text for each question in a global macro with the same name as the variable:</p>
<p class="InputCode" style="margin-left: 35px">global sat "In general, how satisfied are you with your job?"</p>
<p style="margin-left: 35px"> Then your graph commands can start with:</p>
<p class="InputCode" style="margin-left: 35px">graph hbar, over(sat) title($sat)</p>
<p style="margin-left: 35px">This is highly convenient in loops:</p>
<p class="InputCode" style="margin-left: 35px">foreach question of varlist q1-q10 {<br/>
<span class="indent3">graph hbar, over(`question') title($`question')</span><br/>
                  } </p>
<p style="margin-left: 35px">The macro processor will first replace the local macro <span class="InputCode">`question'</span> with a specific question, and then replace the resulting global macro with that question's text.</p>
<p style="margin-left: 35px">If you create a do file that defines global macros for all your questions, you can just put:</p>
<p class="InputCode" style="margin-left: 35px">include question_file.do</p>
<p style="margin-left: 35px">early in any do file that will use them and they'll be ready to go.</p>
</div>
<h2>Bar Graphs vs. Means</h2>
<p>We often see people use means to summarize Lickert scales and other ordered categorical variables like <span class="InputCode">sat</span>. Means  convey useful information at a glance, but they also hide a lot. They can also hide strong  implicit assumptions. For example, having one person change from "Neither Satisfied nor Dissatisfied" to "Somewhat Satisfied" will have exactly the same effect on the mean as someone moving from "Very Dissatisfied" to "Somewhat Dissatisfied," which may or may not be equivalent in any meaningful sense.<img alt="" class="CenterImage" height="250" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/mean.png" width="550"/></p>
<p>A bar graph can be taken in at a glance just like a mean, but conveys far more information. Once you're comfortable making and using bar graphs, they're almost as easy to add to a document as a mean.</p>
<h2>Relationships between a Categorical Variable and a Quantitative Variable</h2>
<p>Bar graphs are also good tools for examining the relationship (joint distribution) of a categorical variable and some other variable. </p>
<p>To create a bar graph where the length of the bar tells you the mean value of a quantitative variable for each category, just tell <span class="InputCode">graph hbar</span> to plot that variable. If you want a different summary statistic, like the median, put that summary statistic in parentheses before the variable name just like you did with <span class="InputCode">(count)</span>.</p>
<p>You'll also need to change the title and y axis title, and set the formatting of the bar labels.</p>
<p class="InputCode">splitvallabels sat<br/>
                  graph hbar <span class="BoldCode">eng</span>, ///<br/>
                  over(sat, label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("<span class="BoldCode">Mean Engagement</span>", size(small)) ///<br/>
                  title("<span class="BoldCode">Mean Engagement by Job Satisfaction</span>" ///<br/>
                  , span size(medium)) ///<br/>
                  blabel(bar<span class="BoldCode">, format(%4.1f)</span>) ///<br/>
                intensity(25)</p>
<p><img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b8.png" width="550"/></p>
<p>This  suggests a strong but nonlinear relationship.</p>
<p>In this plot, the length of the bar measures how far away from zero the mean is. Sometimes that isn't a meaningful measure: for example, a Lickert scale can never be zero. In those cases you might consider using a dot plot instead—just replace <span class="InputCode">hbar</span> with <span class="InputCode">dot</span>:</p>
<p class="InputCode">splitvallabels sat<br/>
                  graph <span class="BoldCode">dot</span> eng, ///<br/>
                  over(sat, label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("Mean Engagement", size(small)) ///<br/>
                  title("Mean Engagement by Job Satisfaction" ///<br/>
                  , span size(medium)) ///<br/>
                  blabel(bar, format(%4.1f)) ///<br/>
                intensity(25)</p>
<p><img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b8b.png" width="550"/></p>
<p>The trouble with this graph is that the dots interfere with the bar labels. You can fix that by putting a white box around each label which will cover up the dots. This is done with the <span class="InputCode">box</span> option and both <span class="InputCode">fcolor(white)</span> and <span class="InputCode">lcolor(white)</span> to set the "fill" (inside) of the box and the line around it to white. The last label (85.8) will look a little funny if its white box covers up the light blue margin around the graph, so increase the range of the y axis to 90 with <span class="InputCode">yscale(range(0 90))</span> so it no longer crosses over into the margin.</p>
<p class="InputCode">splitvallabels sat<br/>
                  graph dot eng, ///<br/>
                  over(sat, label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("Mean Engagement", size(small)) ///<br/>
                  title("Mean Engagement by Job Satisfaction" ///<br/>
                  , span size(medium)) ///<br/>
                  blabel(bar, format(%4.1f) <span class="BoldCode">box fcolor(white) lcolor(white)</span>) ///<br/>
                  intensity(25) ///<br/>
<span class="BoldCode">yscale(range(0 90))</span></p>
<p><img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b8c.png" width="550"/></p>
<div style="border: medium ridge; margin: 10px 50px 15px 100px">
<p style="margin-left: 35px">Tip for advanced users: If you'd like to have the frequencies for each category in the graph too, put them in the value labels! The following code loops over the values of <span class="InputCode">sat</span>, stores the label associated with each value, counts how many observations have that value, constructs a new label containing both the old label and the number of observations, then applies it:</p>
<p class="InputCode" style="margin-left: 35px">levelsof sat, local(vals)<br/>
                  foreach val of local vals {<br/>
<span class="indent3">local oldlabel: label (sat) `val'</span><br/>
<span class="indent3">count if sat==`val'</span><br/>
<span class="indent3">label define newsat `val' "`oldlabel' (N=`r(N)')", add</span><br/>
}<br/>
label values sat newsat                </p>
<p style="margin-left: 35px">This relies on the macro extended function <span class="InputCode">label</span>, which allows you to access value labels in various ways and store them in macros.</p>
<img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b9.png" width="550"/>
<p style="margin-left: 35px">Just remember to change the labels back to the original when you're done:</p>
<p class="InputCode" style="margin-left: 35px">label values sat sat</p>
</div>
<h2>Relationships between a Categorical Variable and a Binary Variable</h2>
<p>If you're thinking of the binary variable as an outcome, then the proportion of "successes" (whatever that means in your data set) in each group may be of interest. But, assuming the binary variable is coded such that 1 means "success" and 0 means "failure,"  the proportion of successes is just the mean of the binary variable and can be plotted just like the mean of <span class="InputCode">eng</span>:</p>
<p class="InputCode">splitvallabels sat<br/>
                  graph <span class="BoldCode">hbar</span> <span class="BoldCode">stay</span>, ///<br/>
                  over(sat, label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("<span class="BoldCode">Proportion Staying</span>", size(small)) ///<br/>
                  title("<span class="BoldCode">Proportion  Unlikely to Leave by Job Satisfaction</span>" ///<br/>
                  , span size(medium)) ///<br/>
                  blabel(bar, format(%4.2f)) ///<br/>
                intensity(25)</p>
<p><img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b10.png" width="550"/></p>
<p>If the binary variable denotes two groups you're comparing, like <span class="InputCode">female</span>, then you should consider frequencies <span class="InputCode">(count)</span> or percentages (the default) for each combination of the two variables. Start with frequencies.</p>
<p>The binary variable to examine will be specified in another <span class="InputCode">over()</span> option, but it makes a big difference which variable you put first. If you put first <span class="InputCode">sat</span> and then <span class="InputCode">female</span>, you'll get:</p>
<p class="InputCode">splitvallabels sat<br/>
                  graph hbar <span class="BoldCode">(count)</span>, ///<br/>
                  over(sat, label(labsize(small)) relabel(`r(relabel)')) ///<br/>
<span class="BoldCode">over(female,label(labsize(small)))</span> ///<br/>
                  ytitle("<span class="BoldCode">Number of Respondents</span>", size(small)) ///<br/>
                  title("<span class="BoldCode">Job Satisfaction by Gender</span>" ///<br/>
                  , span size(medium)) ///<br/>
<span class="BoldCode">blabel(bar)</span> ///<br/>
                intensity(25)</p>
<img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b11.png" width="550"/>
<p>If you put <span class="InputCode">female</span> first and then <span class="InputCode">sat</span>, you'll get:</p>
<blockquote>
<p class="InputCode">splitvallabels sat<br/>
                    graph hbar (count), ///<br/>
<span class="BoldCode">over(female,label(labsize(small)))</span> ///<br/>
                    over(sat, label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                    ytitle("Number of Respondents", size(small)) ///<br/>
                    title("Job Satisfaction by Gender" ///<br/>
                    , span size(medium)) ///<br/>
                    blabel(bar) ///<br/>
                  intensity(25)</p>
</blockquote>
<img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b12.png" width="550"/>
<p>The latter form generally makes for easier comparisons. But in this case the only thing you can easily learn by comparing them is that more females answered the survey than males. It would be much more useful to look at what percentage of males and females are in each satisfaction category. Unfortunately, just telling <span class="InputCode">graph hbar</span> to plot percentages doesn't do that:</p>
<p class="InputCode">splitvallabels sat<br/>
                  graph hbar, /// <span class="BoldCode">percent is the default</span><br/>
                  over(female,label(labsize(small))) ///<br/>
                  over(sat, label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("<span class="BoldCode">Percent of Respondents</span>", size(small)) ///<br/>
                  title("Job Satisfaction by Gender" ///<br/>
                  , span size(medium)) ///<br/>
                  blabel(bar, <span class="BoldCode">format(%4.1f)</span>) ///<br/>
                intensity(25)</p>
<p><img alt="" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b13.png" width="550"/></p>
<p>This gives you the percentages calculated across all respondents, not calculated separately for males and females. The <span class="InputCode">graph hbar</span> command does not allow you to control how the percentages are calculated.</p>
<p>Enter the very useful <span class="InputCode">catplot</span>, by Nick Cox. Get it with:</p>
<p class="InputCode">ssc install catplot</p>
<p>The <span class="InputCode">catplot</span> command is a "wrapper" for <span class="InputCode">graph hbar</span> so most of what we've done carries over directly. What it adds (among other things) is a <span class="InputCode">percent()</span> option that allows you to specify what groups percentages will be calculated over, in this case <span class="InputCode">percent(female)</span>. </p>
<p>The <span class="InputCode">catplot</span> does some things differently than <span class="InputCode">graph hbar</span>:</p>
<ul>
<li>The variables inside the <span class="InputCode">over()</span> options are moved to a variable list directly after the command itself</li>
<li>The options inside the <span class="InputCode">over()</span> options are moved into options called <span class="InputCode">var1opts()</span>, <span class="InputCode">var2opts()</span>, etc. corresponding to the variable order in the variable list</li>
</ul>
<p>Thus the catplot version of the last command becomes:</p>
<p class="InputCode">splitvallabels sat<br/>
<span class="BoldCode">catplot female sat</span>, ///<br/>
<span class="BoldCode">percent(female)</span> ///<br/>
<span class="BoldCode">var1opts</span>(label(labsize(small))) ///<br/>
<span class="BoldCode">var2opts</span>(label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("<span class="BoldCode">Percent of Respondents by Gender</span>", size(small)) ///<br/>
                  title("Job Satisfaction by Gender" ///<br/>
                  , span size(medium)) ///<br/>
                  blabel(bar, format(%4.1f)) ///<br/>
                intensity(25)</p>
<img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b14.png" width="550"/>
<p>This allows us to see that the the relationship between <span class="InputCode">sat</span> and <span class="InputCode">female</span> is complex in this (fictional) data set, with females more likely to be both very satisfied and very dissatisfied.</p>
<p>An alternative form of this graph uses color to distinguish between the groups, adding a legend to define their meanings. You can get this form by adding the <span class="InputCode">asyvars</span> option.</p>
<p class="InputCode">splitvallabels sat<br/>
                  catplot female sat, ///<br/>
                  percent(female) ///<br/>
                  var1opts(label(labsize(small))) ///<br/>
                  var2opts(label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("Percent of Respondents by Gender", size(small)) ///<br/>
                  title("Job Satisfaction by Gender" ///<br/>
                  , span size(medium)) ///<br/>
                  blabel(bar, format(%4.1f)) ///<br/>
                  intensity(25) ///<br/>
<span class="BoldCode">asyvars</span></p>
<img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b15.png" width="550"/>
<p>This greatly reduces the clutter on the left of the graph, at the cost of adding some to the bottom and forcing the reader to look in two places to understand what the bars mean. You should consider what the graph will look like to someone who is colorblind, and may need to think about whether the bars will be distinguishable if printed on a black and white printer (the graphs in this article fail that test). For this particular graph you may want to change the default colors  to avoid unfortunate associations—you'll soon learn how.</p>
<h2>Relationships between Two Categorical Variables</h2>
<p>The code for creating graphs that compare two categorical variables is identical to comparing one categorical variable and one binary, but the result has a lot more bars. The bar labels will overlap unless you shrink them with a <span class="InputCode">size(vsmall)</span> option.</p>
<p class="InputCode">splitvallabels sat<br/>
                  catplot <span class="BoldCode">leave</span> sat, ///<br/>
                  percent(<span class="BoldCode">sat</span>) ///<br/>
                  var1opts(label(labsize(small))) ///<br/>
                  var2opts(label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("<span class="BoldCode">Percent of Respondents by Satisfaction</span>", size(small)) ///<br/>
                  title("<span class="BoldCode">Probability of Leaving by Job Satisfaction</span>" ///<br/>
                  , span size(medium)) ///<br/>
                  blabel(bar, format(%4.1f) <span class="BoldCode">size(vsmall)</span>) ///<br/>
                  intensity(25) ///<br/>
                asyvars</p>
<img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b16.png" width="550"/>
<p>If you stare at this for a bit you'll see that higher levels of satisfaction are associated with lower probabilities of leaving, but it's not obvious at a glance. It would help if the graph reflected the fact that the categories of <span class="InputCode">leave</span> are ordered. Right now both the colors used and the arrangement of the categories in the legend feel arbitrary.</p>
<p>To fix the colors, take control of the individual bars with  <span class="InputCode">bar()</span> options. Each bar will get its own option, identified by a number inside the parentheses. Then you can use options to control the properties of that bar. For example <span class="InputCode">bar(1, color(maroon) fintensity(inten80))</span>, means the first bar will be maroon (dark red) with a fill intensity of 80% (<span class="InputCode">inten80</span> being shorthand for that). We'll make "likely to leave" red (officially <span class="InputCode">maroon</span>), "neither likely nor unlikely" <span class="InputCode">gray</span>, and and "unlikely to leave" blue (officially <span class="InputCode">navy</span>) with the degree of likelihood represented by fill intensity.</p>
<p>Next the legend, which is controlled by putting options within the <span class="InputCode">legend()</span> option. The category ordering will be much clearer if all the entries are in a single row, which you can do with <span class="InputCode">rows(1)</span>. To make that fit, put the labels underneath the color symbols rather than next to them with <span class="InputCode">stack</span>. Shrink the text with <span class="InputCode">size(small)</span>. However, we still need to break the labels into multiple lines. Unfortunately, the syntax for setting legend labels is just different enough that <span class="InputCode">splitvallabels</span> can't help you, so you'll have to do it yourself. The easy way to set a bunch of labels at once is to use the <span class="InputCode">order()</span> option: </p>
<p class="InputCode">order(1 "Very" "likely" 2 "Somewhat" "likely" ///<br/>
3 "Neither likely" "nor unlikely"  ///<br/>
4 "Somewhat" "unlikely" 5 "Very" "unlikely")</p>
<p>This is getting picky, but everything will line up better if you use <span class="InputCode">symplacement(center)</span> to put the color symbols in the center of their space. You can  add a title to the legend itself by putting a <span class="InputCode">title()</span> option inside <span class="InputCode">legend()</span>,  but you'll want to make it <span class="InputCode">size(small)</span>.</p>
<p>The resulting code is long, but each component part is straightforward:</p>
<p class="InputCode">splitvallabels sat<br/>
                  catplot leave sat, ///<br/>
                  percent(sat) ///<br/>
                  var1opts(label(labsize(small))) ///<br/>
                  var2opts(label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("Percent of Respondents by Satisfaction", size(small)) ///<br/>
                  title("Probability of Leaving by Job Satisfaction" ///<br/>
                  , span size(medium)) ///<br/>
                  blabel(bar, format(%4.1f) size(vsmall)) ///<br/>
                  intensity(25) ///<br/>
                  asyvars ///<br/>
<span class="BoldCode">bar(1, color(maroon) fintensity(inten80)) ///<br/>
                  bar(2, color(maroon) fintensity(inten60)) ///<br/>
                  bar(3, color(gray) fintensity(inten40)) ///<br/>
                  bar(4, color(navy) fintensity(inten60)) ///<br/>
                  bar(5, color(navy) fintensity(inten80))	///<br/>
                  legend(rows(1) stack size(small) ///<br/>
                  order(1 "Very" "likely" 2 "Somewhat" "likely" ///<br/>
                  3 "Neither likely" "nor unlikely"  ///<br/>
                  4 "Somewhat" "unlikely" 5 "Very" "unlikely") ///<br/>
                  symplacement(center) ///<br/>
                  title(Likelihood of Leaving, size(small)))</span><br/>
</p>
<img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b17.png" width="550"/>
<p>The result is still fairly cluttered, however, and the bars to be compared aren't very close together. An alternative is to stack the bars for each category  using the <span class="InputCode">stack</span> option. Unfortunately, getting bar labels to work with stacked bars is not straightforward (there won't always be space for them), so take out the <span class="InputCode">blabel()</span> option. </p>
<p class="InputCode">splitvallabels sat<br/>
                  catplot leave sat, ///<br/>
                  percent(sat) ///<br/>
                  var1opts(label(labsize(small))) ///<br/>
                  var2opts(label(labsize(small)) relabel(`r(relabel)')) ///<br/>
                  ytitle("Percent of Respondents by Satisfaction", size(small)) ///<br/>
                  title("Probability of Leaving by Job Satisfaction" ///<br/>
                  , span size(medium)) ///<br/>
                  intensity(25) ///<br/>
                  asyvars <span class="BoldCode">stack</span> ///<br/>
                  bar(1, color(maroon) fintensity(inten80)) ///<br/>
                  bar(2, color(maroon) fintensity(inten60)) ///<br/>
                  bar(3, color(gray) fintensity(inten40)) ///<br/>
                  bar(4, color(navy) fintensity(inten60)) ///<br/>
                  bar(5, color(navy) fintensity(inten80))	///<br/>
                  legend(rows(1) stack size(small) ///<br/>
                  order(1 "Very" "likely" 2 "Somewhat" "likely" ///<br/>
                  3 "Neither likely" "nor unlikely"  ///<br/>
                  4 "Somewhat" "unlikely" 5 "Very" "unlikely") ///<br/>
                  symplacement(center) ///<br/>
                title(Likelihood of Leaving, size(small)))</p>
<img alt="" class="CenterImage" height="400" src="https://ssc.wisc.edu/sscc/pubs/bargraphs/b18.png" width="550"/>
<p>This graph often requires some explanation. But once people grasp that what they should be looking for is how the colors move left or right as you go up or down categories, they can see the relationship between the variables very clearly. Compare with:</p>
<p class="InputCode">tab sat leave, row</p>
<pre class="InputCode">                      |                         leave
                  sat | Very like  Somewhat   Neither l  Somewhat   Very unli |     Total
----------------------+-------------------------------------------------------+----------
    Very Dissatisfied |         5         22         13          6          0 |        46 
                      |     10.87      47.83      28.26      13.04       0.00 |    100.00 
----------------------+-------------------------------------------------------+----------
Somewhat Dissatisifie |         3         24         28         22          3 |        80 
                      |      3.75      30.00      35.00      27.50       3.75 |    100.00 
----------------------+-------------------------------------------------------+----------
Neither Satisfied nor |         5         43         81         82         20 |       231 
                      |      2.16      18.61      35.06      35.50       8.66 |    100.00 
----------------------+-------------------------------------------------------+----------
   Somewhat Satisfied |         3         31         94        118         52 |       298 
                      |      1.01      10.40      31.54      39.60      17.45 |    100.00 
----------------------+-------------------------------------------------------+----------
       Very Satisfied |         5         37        100        129         74 |       345 
                      |      1.45      10.72      28.99      37.39      21.45 |    100.00 
----------------------+-------------------------------------------------------+----------
                Total |        21        157        316        357        149 |     1,000 
                      |      2.10      15.70      31.60      35.70      14.90 |    100.00 </pre>
<p>Once you've created your bar graphs, you'll need to save them in a format that you can use. <a href="https://ssc.wisc.edu/sscc/pubs/4-23.htm">Using Stata Graphs in Documents</a> will help you with that. You may also be interested in <a href="https://ssc.wisc.edu/sscc/pubs/4-24.htm">An Introduction to Stata Graphics</a>, which will introduce you to many other kinds of graphs, and also more options you can use with bar graphs. A do file that contains all the code for this article is available <a href="https://ssc.wisc.edu/sscc/pubs/bargraphs/bargraph.do">here</a>.</p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url>https://ssc.wisc.edu/sscc/pubs/bargraphs/b1.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b2.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b3.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b4.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b5.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b6.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b7.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/mean.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b8.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b8b.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b8c.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b9.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b10.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b11.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b12.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b13.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b14.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b15.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b16.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b17.png, https://ssc.wisc.edu/sscc/pubs/bargraphs/b18.png</img_base_url>
</kb_document>
<kb_document>
<kb_title>Working with Dates in Stata</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p>Stata has many tools for working with dates. This article will introduce you to some of the most useful and easy to use features.</p>
<p>A Stata date is simply a number, but with the <span class="InputCode">%td</span> format applied Stata will interpret that number as "number of days since January 1, 1960." You can then use that number in a variety of ways. Stata has similar tools that measure time in terms of milliseconds, months, quarters, years and more. This article will focus on days, but if you know how to work with days you can quickly learn the others.</p>
<p>Often the first task is  to convert the data you've been given into official Stata dates.	</p>
<h2>Converting Strings to Dates</h2>
<p>If you've been given a date in string form, such as "November 3, 2010", "11/3/2010" or "2010-11-03 08:35:12" it can be converted using the <span class="InputCode">date</span> function. The date function takes two arguments, the string to be converted, and a series of letters called a "mask" that tells Stata how the string is structured. In a date mask, <span class="InputCode">Y</span> means year, <span class="InputCode">M</span> means month, <span class="InputCode">D</span> means day and <span class="InputCode">#</span> means an element should be skipped.</p>
<p> Thus the mask <span class="InputCode">MDY</span> means "month, day, year" and can be used to convert both "November 3, 2010" and "11/3/2010". A date like "2010-11-03 08:35:12" requires the mask <span class="InputCode">YMD###</span> so that the last three numbers are skipped. If you are interested in tracking the time of day you need to switch to the <span class="InputCode">clock</span> function and the <span class="InputCode">%tc</span> format so time is measured in milliseconds rather than days, but they are very similar.</p>
<p>To see this in action, type (or copy and paste) the following into Stata:</p>
<p class="InputCode">use http://www.ssc.wisc.edu/sscc/pubs/files/dates.dta</p>
<p>This is an example data set containing the above dates as <span class="InputCode">dateString1</span>, <span class="InputCode">dateString2</span> and <span class="InputCode">dateString3</span>. To convert them to Stata dates do the following:</p>
<p class="InputCode">gen date1=date(dateString1,"MDY")<br/>
                  gen date2=date(dateString2,"MDY")<br/>
                gen date3=date(dateString3,"YMD###")</p>
<p>Note that the mask goes in quotes.</p>
<h2>Converting Numbers to Dates</h2>
<p>Another common  scenario gives you dates as three separate numeric variables, one for the year, one for the month and one for the day. The <span class="InputCode">year</span>, <span class="InputCode">month</span> and <span class="InputCode">day</span> variables in the example data set contain the same date as the others but in this format. To convert such dates to Stata dates, use the <span class="InputCode">mdy</span> function. It takes three numeric arguments: the month, day and year to be converted.</p>
<p class="InputCode">gen date4=mdy(month,day,year)</p>
<h2>Formatting Date Variables</h2>
<p>While the four date variables you've created are perfectly functional dates as far as Stata is concerned, they're difficult for humans to interpret. However, the <span class="InputCode">%td</span> format tells Stata to print them out as human readable dates: </p>
<p class="InputCode">format date1 %td<br/>
                format date2 %td<br/>
                format date3 %td<br/>
                format date4 %td</p>
<p>This turns the <span class="InputCode">18569</span> now stored in all four variables into <span class="InputCode">03nov2010</span> (18,569 days since January 1, 1960) in all output. Try a <span class="InputCode">list</span> to see the result. If you remember your <a href="https://ssc.wisc.edu/sscc/pubs/sfr-syntax.htm#Varlists">varlist syntax</a>, you can do them all at once with:</p>
<p class="InputCode">format date? %td</p>
<p>You can have Stata output dates in different formats as well. For instructions type <span class="InputCode">help dates</span> and then click on the link <span class="MenuOutput">Formatting date and time values</span>.</p>
<h2>Using Dates</h2>
<p>Often your goal in creating a Stata date will be to create a time variable that can be included in a statistical command. If so, you can probably use it with no further modification. However, there are some common data preparation tasks involving dates.</p>
<h3>Date Constants</h3>
<p>If you need to refer to a particular date in your code, then in principle you could refer to it by number. However, it's usually more convenient to use the same functions used to import date variables. For example, the following are all equivalent ways of referring to November 3, 2010:</p>
<p class="InputCode">18569<br/>
  date("November 3, 2010","MDY")<br/>
  mdy(11,3,2010)</p>
<p>  The <span class="InputCode">td</span> pseudofunction was designed for tasks like this and is somewhat more convenient to use. It takes a single argument (which cannot be a variable name) and converts it to a date on the assumption that the argument is a string containing a date in the format day, month, year. This matches the output of the <span class="InputCode">%td</span> format, e.g. <span class="InputCode">3nov2010</span>. Thus the following is also equivalent:</p>
<p class="InputCode">td(3nov2010)</p>
<p>However, the following is not:</p>
<p class="InputCode">td(11/3/2010)</p>
<p>This will be interpreted as March 11, 2010, not November 3, 2010.</p>
<h3>Extracting Date Components</h3>
<p>Sometimes you need to pull out the components of a date. You can do so with the <span class="InputCode">year</span>, <span class="InputCode">month</span> and <span class="InputCode">day</span> functions:</p>
<p class="InputCode">gen year1=year(date1)<br/>
  gen month1=month(date1)<br/>
  gen day1=day(date1)</p>
<h3></h3>
<h3>Before and After</h3>
<p>Since dates are just numbers, before and after are equivalent to less than and greater than. Thus:</p>
<p class="InputCode">gen before2010=(date1&lt;td(1,1,2010))<br/>
gen after2010=(date1&gt;date("January 1 2010","MDY"))</p>
<p></p>
<p></p>
<p></p>
<h3>Durations and Intervals</h3>
<p>Durations in days can be found using simple subtraction. The example data set contains the dates <span class="InputCode">beginning</span> and <span class="InputCode">ending</span>, and you can find out the duration of the interval between them with:</p>
<p class="InputCode">gen duration=ending-beginning</p>
<p>Durations in months are more difficult because months vary in length. One common approach is to ignore days entirely and calculate the duration solely from the year and month components of the dates involved:</p>
<p class="InputCode">gen durationInMonths=(year(ending)-year(beginning))*12+month(ending)-month(beginning)</p>
<p>Just keep in mind that this approach says January 31 and February 1 are one month apart, while January 1 and January 31 are zero months apart.</p>
<h3>Date Arithmetic</h3>
<p>If you need to add (or subtract) a period measured in days to a date, it is straightforward to do so. Just remember to format all new date variables as dates with <span class="InputCode">%td</span>:</p>
<p class="InputCode">gen tenDaysLater=date1+10<br/>
  gen yesterday=date1-1<br/>
  format %td tenDaysLater  yesterday</p>
<p>If the period is measured in weeks, just multiply by 7.</p>
<h3>Months and Years</h3>
<p>Months and years are problematic because they don't always represent the same amount of time. A month can be anything from 28 to 31 days, and a calendar year is usually 365 days but is 366 days in leap year.</p>
<p>Suppose today's date were April 20th, 2017. If I asked you what the date will be in one month, you'd probably respond May 20th, 2017. If I asked you what the date will be in one year, you'd probably respond April 20th, 2018.</p>
<p>Now consider two particularly problematic dates: January 31, 2016 and February 29, 2016. One month after January 31st cannot be February 31st, because no such day exists. Similarly, one year after February 29th, 2016 cannot be February 29, 2017, because <a href="https://en.wikipedia.org/wiki/The_Pirates_of_Penzance">February 29th only exists in leap years</a>. So what should be done?</p>
<p>If the dates in your data are mostly just months, consider storing them as dates in monthly format, where the underlying number is the number of months since January, 1960 rather than the number of days. Then all these issues go away. You can convert dates to monthly dates with the <span class="InputCode">mofd()</span> function and then give them the <span class="InputCode">%tm</span> format to make them readable:</p>
<p class="InputCode">clear<br/>
  use http://www.ssc.wisc.edu/sscc/pubs/files/moredates.dta<br/>
<br/>
  gen monthlyDate=mofd(date)<br/>
  gen oneMonthLater1=monthlyDate+1<br/>
  gen oneYearLater1=monthlyDate+12<br/>
  format monthlyDate oneMonthLater1 oneYearLater1 %tm</p>
<p>Alternatively, you can define a "standard month" of 30 or 31 days or a "standard year" of 365 days (no, 365.25 days won't work unless you're storing time as well as date). This has the advantage of making all time intervals uniform. It's also easy to program:</p>
<p class="InputCode">gen oneMonthLater2=date+30<br/>
  gen oneYearLater2=date+365<br/>
  format oneMonthLater2 oneYearLater2 %td</p>
<p>However, note that 30 days after January 31, 2016 is March 1, 2016, and 365 days after January 31, 2016 is January 30, 2017. That might not be ideal in some contexts.</p>
<p>Another practical definition of "one month later" would be "the same day in the next month if that exists, otherwise the last day of the next month." Then one month after January 31st would be February 28th or 29th depending on the year. You can implement this, without programming in how many days each month has, using the following algorithm:</p>
<ol>
<li>Convert the date to a monthly date and add one month to it</li>
<li>Create a new date based on the month and year from the  date created in step 1, and the day of the month from the original date</li>
<li>If the resulting date is invalid (e.g. February 31st) subtract days until you get a valid date</li>
</ol>
<p>The following code implements this algorithm:</p>
<p class="InputCode">gen oneMonthLaterTemp=dofm(mofd(date)+1)<br/>
  gen oneMonthLater3=mdy(month(oneMonthLaterTemp),day(date),year(oneMonthLaterTemp))<br/>
  egen numInvalid=total(oneMonthLater3==.) // calculate number of dates that are invalid<br/>
  local i 1 // number of days to subtract from invalid dates<br/>
  while (numInvalid&gt;0) {<br/>
<span class="indent3">replace oneMonthLater3=mdy(month(oneMonthLaterTemp),day(date)-`i',year(oneMonthLaterTemp)) if oneMonthLater3==.</span><br/>
<span class="indent3">local i=`i'+1 // increase number of days to subtract</span><br/>
<span class="indent3">drop numInvalid</span><br/>
<span class="indent3">egen numInvalid=total(oneMonthLater3==.) // see if we still have invalid dates</span><br/>
  }<br/>
  drop oneMonthLaterTemp numInvalid<br/>
  format oneMonthLater3 %td</p>
<p>The<span class="InputCode"> mofd()</span> function converts a regular date to a monthly date, and the <span class="InputCode">dofm()</span> function converts a monthly date to a regular date.</p>
<p>Very similar code could be used for adding years without needing to program in which years are leap years.</p>
<h2>Learning More</h2>
<p>To read the full documentation on Stata dates, type <span class="InputCode">help dates</span> and then click on the <span class="MenuOutput">dates and times</span> link at the top (the PDF documentation is much easier to read in this case). There you'll learn to:</p>
<ul>
<li>Work with times</li>
<li>Use intervals other than days, such as months, quarters or years</li>
<li>Create your own date format for output (e.g. <span class="InputCode">November 3rd, 2010</span> rather than <span class="InputCode">3nov2010</span>)</li>
<li>Track leap seconds, in case you need to be extremely precise--you'll also find an explanation of why such things exist</li>
</ul>
<h2>Complete Do File</h2>
<p>The following is a do file containing all the code from this article (except for a few code fragments that were discussed but can't be run by themselves):</p>
<p class="InputCode">clear all<br/>
set more off<br/>
use http://www.ssc.wisc.edu/sscc/pubs/files/dates.dta<br/>
<br/>
gen date1=date(dateString1,"MDY")<br/>
gen date2=date(dateString2,"MDY")<br/>
gen date3=date(dateString3,"YMD###")<br/>
gen date4=mdy(month,day,year)<br/>
 format date? %td<br/>
<br/>
gen year1=year(date1)<br/>
gen month1=month(date1)<br/>
gen day1=day(date1)<br/>
<br/>
gen before2010=(date1&lt; td(1,1,2010))<br/>
gen after2010=(date1&gt;date("January 1 2010","MDY"))<br/>
<br/>
gen duration=ending-beginning<br/>
gen durationInMonths=(year(ending)-year(beginning))*12+month(ending)-month(beginning)<br/>
<br/>
gen tenDaysLater=date1+10<br/>
gen yesterday=date1-1<br/>
format tenDaysLater yesterday

%td<br/>
<br/>
clear<br/>
use http://www.ssc.wisc.edu/sscc/pubs/files/moredates.dta<br/>
<br/>
gen monthlyDate=mofd(date)<br/>
gen oneMonthLater1=monthlyDate+1<br/>
gen oneYearLater1=monthlyDate+12<br/>
format monthlyDate oneMonthLater1 oneYearLater1 %tm<br/>
<br/>
gen oneMonthLater2=date+30<br/>
gen oneYearLater2=date+365<br/>
format oneMonthLater2 oneYearLater2 %td<br/>
<br/>
gen oneMonthLaterTemp=dofm(mofd(date)+1)<br/>
gen oneMonthLater3=mdy(month(oneMonthLaterTemp),day(date),year(oneMonthLaterTemp))<br/>
egen numInvalid=total(oneMonthLater3==.) // calculate number of dates that are invalid<br/>
local i 1 // number of days to subtract from invalid dates<br/>
while (numInvalid&gt;0) {
<br/>
<span class="indent3">replace oneMonthLater3=mdy(month(oneMonthLaterTemp),day(date)-`i',year(oneMonthLaterTemp)) if oneMonthLater3==.</span><br/>
<span class="indent3">local i=`i'+1 // increase number of days to subtract</span><br/>
<span class="indent3">drop numInvalid</span><br/>
<span class="indent3">egen numInvalid=total(oneMonthLater3==.) // see if we still have invalid dates</span><br/>
}<br/>
drop oneMonthLaterTemp numInvalid<br/>
format oneMonthLater3 %td </p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Exploring Regression Results using Margins</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p>Once you've run a regression, the next challenge is to figure out what the results mean. The <span class="InputCode">margins</span> command is a powerful tool for understanding a model, and this article will show you how to use it. It contains the following sections:</p>
<ol>
<li><a href="#OLS">OLS Regression (With Non-Linear Terms)</a></li>
<li><a href="#marginslogit">Logistical Regression</a></li>
<li><a href="#MultinomialLogit">Multinomial Logit</a></li>
</ol>
<p>Sections 1 and 2 are taken directly from the <a href="https://ssc.wisc.edu/sscc/pubs/sfr-stats.htm">Statistics section of Stata for Researchers</a> (they are reproduced here for the benefit of those looking specifically for information  about using <span class="InputCode">margins</span>). If you're familiar with that material you can to skip to section 3.</p>
<h2 id="OLS">OLS Regression (With Non-linear Terms)</h2>
<p>The <span class="InputCode">margins</span> command can only be used after you've run a regression, and acts on the results of the most recent regression command. For our first example, load the auto data set that comes with Stata and run the following regression:</p>
<p class="InputCode">sysuse auto<br/>
                reg price c.weight##c.weight i.foreign i.rep78 mpg displacement                </p>
<h3>Levels of the Outcome Variable</h3>
<p>If you just type:</p>
<p class="InputCode">margins</p>
<p>all by itself, Stata will calculate the predicted value of the dependent variable for each observation, then report the mean value of those predictions (along with the standard error, t-statistic, etc.).</p>
<p>If margins is followed by a categorical variable, Stata first identifies all the levels of the categorical variable. Then, for each value it calculates what the mean predicted value of the dependent variable <em>would be</em> if all observations had that value for the categorical variable. All other variables are left unchanged. Thus:</p>
<p class="InputCode">margins foreign</p>
<p>first asks, "What would the mean price be if all the cars were domestic?" (but still had their existing weights, displacements, etc.) and then asks "What would the mean price be if all the cars were foreign?"</p>
<p class="InputCode">margins rep78</p>
<p>does the same for all five values of <span class="InputCode">rep78</span>, but since there are so many of them it's a good candidate for a graphical presentation. The <span class="InputCode">marginsplot</span> command takes the results of the previous <span class="InputCode">margins</span> command and turns them into a graph:</p>
<p class="InputCode">marginsplot</p>
<p>For continuous variables <span class="InputCode">margins</span> obviously can't look at all possible values, but you can specify which values you want to examine with the <span class="InputCode">at</span> option:</p>
<p class="InputCode">margins, at(weight=(2000 4000))</p>
<p>This calculates the mean predicted value of <span class="InputCode">price</span> with <span class="InputCode">weight</span> set to 2000 pounds, and then again with <span class="InputCode">weight</span> set to  4000 pounds. Think of each value as a "scenario"—the above scenarios are very simple, but you can make much more complicated scenarios by listing multiple variables and values in the <span class="InputCode">at</span> option. The <span class="InputCode">margins</span> output first assigns a number to each scenario, then gives their results by number.</p>
<p> The values are specified using a <em>numlist</em>. A <em>numlist</em> is a list of numbers just like a <em>varlist</em> is a list of variables and, like a <em>varlist,</em> there are many different ways to define a <em>numlist</em>. Type <span class="InputCode">help numlist</span> to see them all. The simplest method is just to list the numbers you want, as above. You can also define a <em>numlist</em> with the by specifying <span class="Parameter">start</span><span class="InputCode"> (</span><span class="Parameter">interval</span><span class="InputCode">)</span><span class="Parameter"> end</span>:</p>
<p class="InputCode">margins, at(weight=(1500 (500) 5000))</p>
<p>This calculates the mean predicted value of <span class="InputCode">price</span> with <span class="InputCode">weight</span> set to 1500,  2000, 2500, etc. up to 5000. (The actual weights range from 1760 to 4840.) Again, this is a good candidate for a graphic:</p>
<p class="InputCode">marginsplot</p>
<h3>Effect of a Covariate</h3>
<p>If you want to look at the marginal effect of a covariate, or the derivative of the mean predicted value with respect to that covariate, use the <span class="InputCode">dydx</span> option:</p>
<p class="InputCode">margins, dydx(mpg)</p>
<p>In this simple case, the derivative is just the coefficient on<span class="InputCode"> mpg</span>, which will always be the case for a linear model. But consider changing <span class="InputCode">weight</span>: since the model includes both <span class="InputCode">weight</span> and weight squared you have to take into account the fact that both change. This case is particularly confusing (but not unusual) because the coefficient on <span class="InputCode">weight</span> is negative but the coefficient on weight squared is positive. Thus the  net effect of changing <span class="InputCode">weight</span> for any given car will very much depend on its starting weight.</p>
<p>The <span class="InputCode">margins</span> command can very easily tell you the mean effect:</p>
<p class="InputCode">margins, dydx(weight)</p>
<p>What <span class="InputCode">margins</span> does here is take the numerical derivative of the  expected <span class="InputCode">price</span> with respect to <span class="InputCode">weight</span> for each car, and then calculates the mean. In doing so, <span class="InputCode">margins</span> looks at the actual data. Thus it considers the effect of changing the Honda Civic's weight from 1,760 pounds as well as changing the Lincoln Continental's from 4,840 (the weight squared term is more important with the latter than the former). It then averages  them along with all the other cars to get its result of 2.362865, or that each additional pound of <span class="InputCode">weight</span> increases the mean expected <span class="InputCode">price</span> by $2.36.</p>
<p>To see how the effect of <span class="InputCode">weight</span> changes as <span class="InputCode">weight</span> changes, use the <span class="InputCode">at</span> option again and then plot the results:</p>
<p class="InputCode">margins, dydx(weight) at(weight=(1500 (500) 5000))<br/>
                  marginsplot</p>
<p>This tells us that for low values of weight (less than about 2000), increasing weight actually reduces the price of the car. However, for most cars increasing weight increases price.</p>
<p>The <span class="InputCode">dydx</span> option also works for binary variables:</p>
<p class="InputCode">margins, dydx(foreign)</p>
<p>However, because <span class="InputCode">foreign</span> was entered into the model as <span class="InputCode">i.foreign</span>, <span class="InputCode">margins</span> knows that it cannot take the derivative with respect to <span class="InputCode">foreign</span> (i.e. calculate what would happen if all the cars became slightly more foreign). Thus it reports the difference between the scenario where all the cars are foreign and the scenario where all the cars are domestic. You can verify this by running:</p>
<p class="InputCode">margins foreign</p>
<p>and doing the subtraction yourself.</p>
<h2 id="marginslogit">Binary Outcome Models and Predicted Probabilities</h2>
<p>The <span class="InputCode">margins</span> command becomes even more useful with binary outcome models because they are always nonlinear. Clear the <span class="InputCode">auto</span> data set from memory and then load the <span class="InputCode">grad</span> from the SSCC's web site:</p>
<p class="InputCode">clear<br/>
                  use http://ssc.wisc.edu/sscc/pubs/files/grad.dta</p>
<p>This is a fictional data set consisting of 10,000 students. Exactly one half of them are "high socioeconomic status" (<span class="InputCode">highSES</span>) and one half are not. Exactly one half of each group was given an intervention, or "treatment" (<span class="InputCode">treat</span>) designed to increase the probability of graduation. The <span class="InputCode">grad</span> variable tells us whether they did in fact graduate. Your goals are to determine 1) whether the treatment made any difference, and 2) whether the effect of the treatment differed by socioeconomic status (SES).</p>
<p>You can answer the first question with a simple logit model:</p>
<p class="InputCode">logit grad treat highSES</p>
<p>The coefficient on <span class="InputCode">treat</span> is positive and significant, suggesting the intervention did increase the probability of graduation. Note that <span class="InputCode">highSES</span> had an even bigger impact.</p>
<p>Next examine whether the effect depends on SES by adding an interaction between the two:</p>
<p class="InputCode">logit grad treat##highSES</p>
<p>The coefficient on <span class="InputCode">treat#highSES</span> is not significantly different from zero. But does that really mean the treatment had exactly the same effect regardless of SES?</p>
<p>Binary outcomes are often interpreted in terms of odds ratios, so repeat the previous regression with the <span class="InputCode">or</span> <em>option</em> to see them:</p>
<p class="InputCode">logit grad treat##highSES, or</p>
<p>This tells us that the odds of graduating if you are treated are approximately 2.83 times the odds of graduating if you are not treated, regardless of your SES. Researchers sometimes confuse odds ratios with probability ratios; i.e. they say you are 2.83 times more "likely" to graduate if you are treated. This is incorrect.</p>
<p>If you ask <span class="InputCode">margins</span> to examine the interaction between two categorical variables, it will create scenarios for all possible combinations of those variables. You can use this to easily obtain the predicted probability of graduation for all four possible scenarios (high SES/low SES, treated/not treated):</p>
<p class="InputCode">margins highSES#treat</p>
<p>For low SES students, treatment increases the predicted probability of graduation from about .49 to about .73. For high SES students, treatment increases the predicted probability of graduation from about .96 to about .98. Now, if you plug those probabilities  into the formula for calculating the odds ratio, you will find that the odds ratio is 2.83 in both cases (use the full numbers from the <span class="InputCode">margins</span> output, not the two digit approximations given here). Treatment adds the same amount to the linear function that is passed through the logistic function in both cases. But recall the <em>shape</em> of the logistic function:</p>
<p><img alt="Graph of logistic function, with four possible scenarios marked" height="446" src="https://ssc.wisc.edu/sscc/pubs/screenshots/sfr/logit.png" width="631"/></p>
<p>The treatment has a much smaller effect on the probability of graduation for high SES students because their probability is already very high—it can't get much higher. Low SES students are in the part of the logistic curve that slopes steeply, so  changes in the linear function have much larger effects on the predicted probability.</p>
<p>The <span class="InputCode">margins</span> command can most directly answer the question "Does the effect of the treatment vary with SE?" with a combination of <span class="InputCode">dydx()</span> and <span class="InputCode">at()</span>:</p>
<p class="InputCode">margins, dydx(treat) at(highSES=(0 1))</p>
<p></p>
<p>(You can also do this with <span class="InputCode">margins highSES, dydx(treat)</span>.) Once again, these are the same numbers you'd get by subtracting the levels obtained above. We suggest always looking at levels as well as changes—knowing where the changes start from gives you a much better sense of what's going on.</p>
<p>It's a general rule that it's easiest to change the predicted probability for  subjects who are "on the margin;" i.e. those whose predicted probability starts near 0.5. However, this is a property of the logistic function, not the data. It is an assumption you make when you choose to run a logit model.</p>
<h2><a id="MultinomialLogit" name="MultinomialLogit"></a>Multinomial Logit</h2>
<p>Multinomial logit models can be even harder to interpret because the coefficients only compare two states. Clear Stata's memory and load the following data set, which was carefully constructed to illustrate the pitfalls of interpreting multinomial logit results:<br/>
</p>
<p class="InputCode">clear<br/>
                use http://www.ssc.wisc.edu/sscc/pubs/files/margins_mlogit.dta</p>
<p>It contains two variables, an integer <span class="InputCode">y</span> that takes on the values 1, 2 and 3; and a continuous variable <span class="InputCode">x</span>. They are negatively correlated (<span class="InputCode">cor y x</span>). </p>
<p>Now run the following model:</p>
<p class="InputCode">mlogit y x</p>
<p>The coefficient of <span class="InputCode">x</span> for outcome 2 is negative, so it's tempting to say that as <span class="InputCode">x</span> increases the probability of <span class="InputCode">y</span> being 2 decreases. But in fact that's not the case, as the <span class="InputCode">margins</span> command will show you:</p>
<p class="InputCode">margins, dydx(x) predict(outcome(2))</p>
<p>The <span class="InputCode">predict()</span> options allows you to choose the response <span class="InputCode">margins</span> is examining. <span class="InputCode">predict(outcome(2))</span> specifies that you're interested in the expected probability of outcome 2. And in fact the probability of outcome 2 increases with <span class="InputCode">x</span>, the derivative being 0.016.</p>
<p>How can that be? Recall that the coefficients given by <span class="InputCode">mlogit</span> only compare the probability of a given outcome with the base outcome. Thus the <span class="InputCode">x</span> coefficient of -5.34 for outcome 2 tells you that as <span class="InputCode">x</span> increases, observations are likely to move from outcome 2 to outcome 1. Meanwhile the <span class="InputCode">x</span> coefficient of -21.292 for outcome 3 tells you that as <span class="InputCode">x</span> increases observations are likely to move from outcome 3 to outcome 1. What it doesn't tell you is that as <span class="InputCode">x</span> increases observations also move from outcome 3 to outcome 2, and in fact that effect dominates the movement from 2 to 1.</p>
<p>You can  see it if you change the base category of the regression:</p>
<p class="InputCode">mlogit y x, base(2)</p>
<p>Now the coefficients tell you about the probability of each outcome compared to outcome 2, and the fact that the negative <span class="InputCode">x</span> coefficient for outcome 3 is much larger (in absolute terms) than the positive <span class="InputCode">x</span> coefficient for outcome 1 indicates that increasing <span class="InputCode">x</span> increases the probability of outcome 2.</p>
<p>We strongly recommend using <span class="InputCode">margins</span> to explore what your regression results mean.</p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url>https://ssc.wisc.edu/sscc/pubs/screenshots/sfr/logit.png</img_base_url>
</kb_document>
<kb_document>
<kb_title>Speeding up Multiple Imputation in Stata using Parallel Processing</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p>Multiple imputation is computationally intensive and can be time consuming if your data set is large. On the other hand, the process of creating each imputation is independent of the others, which means you can have multiple CPUs working on different imputations at the same time and then combine them when they're all complete. This article will show you how to do so automatically using the SSCC's Condor flock, but the technique can be used on any computer with multiple CPUs (see <a href="#WhatifIdonthaveaccesstoaCondorflock">What if I don't have access to a Condor flock?</a>).</p>
<p>This article uses macros and loops. If you're not familiar with them, you should probably read <a href="https://ssc.wisc.edu/sscc/pubs/stata_prog1.htm">Stata Programming Essentials</a> before proceeding. You'll also need to  run  Stata programs on  the SSCC's Linux servers. If you've never used Linux before this is easier than you probably think--see <a href="https://ssc.wisc.edu/sscc/pubs/linstat.htm">Using Linstat</a> for instructions. You may  want to read <a href="https://ssc.wisc.edu/sscc/pubs/7-1.htm">An Introduction to Condor</a>, though this article will teach you all the Condor commands you'll need.</p>
<p>Stata users who want to do multiple imputation can choose between Stata's official <span class="InputCode">mi</span> commands and the user-written <span class="InputCode">ice</span>. At this point, <span class="InputCode">mi</span> (in particular <span class="InputCode">mi impute chained</span>) can do everything <span class="InputCode">ice</span> can do and we recommend everyone use <span class="InputCode">mi</span>. However, many people are still used to using <span class="InputCode">ice</span>.   Fortunately you can use�<span class="InputCode">ice</span>�for the actual imputation and then convert the data set to�<span class="InputCode">mi</span>'s format so you can do things like combine imputations that were done separately. This article will discuss using both: look for�<span class="InputCode">mi</span>�and�<span class="InputCode">ice</span>�sections describing the commands that are needed only if you're using one or the other.                </p>
<p>We'll introduce three do files. <span class="InputCode">setup.do</span> prepares the data and submits all the imputation jobs to Condor. <span class="InputCode">impute.do</span> is run by Condor--multiple times in parallel--and does the actual imputation. <span class="InputCode">combine.do</span> then combines the results into a single file. Full code for all three do files, with <span class="InputCode">mi</span> and <span class="InputCode">ice</span> versions, can be found at <a href="#CodefortheDoFiles">the end of this article</a>. As written, the do files use an example data set we've made available at <a href="https://www.ssc.wisc.edu/sscc/pubs/files/missing_data.dta">https://www.ssc.wisc.edu/sscc/pubs/files/missing_data.dta</a>. It contains <span class="InputCode">id</span>, <span class="InputCode">y</span> and variables <span class="InputCode">x1</span> through <span class="InputCode">x10</span>. <span class="InputCode">y</span> and all the <span class="InputCode">x</span>'s have missing values. If you want to run the do files as written, make a directory for your work, make it your current directory and then place a copy of this data set in it. The easiest way is probably to run the following in Stata:</p>
<p class="InputCode">use http://www.ssc.wisc.edu/sscc/pubs/files/missing_data.dta<br/>
save missing_data</p>
<p>Alternatively you can adapt the do files to use your data immediately. For this technique to work your data set must have a unique identifier variable, so if your data set does not have one you'll have to create it (<span class="InputCode">gen id=_n </span>will probably do).</p>
<h2><a id="SettingUp" name="SettingUp"></a>Setting Up (setup.do)</h2>
<p>Begin with  the usual housekeeping (clearing the memory, starting a log, etc.).</p>
<h3>mi</h3>
<p><span class="InputCode">mi</span> requires some configuration before you can do any imputing. Begin by loading the data. Then choose the data structure <span class="InputCode">mi</span> should use with the <span class="InputCode">mi set</span> command. If you need to manipulate your data after imputing you should learn what the different formats are and when each should be used--type <span class="InputCode">help my_styles</span>. If not, it makes little difference which you use so use <span class="InputCode">mlong</span>. Next, register the variables you wish to impute and save the result in a new file.</p>
<p class="InputCode">use missing_data<br/>
                  mi set mlong<br/>
                mi register imputed x* y<br/>
                save missing_data_mi, replace
                </p>
<h3>Both</h3>
<p>Now it's time to submit the imputation jobs to Condor. For this example we'll create ten imputations by submitting ten jobs that create one imputation each. <a href="https://www.ssc.wisc.edu/sscc/policies/server_usage.htm">In fairness to other users</a> do not submit more than fifteen jobs, but if you want  more than fifteen imputations  have each job create more than one imputation. The following code submits the jobs:</p>
<p class="InputCode">forvalues jobID=1/10 {<br/>
<span class="indent3">shell condor_stata impute `jobID' &amp;</span><br/>
}</p>
<p>The <span class="InputCode">shell</span> command tell Stata to have Linux execute what follows. <span class="InputCode">condor_stata impute</span> tells Condor to run the Stata job <span class="InputCode">impute.do</span> (the <span class="InputCode">.do</span> is implied). <span class="InputCode">`jobID'</span> passes the current value of the <span class="InputCode">jobID</span> macro to <span class="InputCode">impute.do</span> as an argument--you'll see how <span class="InputCode">impute.do</span> uses it shortly. The <span class="InputCode">&amp;</span> at the end tells Linux to run the <span class="InputCode">condor_stata</span> command in the background, so Stata doesn't have to wait for it to finish before proceeding with the loop.</p>
<h2><a id="Imputing" name="Imputing"></a>Imputing (impute.do)</h2>
<p>When <span class="InputCode">setup.do</span> is complete, ten instances of <span class="InputCode">impute.do</span> have been submitted to Condor and Condor will find CPUs for all of them. They differ in that each one has a different number for its <span class="InputCode">jobID</span> argument.</p>
<p>Since <span class="InputCode">impute.do</span> will always run in batch mode, it doesn't need to clear memory and such. Its first task is to retrieve its <span class="InputCode">jobID</span>. Do so with the <span class="InputCode">args</span> command:</p>
<p class="InputCode">args jobID</p>
<p>This stores the do file's argument in a macro called <span class="InputCode">jobID</span>. You can then use it to start a log file which will be unique to this instance of <span class="InputCode">impute.do</span>:</p>
<p class="InputCode">log using impute`jobID'.log, replace</p>
<p>The ten instances of <span class="InputCode">impute.do</span> will thus create ten log files, named <span class="InputCode">impute1.log</span>, <span class="InputCode">impute2.log</span>, etc. and you can check them individually as needed.</p>
<p>For reproducibility you want to set the seed for the random number generator, but each instance needs a different seed. One easy solution is to set the seed to an arbitrary number plus the instance's <span class="InputCode">jobID</span>:</p>
<p class="InputCode">set seed `=123454321+`jobID''</p>
<h3>mi</h3>
<p>Load the data set you prepared for <span class="InputCode">mi</span> in <span class="InputCode">setup.do</span>:</p>
<p class="InputCode">use missing_data_mi</p>
<p>When <span class="InputCode">mi</span> is actually imputing it creates  temporary files in the current directory, and if the ten instances of impute.do are trying to put their temporary files in the same directory they'll interfere with each other. Thus you need to create a directory for each instance (we'll remove them later) and make that the current directory:</p>
<p class="InputCode">mkdir impute`jobID'<br/>
                  cd impute`jobID'<br/>
</p>
<p>Now you're ready to actually impute:</p>
<p class="InputCode">mi impute chained (regress) x* y, add(1)</p>
<p>When the imputation is complete, go back to the original directory and remove the one you created.</p>
<p class="InputCode">cd ..<br/>
                rmdir impute`jobID'</p>
<p>Note that if your version of <span class="InputCode">impute.do</span> crashes for whatever reason before deleting the directories it creates, you'll need to delete them yourself before running it again.</p>
<p>Finally save the file, including the <span class="InputCode">jobID</span> in the name to make it unique:</p>
<p class="InputCode">save impute`jobID',replace</p>
<h3>ice</h3>
<p>First load the data, then use <span class="InputCode">ice</span> to do the imputation. Use the <span class="InputCode">clear</span> option so the imputed data replaces the old data in memory rather than being saved as a file (since we're not done with it).</p>
<p class="InputCode">use missing_data<br/>
                ice x* y, m(1) clear                </p>
<p>In order to use <span class="InputCode">mi</span>'s tools to combine imputations you need to convert the data to <span class="InputCode">mi</span>'s format. This is done with <span class="InputCode">mi import</span>:</p>
<p class="InputCode">mi import ice, imputed(x* y) clear</p>
<p>The <span class="InputCode">imputed</span> option tells mi that the <span class="InputCode">x</span> variables and <span class="InputCode">y</span> were imputed, but in order to combine imputations <span class="InputCode">mi</span> also needs to be explicitly told that <span class="InputCode">id</span> was not imputed. Do that by registering it as a regular variable:</p>
<p class="InputCode">mi register regular id<br/>
</p>
<p><span class="InputCode">mi import ice</span> puts the data in <span class="InputCode">flong</span> format, since that's basically what <span class="InputCode">ice</span> uses. <span class="InputCode">flong</span> duplicates complete cases unnecessarily, so change the format to <span class="InputCode">mlong</span> (again, type <span class="InputCode">help mi_styles</span> if you want to learn more about the formats <span class="InputCode">mi</span> can use):</p>
<p class="InputCode">mi convert mlong, clear</p>
<p>Now you're ready to save the results. Include <span class="InputCode">jobID</span> in the file name to make it unique.</p>
<p class="InputCode">save impute`jobID',replace</p>
<h2><a id="Combine" name="Combine"></a>Combining (combine.do)</h2>
<p>When all ten instances of <span class="InputCode">impute.do</span> have completed, you'll have ten data sets in your current directory named <span class="InputCode">impute1.dta</span> through <span class="InputCode">impute10.dta</span>. Be sure they're all there before proceeding. You can use the Linux <span class="InputCode">condor_q</span> command on Kite to see if any instances of <span class="InputCode">impute.do</span> are still running. Condor will also email you when each job is completed. (Condor has a tool called DAGMan that can do things like automatically run <span class="InputCode">combine.do</span> when all the instances of <span class="InputCode">impute.do</span> are done, but it's somewhat cumbersome. If you're interested in using it contact the <a href="https://ssc.wisc.edu/sscc/helpdesk.htm">help desk</a> for assistance.)</p>
<p>Your next task is to combine the ten files with one imputation each into one file with ten imputations. Begin (after the usual housekeeping) by loading the first file:</p>
<p class="InputCode">use impute1</p>
<p>Next loop over the remaining files (i.e. 2 through 10), adding them to what's already in memory using <span class="InputCode">mi add</span>:</p>
<p class="InputCode">                  forvalues jobID=2/10 {<br/>
<span class="indent3">mi add id using impute`jobID', assert(match)</span><br/>
}</p>
<p><span class="InputCode">mi add</span> is a bit like a merge, so you need to specify a key variable (in this case <span class="InputCode">id</span>) so it knows which observations to combine. The <span class="InputCode">assert(match)</span> option tells it that every observation should match and the do file should halt if any fail to match. Given how we created these files, a failure to match would mean something went very wrong.</p>
<p>All that's left is to save the output file containing all ten imputations:</p>
<p class="InputCode">save imputed_data, replace</p>
<h2><a id="CodefortheDoFiles" name="CodefortheDoFiles"></a>Code for the Do Files</h2>
<p>Following is complete code for  the do files described in this article. They assume that all the do files and the data are in your current directory, and that you're using an SSCC Linux server and thus can submit jobs to our Condor flock.</p>
<h3>setup.do (mi version)</h3>
<p class="InputCode">clear all<br/>
                  set more off<br/>
                  capture log close<br/>
                  log using setup.log, replace<br/>
<br/>
                  use missing_data<br/>
mi set mlong<br/>
mi register imputed x* y<br/>
save missing_data_mi, replace<br/>
<br/>
forvalues jobID=1/10 {<br/>
<span class="indent3">shell condor_stata  impute `jobID' &amp;</span><br/>
}<br/>
<br/>
log close</p>
<h3>setup.do (ice version)</h3>
<p class="InputCode">clear all<br/>
set more off<br/>
capture log close<br/>
log using setup.log,replace
<br/>
<br/>
forvalues jobID=1/10 {<br/>
<span class="indent3">shell condor_stata -b do impute `jobID' &amp;</span><br/>
}<br/>
<br/>
log close
                </p>
<h3>impute.do (mi version)</h3>
<p class="InputCode">args jobID<br/>
                  log using impute`jobID'.log, replace                  <br/>
                  set seed `=123454321+`jobID''                  <br/>
                  use missing_data_mi                  <br/>
                  mkdir impute`jobID'<br/>
cd impute`jobID'                  <br/>
mi impute chained (regress) x* y, add(1)<br/>
cd ..<br/>
rmdir impute`jobID'<br/>
save impute`jobID',replace<br/>
log close
<br/>
</p>
<h3>impute.do (ice version)</h3>
<p class="InputCode">args jobID<br/>
log using impute`jobID'.log, replace <br/>
set seed `=123454321+`jobID''<br/>
use missing_data<br/>
ice x* y, m(1) clear                <br/>
mi import ice, imputed(x* y) clear                <br/>
mi register regular id                <br/>
mi convert mlong, clear                <br/>
save impute`jobID',replace                <br/>
log close
                </p>
<h3>combine.do (both)</h3>
<p class="InputCode">clear all<br/>
                set more off<br/>
                capture log close<br/>
                log using combine.log, replace<br/>
<br/>
                use impute1                <br/>
                forvalues jobID=2/10 {<br/>
<span class="indent3">mi add id using impute`jobID', assert(match)</span><br/>
}<br/>
save imputed_data, replace</p>
<h2><a id="WhatifIdonthaveaccesstoaCondorflock" name="WhatifIdonthaveaccesstoaCondorflock"></a>What if I don't have access to a Condor flock? </h2>
<p>This article was primarily written to help SSCC members take advantage of the power of the SSCC's Condor flock, but the techniques described can also be used to take advantage of all the CPUs in today's multi-CPU computers. Just replace the command that submits the <span class="InputCode">impute.do</span> jobs to Condor with a command that runs them on your computer. (The <a class="InputCode" href="https://ideas.repec.org/c/boc/bocode/s457527.html">parallel</a> command by George Vega Yon and Brian Quistorff is another option, and easier to use.)</p>
<p> For example, to run these jobs on a Windows PC you might replace:</p>
<p class="InputCode">shell condor_stata -b do impute `jobID' &amp;</p>
<p>with:</p>
<p class="InputCode">winexec "C:\Program Files (x86)\Stata12\StataMP-64.exe" -b do impute `jobID'</p>
<p>The <span class="InputCode">winexec</span> command is very similar to <span class="InputCode">shell</span>, but tells Stata not to wait for the job to finish (since Windows doesn't use <span class="InputCode">&amp;</span> for that). You may need to experiment to find the exact command that will work on your computer.</p>
<p>Some other considerations:</p>
<ul>
<li>Do not submit more jobs than you have CPUs, or they'll just compete for computing time. If you have a two CPU ("Dual Core") computer but want ten imputations, submit two jobs that create five imputations each.</li>
<li>Make sure you have enough memory for all the jobs you submit. If Stata needs to use disk space as virtual memory it will slow down tremendously.</li>
<li>If your <span class="InputCode">profile.do</span> tells Stata to start in a particular directory, remove that command temporarily so the <span class="InputCode">impute.do</span> jobs start in the same directory as <span class="InputCode">setup.do</span>.</li>
<li>If you're not the only one using the computer be sure to leave enough CPUs for others. (SSCC members should <strong>never</strong> use this technique on Winstat--that's what Condor is for.)</li>
</ul>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Multiple Imputation in Stata:</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p class="intro"><em>This is part two of the Multiple Imputation in Stata series. For a list of topics covered by this series, see the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_intro.htm">Introduction</a>.</em></p>
<p>The decision to use multiple imputation rather than simply analyzing complete cases should not be made lightly. First, multiple imputation takes a substantial amount of time to learn, and a substantial amount of time to implement. Expect to spend at least as much time on your imputation model as on your analysis model—the model whose results you are interested in for your research. Second, the practical technique for doing it in the social sciences, multiple imputation by chained equations or MICE, lacks theoretical justification and using it may draw objections from some reviewers (of course not using it may draw objections from other reviewers). Third, it's quite possible to do it wrong and thus get invalid results without realizing it. This leads to a dilemma: if multiple imputation gives different results than complete case analysis, which will you believe? (And if it doesn't, what was the point?) Clearly you'll need to make sure you understand multiple imputation well enough to be confident you're using it properly.</p>
<p>On the other hand, complete cases analysis has substantial weaknesses as well. There is no single right answer to the question of how to handle missing data.</p>
<h2>Why MICE?</h2>
<p>This series will focus almost exclusively on Multiple Imputation by Chained Equations, or MICE, as implemented by the <span class="InputCode">mi impute chained</span> command. We recognize that it does not have the theoretical justification Multivariate Normal (MVN) imputation has. However, most SSCC members work with data sets that include binary and categorical variables, which cannot be modeled with MVN. (There are ways to adapt it for such variables, but they have no more theoretical justification than MICE.) We will not discuss monotone or univariate   imputation methods because we have yet to see an SSCC member with monotone data or just one variable to impute.</p>
<h2>Why <span class="InputCode">mi impute chained</span> rather than <span class="InputCode">ice</span>?</h2>
<p>For many years Patrick Royston's <span class="InputCode">ice</span> command was the standard implementation of MICE in Stata. We express our appreciation for his contribution to the Stata community. However, the new <span class="InputCode">mi impute chained</span> command has all the resources of Stata Corporation behind it, works directly with the  <span class="InputCode">mi</span> framework for handling imputed data, and we feel it is somewhat easier to learn and use. We'll thus use <span class="InputCode">mi impute chained</span> throughout this series and we suggest <span class="InputCode">ice</span> users switch to it.</p>
<h2>Issues to Consider</h2>
<p>Issues you should consider when deciding whether to use multiple imputation or not include the following:</p>
<h3>Power</h3>
<p>The reason you probably considered multiple imputation in the first place was to avoid losing observations because they contain missing values. Multiple imputation allows you to use what information is available in those observations that contain missing values, which can lead to smaller confidence intervals and more ability to reject null hypotheses.</p>
<p><a id="Power" name="Power"></a><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#Power">Example: Power</a></p>
<h3>MCAR Data vs. MAR Data</h3>
<p>In the multiple imputation literature, data are "missing completely at random" (MCAR) if the probability of a particular value being missing is completely independent of both the observed data and the unobserved data. In other words, the complete cases are a random sample. If the data are MCAR, then both complete cases analysis and multiple imputation give unbiased estimates.</p>
<p>If the probability of a particular value being missing depends only on the observed data, then the data is "missing at random" (MAR) and the complete cases are not a random sample. With MAR data, complete cases analysis gives biased results but multiple imputation does not. If you believe your data are MAR rather than MCAR, then you should definitely consider using multiple imputation.</p>
<p>If the probability of a particular value being missing depends on the unobserved data, then the data are "missing not at random" (MNAR). In theory multiple imputation can give unbiased estimates with MNAR data, but only if the imputation method includes a model of the missingness mechanism. You'd need to code such a method yourself; it cannot be done using <span class="InputCode">mi impute</span>, <span class="InputCode">ice</span>, etc. In practice, if your data are MNAR it's going to be very hard to carry out legitimate analysis.</p>
<p>Note that MCAR and MAR do not require that the probability of one value being missing be independent of the probability of another value being missing. Missing values are often linked. For example, if a person was not contacted in a survey wave, that person will be missing all the variables from that wave but that data could still be MCAR, MAR or MNAR.</p>
<p></p>
<p><a id="missmech" name="missmech"></a><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#MCARvsMARvsMNAR">Example: MCAR vs. MAR vs. MNAR</a></p>
<h4><a id="AreMyDataMCARMARorMNAR" name="AreMyDataMCARMARorMNAR"></a>Are My Data MCAR, MAR, or MNAR?</h4>
<p>Testing whether a given data set is MCAR or MAR is straightforward. First create a new indicator variable for each existing variable which is 1 if a given observation is missing that variable and 0 if it is not. The <span class="InputCode">misstable</span> command can do this part automatically with the <span class="InputCode">gen()</span> option. Then run  logit models to test if any of the other variables predict whether a given variable is missing. If they do, then the data is MAR rather than MCAR.</p>
<p>If you had variables <span class="InputCode">y,</span> <span class="InputCode">x1</span> and <span class="InputCode">x2</span>, the code would look like:</p>
<p class="InputCode">misstable sum, gen(miss_)<br/>
                  logit miss_y x1 x2<br/>
                  logit miss_x1 y x2<br/>
                  logit miss_x2 y x1</p>
<p>It would also be a good idea to run t-tests to see if the values of the other variables vary between missingness groups:</p>
<p class="InputCode">ttest x1, by(miss_y)<br/>
                  ttest x2, by(miss_y)<br/>
                  ttest y, by(miss_x1)<br/>
                  ttest x2, by(miss_x1)<br/>
                  ttest y, by(miss_x2)<br/>
                  ttest x1, by(miss_x2) </p>
<p>The following code automates this entire process:</p>
<p class="InputCode">                  local numvars <span class="Parameter">list of all numeric variables in data set</span><br/>
                  local missvars <span class="Parameter">list of all variables with missing values in data set</span><br/>
  misstable sum, gen(miss_)<br/>
                  foreach var of local missvars {<br/>
<span class="indent3">local covars: list numvars - var</span><br/>
<span class="indent3">display _newline(3) "logit missingness of `var' on `covars'"</span><br/>
<span class="indent3">logit miss_`var' `covars'</span><br/>
<span class="indent3">foreach nvar of local covars {</span><br/>
<span class="indent6">display _newline(3) "ttest of `nvar' by missingness of `var'"</span><br/>
<span class="indent6">ttest `nvar', by(miss_`var')</span><br/>
<span class="indent3">}</span><br/>
                  }</p>
<p>If you have a lot of variables and can put them into a convenient <em>varlist</em> (like <span class="InputCode">x1-x10</span> or even <span class="InputCode">_all</span>) replace the two initial <span class="InputCode">local</span> commands with <span class="InputCode">unab</span>:</p>
<p class="InputCode">unab numvars: <span class="Parameter">numeric variables as varlist</span><br/>
                  unab missvars: <span class="Parameter">variables with missing values as varlist</span></p>
<p>There is no formal test for determining whether a given set of logit results means the data is MCAR or MAR, but they will give you a sense of how close the data are to MCAR and how big a problem the deviations from MCAR are likely to be. The bigger the deviation the stronger the case for using multiple imputation rather than complete cases analysis.</p>
<p></p>
<p>By definition you cannot determine whether data are MNAR  by looking at the observed values. Think carefully about how the data was collected and consider whether some values of the variables might make the data more or less likely to be observed. For example, people with very high or very low incomes might be less willing to disclose them, or people with high BMIs. People with a strong interest in the topic of a survey might be more likely to respond than those who care less. Schools might try very hard to make sure  students they expect to do well take standardized tests but put much less effort into having students they expect to do poorly take them. In the last example, adding variables like grades or socioeconomic status that predict test performance and thus probability of taking the test might make the data plausibly MAR.</p>
<h3>Amount of Missing Data</h3>
<p>If you have low amounts of missing data (say, 1%) then multiple imputation and complete cases analysis are very likely to give essentially the same results, and complete cases analysis is much easier.</p>
<p>On the other hand, if you have very large amounts of missing data then your final results will be driven in large part by your imputation model rather than the observed data. There's no consensus on how much missing data is too much for multiple imputation, but certainly imputing 50% of your data is asking for trouble.</p>
<p>Multiple imputation is useful somewhere in between.</p>
<p>Next: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_models.htm">Creating Imputation Models</a></p>
<p>Previous: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_intro.htm">Introduction</a></p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Multiple Imputation in Stata: Estimating</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p><em>This article is part of the Multiple Imputation in Stata series. For a list of topics covered by this series, see the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_intro.htm">Introduction</a>.</em></p>
<p>In most cases, the hard work of using multiple imputation comes in the imputation process. Once the imputations are created and checked, Stata makes estimation using the imputed data relatively easy.</p>
<h2>mi estimate</h2>
<p>The main command for running estimations on imputed data is <span class="InputCode">mi estimate</span>. It is a prefix command, like <span class="InputCode">svy</span> or <span class="InputCode">by</span>, meaning that it goes in front of whatever estimation command you're running.</p>
<p>The <span class="InputCode">mi estimate</span> command first runs the estimation command on each imputation separately. It then combines the results using Rubin's rules and displays the output. Because the output is created by <span class="InputCode">mi estimate</span>, options that affect output, such as <span class="InputCode">or</span> to display odds ratios, must be applied to <span class="InputCode">mi estimate</span> rather than the estimation command. Thus:</p>
<p class="InputCode">mi estimate, or: logit y x</p>
<p><strong>not:</strong></p>
<p class="InputCode">mi estimate: logit y x, or</p>
<p><span class="InputCode">mi estimate</span> has a list of estimation commands for which it knows Rubin's rules are appropriate. If a command is not on that list, you can tell <span class="InputCode">mi estimate</span> to apply them anyway with the <span class="InputCode">cmdok</span> ("command ok") option. However, it is your responsibility to ensure that the results will be valid.                </p>
<h2 id="SubsamplesBasedonImputedVariables">Subsamples Based on Imputed Variables</h2>
<p>Consider a regression like:                </p>
<p class="InputCode">mi estimate: reg wage edu exp if race==1</p>
<p>If <span class="InputCode">race</span> is an imputed variable, then some observations  will likely have a one for <span class="InputCode">race</span> in some imputations and not others. Thus the subsample to be used will vary between imputations, and <span class="InputCode">mi estimate</span> will give you an error message.</p>
<p>You have two options at this point. One is to simply tell <span class="InputCode">mi estimate</span> to ignore the problem with the <span class="InputCode">esampvaryok</span> option. The Stata documentation says this may result in "may result in biased or inefficient estimates" but we don't have any guidance at this time as to the seriousness of the problem.</p>
<p class="InputCode">mi estimate, esampvaryok: reg wage edu exp if race==1</p>
<p>The other is to not use observations that have imputed values of the variables used to select the subsample. Hopefully you created indicator variables telling you which observations are missing which variables in the process of <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_decide.htm#AreMyDataMCARMARorMNAR">determining whether your data are MCAR, MCAR or MNAR</a> with:</p>
<p class="InputCode">misstable sum, gen(miss_)</p>
<p>If so, you can use those variables as part of your subsample selection:</p>
<p class="InputCode">mi estimate: reg wage edu exp if race==1 &amp; !miss_race</p>
<p>Of course this raises the same issues as complete cases analysis, though the effects will likely be smaller.</p>
<h2>Dropping Variables</h2>
<p>More rarely, you could run into problems with different imputations using different sets of variables. In our experience that's been the result of perfect prediction in some imputations and not others, which suggests problems with the model being run (such as too many categorical covariates for the number of observations available). But it can also arise from the estimation command choosing different base categories. In that case specifying the base category should fix the problem.</p>
<h2 id="Postestimation">Postestimation</h2>
<p>Postestimation with imputed data must be done with caution. Rubin's rules require certain assumptions to be valid, notably asymptotic normality, and if a quantity does not meet those assumptions then Rubin's rules cannot provide a valid estimate of it. Fortunately, regression coefficients do meet those assumptions. Some quantities can be estimated if they are transformed to make them approximately normal, such as R-squared values. Others simply cannot, such as likelihood ratio test statistics. See <a href="http://onlinelibrary.wiley.com.ezproxy.library.wisc.edu/doi/10.1002/sim.4067/pdf">White, Royston, and Wood</a> for a list of quantities that can and cannot be combined using Rubin's Rules. </p>
<p>Unlike standard estimation commands, <span class="InputCode">mi estimate</span> cannot save all the information needed for  postestimation tasks in the <span class="InputCode">e()</span> vector. Some tasks require the <span class="InputCode">e()</span> vector from the regression run on each completed data set. If you're planning to do postestimation, tell <span class="InputCode">mi estimate</span> to store the needed information in a small file with the <span class="InputCode">saving()</span> option:</p>
<p class="InputCode">mi estimate, saving(myestimates, replace): ... </p>
<p>This will create the file <span class="InputCode">myestimates.ster</span> in the current directory.</p>
<h3 id="TestsofCoefficients">Tests of Coefficients</h3>
<p>Hypothesis tests on coefficients can be performed using the <span class="InputCode">mi test</span> command. For testing whether coefficients are equal to zero, the syntax is the same as the regular <span class="InputCode">test</span> command. However, testing transformations or combinations of coefficients is more complicated—type <span class="InputCode">help mi test</span> for more information.</p>
<p>Likelihood ratio tests cannot be performed with multiply imputed data. However, if your goal is to test whether adding covariates  improves your basic model, you can test the hypothesis that the coefficients on all those additional covariates are jointly zero.</p>
<h3 id="Prediction">Prediction</h3>
<p>Predicted values can be treated as parameters to be estimated. Linear predictions meet the assumptions of Rubin's rules and thus they can be computed for each imputation and then combined as usual. This is done using the <span class="InputCode">mi predict</span> command, but <span class="InputCode">mi predict</span> needs the additional information contained in the estimates file saved by <span class="InputCode">mi estimate</span>. Thus the full command is:</p>
<p class="InputCode">mi predict <span class="Parameter">myprediction</span> using <span class="Parameter">myestimates</span></p>
<p>Predicted probabilities do not meet the assumptions of Rubin's rules. However, you can estimate predicted probabilities by first estimating the linear prediction using <span class="InputCode">mi predict</span> and then putting the result through an inverse-logit transformation:</p>
<p class="InputCode">mi predict linear_prediction using myestimates, xb<br/>
                  mi xeq: gen predicted_probability=invlogit(linear_prediction)</p>
<p>The <span class="InputCode">xb</span>  option tells <span class="InputCode">mi predict</span> to calculate the linear prediction even if the most recent regression involved probabilities.</p>
<h2 id="MonteCarloError">Monte Carlo Error and the Number of Imputations</h2>
<p>Since multiple imputation includes a random component, repeating the same analysis will give slightly different results each time (unless you set the seed of the random number generator). This is obviously an undesirable property, but acceptable as long as the amount of variation is small enough to be unimportant. The variation due to the random component is called the Monte Carlo error.</p>
<p><span class="InputCode">mi estimate</span> with the <span class="InputCode">mcerror</span> option will report an  estimate of the  Monte Carlo error in estimation results. The process for calculating it involves leaving out one imputation at a time. <a href="http://onlinelibrary.wiley.com.ezproxy.library.wisc.edu/doi/10.1002/sim.4067/pdf">White, Royston, and Wood</a> suggest the following guidelines for what constitutes an acceptable amount of Monte Carlo error:</p>
<ol>
<li>The Monte Carlo error of a coefficient should be less than or equal to 10% of its standard error</li>
<li>The Monte Carlo error of a coefficient's T-statistic should be less than or equal to 0.1</li>
<li>The Monte Carlo error of a coefficient's P-value should be less than or equal to 0.01 if the true P-value is 0.05, or 0.02 if the true P-value is 0.1</li>
</ol>
<p>If those conditions are not met, you should increase the number of imputations.</p>
<h2>Example</h2>
<p>Consider the example data set we imputed in an <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_impute.htm">earlier section</a>. It contains (fictional) <span class="InputCode">wage</span> data, which we will model using the covariates <span class="InputCode">exp</span>, <span class="InputCode">edu</span>, <span class="InputCode">female</span>, <span class="InputCode">urban</span> and <span class="InputCode">race</span>. Given what we found in the prior section, we will also interact <span class="InputCode">female</span> with <span class="InputCode">exp</span> and <span class="InputCode">edu</span>.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/mi1.dta">Data set to be analyzed (includes imputations)</a></p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/miexan.do">Do file that carries out the analysis</a></p>
<p>Complete cases analysis (obtained with <span class="InputCode">mi xeq 0:</span> since the data set contains imputations) gives the following results:</p>
<p class="InputCode"> mi xeq 0: reg wage  female##(c.exp i.edu) urban i.race</p>
<pre class="InputCode">
m=0 data:
-&gt; reg wage  female##(c.exp i.edu) urban i.race

      Source |       SS       df       MS              Number of obs =    1779
-------------+------------------------------           F( 12,  1766) =   98.93
       Model |  1.0350e+12    12  8.6247e+10           Prob &gt; F      =  0.0000
    Residual |  1.5396e+12  1766   871809267           R-squared     =  0.4020
-------------+------------------------------           Adj R-squared =  0.3979
       Total |  2.5746e+12  1778  1.4480e+09           Root MSE      =   29526

------------------------------------------------------------------------------
        wage |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    1.female |  -7033.218   4180.626    -1.68   0.093    -15232.71    1166.277
         exp |   2004.479    102.498    19.56   0.000     1803.449    2205.509
             |
         edu |
          2  |   10679.82   2634.031     4.05   0.000     5513.673    15845.96
          3  |   28279.73   2844.885     9.94   0.000     22700.03    33859.42
          4  |   51097.61   4591.212    11.13   0.000     42092.83    60102.39
             |
female#c.exp |
          1  |   -511.406   150.6243    -3.40   0.001    -806.8267   -215.9853
             |
  female#edu |
        1 2  |    -5736.4   4041.507    -1.42   0.156    -13663.04     2190.24
        1 3  |  -3876.886   4208.948    -0.92   0.357    -12131.93    4378.159
        1 4  |  -12072.54   5845.627    -2.07   0.039    -23537.62   -607.4622
             |
       urban |   4076.262   1577.229     2.58   0.010     982.8305    7169.694
             |
        race |
          1  |  -4409.319    1739.41    -2.53   0.011    -7820.838   -997.8001
          2  |  -4952.449   1790.243    -2.77   0.006    -8463.667   -1441.232
             |
       _cons |   31591.61   3200.808     9.87   0.000     25313.84    37869.38
------------------------------------------------------------------------------</pre>
<p>Compare with results using the imputations:</p>
<p class="InputCode">mi estimate, saving(miexan,replace): reg wage  female##(c.exp i.edu) urban i.race</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =          5
Linear regression                                 Number of obs   =       3000
                                                  Average RVI     =     0.3261
                                                  Largest FMI     =     0.3672
                                                  Complete DF     =       2987
DF adjustment:   Small sample                     DF:     min     =      35.46
                                                          avg     =     206.66
                                                          max     =     710.00
Model F test:       Equal FMI                     F(  12,  516.3) =     122.88
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
        wage |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    1.female |  -7637.933   3554.531    -2.15   0.034    -14676.67   -599.1996
         exp |   2003.071   82.22401    24.36   0.000     1841.639    2164.502
             |
         edu |
          2  |   12388.95   2190.569     5.66   0.000     8063.544    16714.36
          3  |   28619.32   2443.457    11.71   0.000     23764.93    33473.71
          4  |   51773.01   4248.138    12.19   0.000     43152.79    60393.23
             |
female#c.exp |
          1  |  -459.7754    130.917    -3.51   0.001    -719.6377   -199.9131
             |
  female#edu |
        1 2  |   -5981.89   3390.213    -1.76   0.080     -12676.1    712.3196
        1 3  |   -4640.03   3554.687    -1.31   0.194    -11672.93    2392.866
        1 4  |  -12926.75   5274.621    -2.45   0.018    -23517.89   -2335.615
             |
       urban |   4467.026   1465.326     3.05   0.004     1508.758    7425.294
             |
        race |
          1  |  -3221.866   1394.161    -2.31   0.021    -5960.173    -483.559
          2  |  -5977.193   1579.916    -3.78   0.000    -9123.292   -2831.093
             |
       _cons |   30617.76   2545.795    12.03   0.000     25614.32     35621.2
------------------------------------------------------------------------------</pre>
<p>The 95% confidence intervals are smaller, which is just enough to put the P-value of <span class="InputCode">female</span> under the .05 cutoff for "significance."</p>
<p>These results were calculated with just five imputations, which we suggested as a starting point. How much Monte Carlo error does this leave?</p>
<p class="InputCode">mi estimate, mcerr: reg wage  female##(c.exp i.edu) urban i.race</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =          5
Linear regression                                 Number of obs   =       3000
                                                  Average RVI     =     0.3261
                                                  Largest FMI     =     0.3672
                                                  Complete DF     =       2987
DF adjustment:   Small sample                     DF:     min     =      35.46
                                                          avg     =     206.66
                                                          max     =     710.00
Model F test:       Equal FMI                     F(  12,  516.3) =     122.88
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
        wage |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    1.female |  -7637.933   3554.531    -2.15   0.034    -14676.67   -599.1996
             |   614.5666   219.6112     0.22   0.018     810.9298    823.4979
             |
         exp |   2003.071   82.22401    24.36   0.000     1841.639    2164.502
             |    8.54593   1.876621     0.61   0.000     10.89688    7.715809
             |
             |
         edu |
          2  |   12388.95   2190.569     5.66   0.000     8063.544    16714.36
             |   347.7053   136.7152     0.24   0.000     182.1151    637.3576
             |
          3  |   28619.32   2443.457    11.71   0.000     23764.93    33473.71
             |   453.6681   191.0128     0.92   0.000     628.9118    692.7885
             |
          4  |   51773.01   4248.138    12.19   0.000     43152.79    60393.23
             |   1000.615   225.8875     0.44   0.000     494.3933    1592.194
             |
             |
female#c.exp |
          1  |  -459.7754    130.917    -3.51   0.001    -719.6377   -199.9131
             |   23.88987   6.507623     0.26   0.001     27.60468    30.27696
             |
             |
  female#edu |
        1 2  |   -5981.89   3390.213    -1.76   0.080     -12676.1    712.3196
             |   538.2219   166.8896     0.15   0.024     785.7183    522.4397
             |
        1 3  |   -4640.03   3554.687    -1.31   0.194    -11672.93    2392.866
             |   600.5342   309.3857     0.30   0.090     278.1348    1288.453
             |
        1 4  |  -12926.75   5274.621    -2.45   0.018    -23517.89   -2335.615
             |   1134.893   383.1226     0.21   0.012     1831.385    1154.305
             |
             |
       urban |   4467.026   1465.326     3.05   0.004     1508.758    7425.294
             |   331.6953   169.3962     0.58   0.006     715.5021    340.4445
             |
             |
        race |
          1  |  -3221.866   1394.161    -2.31   0.021    -5960.173    -483.559
             |   155.3758   26.42719     0.10   0.006     183.4623    145.6764
             |
          2  |  -5977.193   1579.916    -3.78   0.000    -9123.292   -2831.093
             |   305.4687   167.2478     0.61   0.001     251.5403    676.1527
             |
             |
       _cons |   30617.76   2545.795    12.03   0.000     25614.32     35621.2
             |   307.1376   84.98822     0.48   0.000     423.9462     281.053
------------------------------------------------------------------------------
Note: values displayed beneath estimates are Monte Carlo error estimates.</pre>
<p>A brief glance at the estimates for <span class="InputCode">female</span> shows this does not meet the suggested criteria: the Monte Carlo error on the coefficient is about 17% of the standard error rather than 10%, and the Monte Carlo error on the P-value is .018 when we'd want it to be less than .01 if we believe the true P-value is .05 or less. This suggests you should use more imputations.</p>
<p>Even if it turned out that the Monte Carlo error was acceptable, we'd still recommend using more imputations in this case. About 40% of the observations are missing values, so White, Royston, and Wood would suggest 40 imputations. While that may seem like a lot, the entire process from imputation to analysis (including many diagnostics) still ran in less than 15 minutes in our testing. Thus there's no reason not to use at least that many imputations when you're ready to produce final results.</p>
<p> Here are the results with 40 imputations, and you'll see that the Monte Carlo error now meets the guidelines:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         40
Linear regression                                 Number of obs   =       3000
                                                  Average RVI     =     0.2340
                                                  Largest FMI     =     0.2505
                                                  Complete DF     =       2987
DF adjustment:   Small sample                     DF:     min     =     494.37
                                                          avg     =     723.63
                                                          max     =    1046.30
Model F test:       Equal FMI                     F(  12, 2407.3) =     134.54
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
        wage |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    1.female |  -8444.446   3568.764    -2.37   0.018    -15450.36   -1438.528
             |   242.2941   70.94422     0.08   0.004      294.817    266.1893
             |
         exp |   1981.635   90.55428    21.88   0.000      1803.76    2159.509
             |   6.812604   2.379668     0.61   0.000     9.595854    6.819707
             |
             |
         edu |
          2  |   11896.82   2277.964     5.22   0.000     7423.364    16370.28
             |   164.9826    51.1388     0.14   0.000     200.7386    187.6542
             |
          3  |   28702.41   2405.316    11.93   0.000      23980.9    33423.92
             |   160.1252   56.74204     0.31   0.000     218.7132    170.9788
             |
          4  |   52000.64   3991.622    13.03   0.000        44158    59843.27
             |   310.4559   94.26391     0.35   0.000     426.0258    288.6724
             |
             |
female#c.exp |
          1  |  -435.3699   126.7969    -3.43   0.001     -684.175   -186.5648
             |   7.621008   1.908626     0.08   0.000     8.635785    8.400978
             |
             |
  female#edu |
        1 2  |  -5278.077   3463.037    -1.52   0.128    -12076.34    1520.188
             |   234.4022   91.09243     0.07   0.017     330.3471    259.2769
             |
        1 3  |  -4498.973   3525.953    -1.28   0.202    -11418.57    2420.619
             |   220.3948   79.45707     0.07   0.025     265.3468    277.5948
             |
        1 4  |   -11832.2   4963.477    -2.38   0.017    -21576.09   -2088.301
             |   336.8887   98.06887     0.08   0.004     383.8081    395.9982
             |
             |
       urban |   4301.578     1351.3     3.18   0.002     1648.864    6954.293
             |   91.36621   21.07064     0.09   0.000     104.6887    96.41936
             |
             |
        race |
          1  |  -3478.967   1540.658    -2.26   0.024    -6505.774   -452.1594
             |   118.6348   35.97534     0.11   0.006     120.0791    155.5802
             |
          2  |  -5657.502   1575.349    -3.59   0.000    -8751.359   -2563.646
             |   115.2371   35.82667     0.12   0.000     120.9807    149.2301
             |
             |
       _cons |   31156.05   2678.619    11.63   0.000     25898.25    36413.85
             |   176.8282   54.19707     0.23   0.000     177.3412    233.6436
------------------------------------------------------------------------------
Note: values displayed beneath estimates are Monte Carlo error estimates.</pre>
<p>One reasonable question is whether the interactions are actually required, and with unimputed data one might use a likelihood ratio test to answer it. With imputed data you'll instead test whether the coefficients on the interaction terms are jointly equal to zero:</p>
<p class="InputCode">mi test 1.female#c.exp 1.female#2.edu 1.female#3.edu 1.female#4.edu</p>
<pre class="InputCode">note: assuming equal fractions of missing information

 ( 1)  1.female#c.exp = 0
 ( 2)  1.female#2.edu = 0
 ( 3)  1.female#3.edu = 0
 ( 4)  1.female#4.edu = 0

       F(  4, 156.0) =    4.63
            Prob &gt; F =    0.0015</pre>
<p>Given that some of the terms are significantly different from zero on their own, it's no surprise that the joint test rejects the hypothesis that they are all zero.</p>
<p>Finally, if we wanted to calculate predicted wages, we would use the following:</p>
<p class="InputCode">mi predict wagehat using miexan</p>
<p>Note the use of the <span class="InputCode">miexan.ster</span> file (without needing to specify the extension) created by the initial <span class="InputCode">mi estimate</span> command. It contains the coefficients from the regressions on each completed data set, which are needed to form the individual predictions combined by <span class="InputCode">mi predict</span>.</p>
<p>Next: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm">Examples</a></p>
<p>Previous: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_manage.htm">Managing Multiply Imputed Data</a></p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Multiple Imputation Examples</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p><em>This article is part of the Multiple Imputation in Stata series. For a list of topics covered by this series, see the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_intro.htm">Introduction</a>.</em></p>
<p>This article contains examples that illustrate some of the issues involved in using multiple imputation. Articles in the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm">Multiple Imputation in Stata</a> series refer to these examples, and more discussion of the principles involved can be found in those articles. However, it is possible to read this article independently, or to just read about the particular example that interests you (see the list of examples below).</p>
<p> These examples are not intended to test the validity of the techniques used or rigorously compare their effectiveness. However, they should give you some intuition about how they work and their strengths and weaknesses. Some of the simulation parameters (and occasionally the seed for the random number generator) were chosen in order to highlight the issue at hand, but none of them are atypical of real-world situations. The data are generated randomly from standard distributions in such a way that the "right" answers are known and you can see how closely different techniques approach those answers.</p>
<p>A Stata do file is provided for each example, along with commentary and selected output (in this web page). The do file also generates the data set used, with a set seed for reproducibility. Our suggestion is that you open the do file in Stata's do file editor or your favorite text editor and read it in parallel with the discussion in the article. Please note that the example do files are not intended to demonstrate the entire process of multiple imputation—they don't always check the fit or convergence of the imputation models, for example, which are very important things to do in real world use of multiple imputation.</p>
<p>Each example concludes with a "Lessons Learned" section, but we'd like to highlight one overall lesson: <em>Multiple imputation can be a useful tool, but there are many ways to get it wrong and invalidate your results. Be very careful, and don't expect it to be quick and easy.</em></p>
<p>The examples are:</p>
<ol>
<li><a href="#Power">Power</a></li>
<li><a href="#MCARvsMARvsMNAR">MCAR vs. MAR vs. MNAR</a></li>
<li><a href="#ImputingtheDependentVariable">Imputing the Dependent Variable</a></li>
<li><a href="#Non-NormalData">Non-Normal Data</a></li>
<li><a href="#Transformations">Transformations</a></li>
<li><a href="#Non-Linearity">Non-Linearity</a></li>
<li><a href="#Interactions">Interactions</a></li>
</ol>
<p> </p>
<h2><a id="Power" name="Power"></a>Power</h2>
<p>The most common motivation for using multiple imputation is to try to increase the power of statistical tests by increasing the number of observations used (i.e. by not losing observations due to missing values). In our experience it rarely makes a large difference in practice. This example uses ideal circumstances to illustrate what extremely successful multiple imputation would look like.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/power.do">Code for this example</a></p>
<h3>Data</h3>
<p><strong>Observations</strong>: 1,000</p>
<p><strong>Variables</strong>:</p>
<ul>
<li><span class="InputCode">x1</span>-<span class="InputCode">x10</span> drawn from standard normal distribution (independently)</li>
<li><span class="InputCode">y</span> is the sum of all <span class="InputCode">x</span>'s, plus a normal error term</li>
</ul>
<p><strong>Missingness</strong>: Each value of the <span class="InputCode">x</span> variables has a 10% chance of being missing (MCAR).</p>
<p><strong>Right Answers</strong>: Regressing <span class="InputCode">y</span> on all the <span class="InputCode">x</span>'s, each <span class="InputCode">x</span> should have a coefficient of 1.</p>
<h3>Procedure</h3>
<p>Begin with complete cases analysis:</p>
<pre class="InputCode">      Source |       SS       df       MS              Number of obs =     369
-------------+------------------------------           F( 10,   358) =    6.71
       Model |  3882.86207    10  388.286207           Prob &gt; F      =  0.0000
    Residual |  20722.7734   358   57.884842           R-squared     =  0.1578
-------------+------------------------------           Adj R-squared =  0.1343
       Total |  24605.6355   368    66.86314           Root MSE      =  7.6082

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          x1 |    .733993   .4216582     1.74   0.083    -.0952453    1.563231
          x2 |   1.664231   .3867292     4.30   0.000      .903684    2.424777
          x3 |    1.51406   .3907875     3.87   0.000     .7455327    2.282588
          x4 |   .2801067    .395164     0.71   0.479    -.4970278    1.057241
          x5 |   .8524305   .4076557     2.09   0.037     .0507297    1.654131
          x6 |   .7704437    .413519     1.86   0.063     -.042788    1.583675
          x7 |   .6512155   .3938107     1.65   0.099    -.1232575    1.425689
          x8 |   .9173208   .3969585     2.31   0.021     .1366572    1.697984
          x9 |   .8736406   .4115488     2.12   0.034     .0642835    1.682998
         x10 |   .9222064   .4123417     2.24   0.026       .11129    1.733123
       _cons |  -.1999121   .3975523    -0.50   0.615    -.9817434    .5819192
------------------------------------------------------------------------------</pre>
<p>Note that although each <span class="InputCode">x</span> value has just a 10% chance of being missing, because we have ten <span class="InputCode">x</span> variables per observation almost 2/3 of the observations are missing at least one value and must be dropped. As a result, the standard errors are quite large, and the coefficients on four of the ten <span class="InputCode">x</span>'s are not significantly different from zero.</p>
<p>After multiple imputation, the same regression gives the following:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =       1000
                                                  Average RVI     =     0.1483
                                                  Largest FMI     =     0.2356
                                                  Complete DF     =        989
DF adjustment:   Small sample                     DF:     min     =     142.63
                                                          avg     =     425.17
                                                          max     =     928.47
Model F test:       Equal FMI                     F(  10,  802.0) =      18.13
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          x1 |    .793598   .2679515     2.96   0.003     .2657743    1.321422
          x2 |   1.133405   .2482047     4.57   0.000     .6460259    1.620785
          x3 |    1.33182   .2521916     5.28   0.000     .8364684    1.827172
          x4 |   1.159887   .2714759     4.27   0.000      .624309    1.695465
          x5 |   1.181207   .2901044     4.07   0.000      .607747    1.754667
          x6 |   1.305636   .2835268     4.60   0.000     .7466279    1.864645
          x7 |   .6258813   .2568191     2.44   0.015      .121268    1.130494
          x8 |   1.143631    .253376     4.51   0.000     .6461328     1.64113
          x9 |   1.112347   .2838261     3.92   0.000     .5520503    1.672644
         x10 |   1.053309   .2612807     4.03   0.000     .5397648    1.566854
       _cons |   .0305628   .2499378     0.12   0.903    -.4599457    .5210712
------------------------------------------------------------------------------</pre>
<p>The standard errors are much smaller than with complete cases analysis, and all of the coefficients are  significantly different from zero. This illustrates the primary motivation for using multiple imputation.</p>
<h3>Lessons Learned</h3>
<p>This data set is ideal for multiple imputation because it has large numbers of observations with partial data. Complete cases analysis must discard all these observations. Multiple imputation can use the information they contain to improve the results.</p>
<p>Note that the imputation model could not do a very good job of predicting the missing values of the <span class="InputCode">x</span>'s based on the observed data. The <span class="InputCode">x</span>'s are completely independent of each other and thus have no predictive power. <span class="InputCode">y</span> has some but not much: if you regress each <span class="InputCode">x</span> on <span class="InputCode">y</span> and all the other <span class="InputCode">x</span>'s (which is what the  imputation model used does), the R-squared values are all less than 0.1 and most are less than 0.03.  The success of multiple imputation does not depend on imputing the "right" values of the missing variables for individual observations, but rather on correctly modeling their distribution conditional on the observed data.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_decide.htm#Power">Return to the source article</a></p>
<h3></h3>
<h2><a id="MCARvsMARvsMNAR" name="MCARvsMARvsMNAR"></a>MCAR vs. MAR vs. MNAR</h2>
<p>Whether your data are Missing Completely at Random (probability of being missing does not depend on either observed or unobserved data), Missing at Random (probability of being missing depends only on observed data), or Missing Not at Random (probability of being missing depends on unobserved data) is very important in deciding how to analyze it. This example shows how complete cases analysis and multiple imputation respond to different mechanisms for missingness.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/missmech.do">Code for this example</a></p>
<h3>Data</h3>
<p><strong>Observations</strong>: 1,000</p>
<p><strong>Variables</strong>:</p>
<ul>
<li><span class="InputCode">x</span> is drawn from standard normal distribution</li>
<li><span class="InputCode">y</span> is x plus a normal error term</li>
</ul>
<p><strong>Missingness</strong>:</p>
<ul>
<li><span class="InputCode">y</span> is always observed</li>
<li>First run: probability of <span class="InputCode">x</span> being missing is 10% for all observations (MCAR)</li>
<li>Second run: probability of <span class="InputCode">x</span> being missing is proportional to <span class="InputCode">y</span> (MAR)</li>
<li>Third run: probability of <span class="InputCode">x</span> being missing is proportional to x (MNAR)</li>
</ul>
<p><strong>Right Answers</strong>: Regressing <span class="InputCode">y</span> on <span class="InputCode">x</span>, <span class="InputCode">x</span> should have a coefficient of 1.</p>
<h3>Procedure</h3>
<p>We'll analyze this data set three times, once when it is MCAR, once when it is MAR, and once when it is MNAR.</p>
<h4>MCAR</h4>
<p>In the first run, the data set is MCAR and both complete cases analysis and multiple imputation give unbiased results.</p>
<p>Complete cases analysis:</p>
<pre class="InputCode">      Source |       SS       df       MS              Number of obs =     882
-------------+------------------------------           F(  1,   880) =  932.47
       Model |   918.60551     1   918.60551           Prob &gt; F      =  0.0000
    Residual |  866.913167   880  .985128598           R-squared     =  0.5145
-------------+------------------------------           Adj R-squared =  0.5139
       Total |  1785.51868   881  2.02669543           Root MSE      =  .99254

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9842461   .0322319    30.54   0.000     .9209858    1.047506
       _cons |  -.0481664   .0334249    -1.44   0.150    -.1137683    .0174354
------------------------------------------------------------------------------
</pre>
<p>Multiple imputation:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =       1000
                                                  Average RVI     =     0.1112
                                                  Largest FMI     =     0.1583
                                                  Complete DF     =        998
DF adjustment:   Small sample                     DF:     min     =     262.51
                                                          avg     =     541.98
                                                          max     =     821.46
Model F test:       Equal FMI                     F(   1,  821.5) =    1014.02
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9837383   .0308928    31.84   0.000     .9231003    1.044376
       _cons |  -.0377668   .0342445    -1.10   0.271    -.1051958    .0296621
------------------------------------------------------------------------------</pre>
<h4>MAR</h4>
<p>Now, consider the case where the probability of <span class="InputCode">x</span> being missing is proportional to <span class="InputCode">y</span> (which is always observed), making the data MAR. With MAR data, complete cases analysis is biased:</p>
<pre class="InputCode">      Source |       SS       df       MS              Number of obs =     652
-------------+------------------------------           F(  1,   650) =  375.85
       Model |  252.090968     1  252.090968           Prob &gt; F      =  0.0000
    Residual |  435.966034   650  .670716976           R-squared     =  0.3664
-------------+------------------------------           Adj R-squared =  0.3654
       Total |  688.057002   651   1.0569232           Root MSE      =  .81897

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .6857927    .035374    19.39   0.000     .6163317    .7552538
       _cons |   -.558313   .0352209   -15.85   0.000    -.6274736   -.4891525
------------------------------------------------------------------------------</pre>
<p>However, multiple imputation gives unbiased estimates:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =       1000
                                                  Average RVI     =     0.4234
                                                  Largest FMI     =     0.3335
                                                  Complete DF     =        998
DF adjustment:   Small sample                     DF:     min     =      78.63
                                                          avg     =      90.20
                                                          max     =     101.78
Model F test:       Equal FMI                     F(   1,   78.6) =     750.78
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9909115   .0361642    27.40   0.000     .9189232      1.0629
       _cons |  -.0544014   .0366092    -1.49   0.140    -.1270175    .0182146
------------------------------------------------------------------------------</pre>
<h4>MNAR</h4>
<p>Finally  consider the case where the probability of <span class="InputCode">x</span> being missing proportional to <span class="InputCode">x</span>. This makes the data missing not at random (MNAR), and with MNAR data both complete cases analysis and multiple imputation are biased.</p>
<p>Complete cases analysis:</p>
<pre class="InputCode">      Source |       SS       df       MS              Number of obs =     679
-------------+------------------------------           F(  1,   677) =  463.29
       Model |   464.94386     1   464.94386           Prob &gt; F      =  0.0000
    Residual |  679.422756   677  1.00357866           R-squared     =  0.4063
-------------+------------------------------           Adj R-squared =  0.4054
       Total |  1144.36662   678  1.68785636           Root MSE      =  1.0018

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   1.093581   .0508074    21.52   0.000     .9938225     1.19334
       _cons |   .0420364   .0469578     0.90   0.371     -.050164    .1342368
------------------------------------------------------------------------------</pre>
<p>Multiple imputation:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =       1000
                                                  Average RVI     =     0.3930
                                                  Largest FMI     =     0.3425
                                                  Complete DF     =        998
DF adjustment:   Small sample                     DF:     min     =      74.96
                                                          avg     =     144.00
                                                          max     =     213.04
Model F test:       Equal FMI                     F(   1,   75.0) =     561.03
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   1.223772   .0516665    23.69   0.000     1.120846    1.326698
       _cons |   .3816984   .0402614     9.48   0.000     .3023366    .4610602
------------------------------------------------------------------------------</pre>
<p>Complete cases analysis actually does better with this particular data set, but that's not true in general.</p>
<h3>Lessons Learned</h3>
<p>Always investigate whether your data set is plausibly MCAR or MAR. (For example, run logits on indicators of missingness and see if anything predicts it—if it does the data set is MAR rather than MCAR.) If your data set is MAR,  consider using multiple imputation rather than complete cases analysis.</p>
<p> MNAR, by definition, cannot be detected by looking at the observed data. You'll have to think carefully about how the data was collected and consider whether some values of the variables might make the data more or less likely to be observed. Unfortunately, the options for working with MNAR data are limited.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_decide.htm#missmech">Return to the source article</a></p>
<h2><a id="ImputingtheDependentVariable" name="ImputingtheDependentVariable"></a>Imputing the Dependent Variable</h2>
<p>Many researchers believe it is inappropriate to use imputed values of the dependent variable in the analysis model, especially if the variables used in the imputation model are the same as the variables used in the analysis model. The thinking is that the imputed values add no information since they were generated using the model that is being analyzed.</p>
<p>Unfortunately, this sometimes is misunderstood as meaning that the dependent variable should not be included in the imputation model. That would be a major mistake.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/imputey.do">Complete code for this example</a></p>
<h3>Data</h3>
<p><strong>Observations</strong>: 1,000</p>
<p><strong>Variables</strong>:</p>
<ul>
<li><span class="InputCode">x1</span>-<span class="InputCode">x3</span> drawn from standard normal distribution (independently)</li>
<li><span class="InputCode">y</span> is the sum of all <span class="InputCode">x</span>'s, plus a normal error term</li>
</ul>
<p><strong>Missingness</strong>: <span class="InputCode">y</span> and <span class="InputCode">x1-x3</span> have a 20% probability of being missing (MCAR).</p>
<p><strong>Right Answers</strong>: Regressing <span class="InputCode">y</span> on <span class="InputCode">x1-x3</span>, the coefficient on each should be 1.</p>
<h3>Procedure</h3>
<p>The following are the results of complete cases analysis:</p>
<pre class="InputCode">      Source |       SS       df       MS              Number of obs =    4079
-------------+------------------------------           F(  3,  4075) = 3944.41
       Model |  12264.8582     3  4088.28608           Prob &gt; F      =  0.0000
    Residual |  4223.63457  4075  1.03647474           R-squared     =  0.7438
-------------+------------------------------           Adj R-squared =  0.7437
       Total |  16488.4928  4078  4.04327926           Root MSE      =  1.0181

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          x1 |   .9834974   .0160921    61.12   0.000     .9519481    1.015047
          x2 |   1.000172   .0160265    62.41   0.000     .9687511    1.031593
          x3 |   1.003089   .0159724    62.80   0.000     .9717744    1.034404
       _cons |  -.0003362   .0159445    -0.02   0.983    -.0315962    .0309238
------------------------------------------------------------------------------</pre>
<p>If you leave <span class="InputCode">y</span> out of the imputation model, imputing only <span class="InputCode">x1-x3</span>, the coefficients on the <span class="InputCode">x</span>'s in the final model are biased towards zero:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =       7993
                                                  Average RVI     =     0.4014
                                                  Largest FMI     =     0.3666
                                                  Complete DF     =       7989
DF adjustment:   Small sample                     DF:     min     =      72.62
                                                          avg     =     119.53
                                                          max     =     161.84
Model F test:       Equal FMI                     F(   3,  223.0) =    1761.75
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          x1 |   .7946113   .0185175    42.91   0.000     .7580219    .8312007
          x2 |   .7983325   .0199962    39.92   0.000     .7584766    .8381885
          x3 |   .8001109   .0193738    41.30   0.000     .7616436    .8385782
       _cons |   .0041643   .0183891     0.23   0.821    -.0321492    .0404778
------------------------------------------------------------------------------</pre>
<p>Why is that? An easy way to see the problem is to look at correlations. First, the correlation between <span class="InputCode">y</span> and <span class="InputCode">x1</span> in the unimputed data:</p>
<pre class="InputCode">            |        y       x1
-------------+------------------
           y |   1.0000
          x1 |   0.5032   1.0000</pre>
<p>Now, the correlation between <span class="InputCode">y</span> and <span class="InputCode">x1</span> for those observations where <span class="InputCode">x1</span> is imputed (this is calculated for the first imputation, but the others are similar):</p>
<pre class="InputCode">             |        y       x1
-------------+------------------
           y |   1.0000
          x1 |  -0.0073   1.0000</pre>
<p>Because <span class="InputCode">y</span> is not included in the imputation model, the imputation model creates values of <span class="InputCode">x1</span> (and <span class="InputCode">x2</span> and <span class="InputCode">x3</span>) which are not correlated with <span class="InputCode">y</span>. This does not match the observed data. It also biases the results of the final model by adding observations in which <span class="InputCode">y</span> really is unrelated to <span class="InputCode">x1</span>, <span class="InputCode">x2,</span> and <span class="InputCode">x3</span>.</p>
<p>This problem goes away if <span class="InputCode">y</span> is included in the imputation model. Given the nature of chained equations, this means that values of <span class="InputCode">y </span>must be imputed. However, you're under no obligation to use those values in your analysis model. Simply create an indicator variable for "<span class="InputCode">y</span> is missing in the observed data," which can be done automatically with <span class="InputCode">misstable sum, gen(miss_)</span>, and then add <span class="InputCode">if !miss_y</span> to the regression command. This restricts the regression to those observations where <span class="InputCode">y</span> is actually observed, so any imputed values of <span class="InputCode">y</span> are not used. Here are the results:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =       7993
                                                  Average RVI     =     0.6402
                                                  Largest FMI     =     0.5076
                                                  Complete DF     =       7989
DF adjustment:   Small sample                     DF:     min     =      38.27
                                                          avg     =      82.39
                                                          max     =     182.83
Model F test:       Equal FMI                     F(   3,  137.9) =    4691.72
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          x1 |   .9852388   .0150348    65.53   0.000     .9550372     1.01544
          x2 |   .9923264   .0158077    62.77   0.000     .9603327     1.02432
          x3 |   .9936324   .0129186    76.91   0.000     .9681437    1.019121
       _cons |  -.0015309   .0145755    -0.11   0.917    -.0306997    .0276378
------------------------------------------------------------------------------</pre>
<p>On the other hand, using the imputed values of <span class="InputCode">y</span> turns out to make almost no difference in this case (see the complete code).</p>
<h3>Lessons Learned</h3>
<p>Always include the dependent variable in your imputation model. Whether you should use imputed values of the dependent variable in your analysis model is unclear, but always impute them.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_models.htm#depvar">Return to the source article</a></p>
<h2><a id="Non-NormalData" name="Non-NormalData"></a>Non-Normal Data</h2>
<p>The obvious way to impute a continuous variable is regression (i.e. the <span class="InputCode">regress</span> command in <span class="InputCode">mi impute chained</span>). However, it applies a normal error term. What happens if a variable is not at all normally distributed?</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/nonnormal.do">Complete code for this example</a></p>
<h3>Data</h3>
<p><strong>Observations</strong>: 10,000</p>
<p><strong>Variables</strong>:</p>
<ul>
<li><span class="InputCode">g</span> is a binary (1/0) variable with a 50% probability of being 1</li>
<li><span class="InputCode">x</span> is drawn from the standard normal distribution, then 5 is added if <span class="InputCode">g</span> is 1. Thus <span class="InputCode">x</span> is bimodal.</li>
<li><span class="InputCode">y</span> is <span class="InputCode">x</span> plus a normal error term</li>
</ul>
<p><strong>Missingness</strong>: Both <span class="InputCode">y</span> and <span class="InputCode">x</span> have a 10% probability of being missing (MCAR).</p>
<p><strong>Right Answers</strong>: Regressing <span class="InputCode">y</span> on  <span class="InputCode">x</span>, <span class="InputCode">x</span> should have a coefficient of 1.</p>
<h3>Procedure</h3>
<p>Given the way <span class="InputCode">x</span> was constructed, it has a bimodal distribution and is definitely not normal:</p>
<p><img alt="Original distribution of x" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/nonnormal1.png" width="585"/></p>
<p>The obvious imputation model (regress <span class="InputCode">x</span> on <span class="InputCode">y</span>) captures some of this bimodality because of the influence of <span class="InputCode">y</span>. However, the error term is normal and does not take into account the non-normal distribution of the data. Thus this model is too likely to create imputed values that are near 2.5 (the "valley" between the two "peaks") and the distribution of the imputed data is "too normal":</p>
<p><img alt="x imputed with regression--too normal" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/nonnormal2.png" width="585"/></p>
<p>However, the regression results are quite good:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =      10000
                                                  Average RVI     =     0.2471
                                                  Largest FMI     =     0.1977
                                                  Complete DF     =       9998
DF adjustment:   Small sample                     DF:     min     =     238.92
                                                          avg     =     414.68
                                                          max     =     590.43
Model F test:       Equal FMI                     F(   1,  590.4) =   63916.86
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   1.001225   .0039603   252.82   0.000     .9934473    1.009003
       _cons |  -.0068119   .0152489    -0.45   0.655    -.0368514    .0232276
------------------------------------------------------------------------------</pre>
<p>Replacing regression with Predictive Mean Matching gives a much better fit. PMM begins with regression, but it then finds the observed value of <span class="InputCode">x</span> that is the nearest match. Because there are fewer observed values in the "valley" PMM is less likely to find a match there, resulting in a distribution that is closer to the original:</p>
<p><img alt="x imputed with pmm--much better fit" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/nonnormal3.png" width="585"/></p>
<p>Regression results after imputing with PMM are also good:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =      10000
                                                  Average RVI     =     0.2885
                                                  Largest FMI     =     0.2698
                                                  Complete DF     =       9998
DF adjustment:   Small sample                     DF:     min     =     131.79
                                                          avg     =     148.15
                                                          max     =     164.50
Model F test:       Equal FMI                     F(   1,  131.8) =   53733.02
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9976108   .0043037   231.80   0.000     .9890976    1.006124
       _cons |   .0105094   .0156034     0.67   0.502    -.0202993    .0413181
------------------------------------------------------------------------------</pre>
<p>On the other hand, <span class="InputCode">x</span> is distributed normally within each <span class="InputCode">g</span> group. If you impute the two groups separately then  ordinary regression fits the data quite well:</p>
<p><img alt="x imputed with regression, but separately for the two groups" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/nonnormal4.png" width="585"/></p>
<p>The regression results are also good:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =      10000
                                                  Average RVI     =     0.6503
                                                  Largest FMI     =     0.4656
                                                  Complete DF     =       9998
DF adjustment:   Small sample                     DF:     min     =      45.53
                                                          avg     =      63.49
                                                          max     =      81.45
Model F test:       Equal FMI                     F(   1,   81.4) =   48664.75
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9996345   .0045314   220.60   0.000     .9906191     1.00865
       _cons |   .0032657   .0183149     0.18   0.859    -.0336105    .0401419
------------------------------------------------------------------------------</pre>
<h3>Lessons Learned</h3>
<p>PMM can be a very effective tool for imputing non-normal data. On the other hand, if you can identify groups whose data may vary in systematically different ways, consider imputing them separately.</p>
<p>However, the regression results were uniformly good, even when the data were imputed using the original regression model where the distribution of the imputed values didn't match the distribution of the observed values very well. In part this is because we had a large number of observations and a simple model, but in general the relationship between face validity and getting valid estimates using the imputed data is   unclear.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_models.htm#nonnorm">Return to the source article</a></p>
<h2><a id="Transformations" name="Transformations"></a>Transformations</h2>
<p>Given non-normal data, it's appealing to try to transform it in a way that makes it more normal. But what if the other variables in the model are related to the original values of the variable rather than the transformed values?</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/transform.do">Complete code for this example</a></p>
<h3>Data</h3>
<p><strong>Observations</strong>: 10,000</p>
<p><strong>Variables</strong>:</p>
<ul>
<li><span class="InputCode">x1</span> is drawn from the standard normal distribution, then exponentiated. Thus it is log-normal</li>
<li><span class="InputCode">x2</span> is drawn from the standard normal distribution</li>
<li><span class="InputCode">y</span> is the sum of <span class="InputCode">x1</span>, <span class="InputCode">x2</span> and  a normal error term</li>
</ul>
<p><strong>Missingness</strong>: <span class="InputCode">x1</span>, <span class="InputCode">x2</span> and <span class="InputCode">y</span> have a 10% probability of being missing (MCAR)</p>
<p><strong>Right Answers</strong>: Regressing <span class="InputCode">y</span> on  <span class="InputCode">x1</span> and <span class="InputCode">x2</span>, both should have a coefficient of 1.</p>
<h3>Procedure</h3>
<p>First, complete cases analysis for comparison. It does quite well, as we'd expect with MCAR data (and a simple model with lots of observations):</p>
<pre class="InputCode">      Source |       SS       df       MS              Number of obs =    7299
-------------+------------------------------           F(  2,  7296) =19024.33
       Model |  38421.4045     2  19210.7023           Prob &gt; F      =  0.0000
    Residual |  7367.47472  7296  1.00979643           R-squared     =  0.8391
-------------+------------------------------           Adj R-squared =  0.8391
       Total |  45788.8793  7298  6.27416816           Root MSE      =  1.0049

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          x1 |   1.001594   .0057231   175.01   0.000     .9903751    1.012813
          x2 |   .9973623   .0117407    84.95   0.000     .9743471    1.020377
       _cons |   .0035444   .0149663     0.24   0.813     -.025794    .0328827
------------------------------------------------------------------------------</pre>
<p>This data set presents a dilemma for imputation: <span class="InputCode">x1</span> can be made normal simply by taking its log, but <span class="InputCode">y</span> is related to <span class="InputCode">x1</span>, not the log of <span class="InputCode">x1</span>. Regressing <span class="InputCode">ln(x1)</span> on <span class="InputCode">x2</span> and <span class="InputCode">y</span> (as the obvious imputation model will do) results in the following plot of residuals vs. fitted values (<span class="InputCode">rvfplot</span>):</p>
<p><img alt="rvfplot--the pattern indicates that the model is mispecified" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/transform1.png" width="585"/></p>
<p>If the model were specified correctly, we'd expect the points to be randomly distributed around the <span class="InputCode">y</span> axis regardless of their <span class="InputCode">x</span> location. Clearly that's not the case.</p>
<p>The obvious way to impute this data would be to:</p>
<ol>
<li>Log transform <span class="InputCode">x1</span> by creating <span class="InputCode">lnx1 = ln(x1)</span></li>
<li>Impute <span class="InputCode">lnx1</span> using regression</li>
<li>Create the passive variable <span class="InputCode">ix1 = exp(lnx1)</span> to reverse the transformation</li>
</ol>
<p>Here are the regression results after doing so:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =      10000
                                                  Average RVI     =   196.4421
                                                  Largest FMI     =     0.9987
                                                  Complete DF     =       9997
DF adjustment:   Small sample                     DF:     min     =       5.89
                                                          avg     =     242.41
                                                          max     =     701.09
Model F test:       Equal FMI                     F(   2,   12.8) =       3.84
Within VCE type:          OLS                     Prob &gt; F        =     0.0493

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         ix1 |    .014417   .0213705     0.67   0.525    -.0381112    .0669452
          x2 |   .9951566   .0235333    42.29   0.000     .9489525    1.041361
       _cons |   1.583044   .0384844    41.13   0.000     1.502835    1.663253
------------------------------------------------------------------------------</pre>
<p>As you see, the coefficient on <span class="InputCode">ix1</span> (imputed untransformed <span class="InputCode">x1</span>) is very strongly biased towards zero. Now try imputing <span class="InputCode">lnx1</span> using Predictive Mean Matching rather than regression:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =      10000
                                                  Average RVI     =     0.7058
                                                  Largest FMI     =     0.6292
                                                  Complete DF     =       9997
DF adjustment:   Small sample                     DF:     min     =      24.80
                                                          avg     =      53.56
                                                          max     =      74.84
Model F test:       Equal FMI                     F(   2,   52.8) =    8286.63
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         ix1 |   .9554301    .009311   102.61   0.000     .9362457    .9746144
          x2 |   .9537128   .0146582    65.06   0.000     .9245112    .9829144
       _cons |   .0843407   .0192948     4.37   0.000     .0457588    .1229226
------------------------------------------------------------------------------</pre>
<p>Now both coefficients are biased towards zero, though not nearly as strongly.</p>
<p>What if we simply ignore the fact that <span class="InputCode">x1</span> is not normally distributed and impute it directly? The results are actually  better:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =      10000
                                                  Average RVI     =     0.3873
                                                  Largest FMI     =     0.3401
                                                  Complete DF     =       9997
DF adjustment:   Small sample                     DF:     min     =      84.25
                                                          avg     =     179.94
                                                          max     =     337.16
Model F test:       Equal FMI                     F(   2,  133.4) =   17538.58
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          x1 |   .9980159   .0060605   164.68   0.000     .9859646    1.010067
          x2 |   .9912113   .0117879    84.09   0.000      .967869    1.014554
       _cons |   .0108679   .0140238     0.77   0.439    -.0167172     .038453
------------------------------------------------------------------------------</pre>
<p>This clearly works well, but raises the question of face validity. Here is a kernel density plot of the observed values of <span class="InputCode">x1</span>:</p>
<p><img alt="kdensity of observed, log-normal data" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/transform2.png" width="585"/></p>
<p>Here is a kernel density plot of the imputed values of <span class="InputCode">x1</span>:</p>
<p><img alt="kdensity of imputed data--many values &lt;0, too spread out" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/transform3.png" width="585"/></p>
<p>The obvious problem is that the imputed values of <span class="InputCode">x1</span> are frequently less than zero while the observed values of <span class="InputCode">x1</span> are always positive. This would be especially problematic if you thought you might want to use a log transform later in the process.</p>
<p>Given the constraint that <span class="InputCode">x1</span> cannot be less than zero, <span class="InputCode">truncreg</span> seems like a plausible alternative. Unfortunately, <span class="InputCode">truncreg</span> fails to converge for this data set (for reasons not yet clear to us). But PMM will also honor that constraint:</p>
<p><img alt="kdensity plot of imputed x1 using PMM" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/transform5.png" width="585"/></p>
<p>Here are the regression results with PMM:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =      10000
                                                  Average RVI     =     0.3702
                                                  Largest FMI     =     0.3664
                                                  Complete DF     =       9997
DF adjustment:   Small sample                     DF:     min     =      72.89
                                                          avg     =     147.11
                                                          max     =     193.33
Model F test:       Equal FMI                     F(   2,  145.1) =   17890.15
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          x1 |   1.000568   .0056325   177.64   0.000     .9894588    1.011677
          x2 |   .9977033    .012447    80.16   0.000     .9728958    1.022511
       _cons |    .009171    .014612     0.63   0.531    -.0196674    .0380094
------------------------------------------------------------------------------</pre>
<h3>Lessons Learned</h3>
<p>The lesson of this example is not that you should never transform covariates. Keep in mind that what makes transforming <span class="InputCode">x1</span> problematic in this example is that the relationship between <span class="InputCode">y</span> and <span class="InputCode">x1</span> is known to be linear.</p>
<p> The real lesson is that misspecification in your imputation model can cause problems in your analysis model. Be sure to run the regressions implied by your imputation models separately and check them for misspecification. A secondary lesson is that PMM can work well for non-normal data.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_models.htm#transform">Return to the source article</a></p>
<h2><a id="Non-Linearity" name="Non-Linearity"></a>Non-Linearity</h2>
<p>Non-linear terms in your analysis model present a major challenge in creating an imputation model, because the non-linear relationship between variables can't be easily inverted.</p>
<p>Note: in this example we'll frequently use Stata's syntax for interactions to create squared terms. A varlist of <span class="InputCode">c.x##c.x</span> is equivalent to the  varlist  <span class="InputCode">x  x2</span> where <span class="InputCode">x2</span> is defined by <span class="InputCode">gen x2=x^2</span>. The squared term shows up in the output as <span class="InputCode">c.x#c.x</span>. Using <span class="InputCode">c.x##c.x</span> is convenient because you don't have to create a separate variable, and because in that form post-estimation commands can take into account the fact that <span class="InputCode">x</span> and <span class="InputCode">x^2</span> are not independent variables.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/nonlinear.do">Complete code for this example</a></p>
<h3>Data</h3>
<p><strong>Observations</strong>: 1,000</p>
<p><strong>Variables</strong>:</p>
<ul>
<li><span class="InputCode">x</span> is drawn from the standard normal distribution</li>
<li><span class="InputCode">y</span> is <span class="InputCode">x</span> plus <span class="InputCode">x^2</span> plus a normal error term</li>
</ul>
<p><strong>Missingness</strong>: <span class="InputCode">y</span> and <span class="InputCode">x</span> have a 10% probability of being missing (MCAR).</p>
<p><strong>Right Answers</strong>: Regressing <span class="InputCode">y</span> on  <span class="InputCode">x</span> and <span class="InputCode">x^2</span>, both should have a coefficient of 1.</p>
<h3>Procedure</h3>
<p>First, complete cases analysis for comparison:</p>
<pre class="InputCode">      Source |       SS       df       MS              Number of obs =     815
-------------+------------------------------           F(  2,   812) = 1184.66
       Model |   2350.4449     2  1175.22245           Prob &gt; F      =  0.0000
    Residual |  805.533089   812  .992035824           R-squared     =  0.7448
-------------+------------------------------           Adj R-squared =  0.7441
       Total |  3155.97799   814  3.87712284           Root MSE      =  .99601

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9798841   .0344861    28.41   0.000     .9121917    1.047577
             |
     c.x#c.x |   .9948393   .0244134    40.75   0.000     .9469185     1.04276
             |
       _cons |  -.0047983   .0429475    -0.11   0.911    -.0890994    .0795029
------------------------------------------------------------------------------</pre>
<p>The obvious thing to do is to impute <span class="InputCode">x</span>, then allow <span class="InputCode">x^2</span> to be a passive function of <span class="InputCode">x</span> (it makes no difference whether you create a variable for <span class="InputCode">x^2</span> using <span class="InputCode">mi passive</span> or allow Stata to do it for you by putting <span class="InputCode">c.x##c.x</span> in your regression). However, this means that the imputation model for <span class="InputCode">x</span> simply regresses <span class="InputCode">x</span> on <span class="InputCode">y</span>. This is misspecified, as you can see from an <span class="InputCode">rvfplot</span>:</p>
<p><img alt="rvfplot shows that the model is misspecified" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/nonlinear.png" width="585"/></p>
<p>Here are the regression results. Because the imputation model is misspecified , the coefficient on <span class="InputCode">x^2</span> (<span class="InputCode">c.x#c.x</span>) is biased:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =       1000
                                                  Average RVI     =     0.9729
                                                  Largest FMI     =     0.5328
                                                  Complete DF     =        997
DF adjustment:   Small sample                     DF:     min     =      32.79
                                                          avg     =      37.89
                                                          max     =      46.68
Model F test:       Equal FMI                     F(   2,   50.2) =     351.77
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9586506   .0570451    16.81   0.000     .8427484    1.074553
             |
     c.x#c.x |    .844537   .0409527    20.62   0.000     .7611974    .9278765
             |
       _cons |   .1986436   .0662195     3.00   0.004     .0654027    .3318845
------------------------------------------------------------------------------</pre>
<p>Next  impute <span class="InputCode">x</span> using PMM. It's an improvement but still biased:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =       1000
                                                  Average RVI     =     0.3974
                                                  Largest FMI     =     0.3984
                                                  Complete DF     =        997
DF adjustment:   Small sample                     DF:     min     =      56.92
                                                          avg     =     101.37
                                                          max     =     134.94
Model F test:       Equal FMI                     F(   2,  167.9) =     763.23
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9511347    .041403    22.97   0.000     .8691019    1.033167
             |
     c.x#c.x |   .9170003    .027871    32.90   0.000     .8618798    .9721207
             |
       _cons |   .1160666   .0565846     2.05   0.045     .0027546    .2293787
------------------------------------------------------------------------------</pre>
<p>The <span class="InputCode">include()</span> option of <span class="InputCode">mi impute chained</span> is usually used to add variables to the imputation model of an individual variable, but it can also accept expressions. Does adding <span class="InputCode">x^2</span> to the imputation for <span class="InputCode">y</span> with <span class="InputCode">(regress, include((x^2))) y</span> fix the problem? Unfortunately not, though it helps:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =       1000
                                                  Average RVI     =     1.1210
                                                  Largest FMI     =     0.7256
                                                  Complete DF     =        997
DF adjustment:   Small sample                     DF:     min     =      17.48
                                                          avg     =      70.29
                                                          max     =     118.18
Model F test:       Equal FMI                     F(   2,   39.1) =     377.13
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9629371   .0452655    21.27   0.000     .8727678    1.053106
             |
     c.x#c.x |   .8874316   .0475562    18.66   0.000     .7873051    .9875581
             |
       _cons |    .125851    .053619     2.35   0.021     .0196724    .2320296
------------------------------------------------------------------------------</pre>
<p>The imputation model for <span class="InputCode">y</span> has been fixed, but the imputation model for <span class="InputCode">x</span> is still misspecified and this still biases the results of the analysis model. Unfortunately, the true dependence of <span class="InputCode">x</span> on <span class="InputCode">y</span> does not match any standard regression model.</p>
<p>Next we'll use what White, Royston and Wood (<a href="http://onlinelibrary.wiley.com.ezproxy.library.wisc.edu/doi/10.1002/sim.4067/pdf">Multiple imputation using chained equations: Issues and guidance for practice</a>, Statistics in Medicine, November 2010) call the "Just Another Variable" approach. This creates a variable to contain <span class="InputCode">x^2</span> (<span class="InputCode">x2</span>) and then imputes both <span class="InputCode">x</span> and <span class="InputCode">x2</span> as if <span class="InputCode">x2</span> were "just another variable" rather than being determined by <span class="InputCode">x</span>. The obvious disadvantage is that in the imputed data <span class="InputCode">x2</span> is not equal to <span class="InputCode">x^2</span>, so it lacks face validity. But the regression results are a huge improvement:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =       1000
                                                  Average RVI     =     0.2375
                                                  Largest FMI     =     0.3033
                                                  Complete DF     =        997
DF adjustment:   Small sample                     DF:     min     =      92.98
                                                          avg     =     331.43
                                                          max     =     610.11
Model F test:       Equal FMI                     F(   2,  496.2) =    1330.20
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9905117   .0324469    30.53   0.000     .9267905    1.054233
          x2 |   1.005361   .0240329    41.83   0.000     .9580613    1.052662
       _cons |  -.0018354    .046395    -0.04   0.969    -.0939669    .0902961
------------------------------------------------------------------------------</pre>
<p>So would imputing <span class="InputCode">x</span> and <span class="InputCode">x2</span> using PMM be even better on the theory that PMM is better in sticky situations of all sorts? Not this time:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =       1000
                                                  Average RVI     =     0.3068
                                                  Largest FMI     =     0.3151
                                                  Complete DF     =        997
DF adjustment:   Small sample                     DF:     min     =      86.95
                                                          avg     =     201.95
                                                          max     =     312.22
Model F test:       Equal FMI                     F(   2,  163.9) =    1138.59
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9761798   .0346402    28.18   0.000     .9078863    1.044473
          x2 |   .9987715   .0258422    38.65   0.000      .947407    1.050136
       _cons |  -.0032279   .0416431    -0.08   0.938    -.0851645    .0787087
------------------------------------------------------------------------------</pre>
<h3>Lessons Learned</h3>
<p>Again, the principal lesson is that misspecification in your imputation model can lead to bias in your analysis model. Be very careful, and always check the fit of your imputation models.</p>
<p> This is a continuing research area, but there appears to be some agreement that imputing non-linear terms as passive variables should be avoided. "Just Another Variable" seems to be a good alternative.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_models.htm#nonlin">Return to the source article</a></p>
<h2><a id="Interactions" name="Interactions"></a>Interactions</h2>
<p>Interaction terms in your analysis model also lead to challenges in creating an imputation model because it should take into account the interaction in the model for the covariates being interacted.</p>
<p>Note: we'll again use Stata's syntax for interactions. Putting <span class="InputCode">g##c.x</span> in the covariate list of a regression regresses the dependent variable on <span class="InputCode">g</span>, continuous variable <span class="InputCode">x</span>, and the interaction between <span class="InputCode">g</span> and <span class="InputCode">x</span>. The interaction term shows up in the output as <span class="InputCode">g#c.x</span>.</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/interact.do">Complete code for this example</a></p>
<h3>Data</h3>
<p><strong>Observations</strong>: 10,000</p>
<p><strong>Variables</strong>:</p>
<ul>
<li><span class="InputCode">g</span> is a binary (1/0) variable with a 50% probability of being 1</li>
<li><span class="InputCode">x</span> is drawn from the standard normal distribution</li>
<li><span class="InputCode">y</span> is <span class="InputCode">x</span> plus <span class="InputCode">x*g</span> plus a normal error term. Put differently, <span class="InputCode">y</span> is <span class="InputCode">x</span> plus a normal  error term for group 0 and <span class="InputCode">2*x</span> plus a normal error term for group 1.</li>
</ul>
<p><strong>Missingness</strong>: <span class="InputCode">y</span> and <span class="InputCode">x</span> have a 20% probability of being missing (MCAR).</p>
<p><strong>Right Answers</strong>: Regressing <span class="InputCode">y</span> on  <span class="InputCode">x</span>, <span class="InputCode">g</span>, and the interaction between <span class="InputCode">x</span> and <span class="InputCode">g</span>, the coefficients for <span class="InputCode">x</span> and the interaction term should be 1, and the coefficient on <span class="InputCode">g</span> should be 0.</p>
<h3>Procedure</h3>
<p> The following are the results of complete cases analysis:</p>
<pre class="InputCode">      Source |       SS       df       MS              Number of obs =    6337
-------------+------------------------------           F(  3,  6333) = 5435.30
       Model |  16167.6705     3   5389.2235           Prob &gt; F      =  0.0000
    Residual |  6279.30945  6333  .991522099           R-squared     =  0.7203
-------------+------------------------------           Adj R-squared =  0.7201
       Total |    22446.98  6336   3.5427683           Root MSE      =  .99575

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         1.g |   -.012915   .0250189    -0.52   0.606    -.0619605    .0361305
           x |   .9842869    .017752    55.45   0.000     .9494869    1.019087
             |
       g#c.x |
          1  |   1.026204   .0249124    41.19   0.000     .9773671    1.075041
             |
       _cons |  -.0195648   .0177789    -1.10   0.271    -.0544175    .0152879
------------------------------------------------------------------------------</pre>
<p>The obvious way to impute this data is to impute <span class="InputCode">x</span> and create the interaction term passively. However that means <span class="InputCode">x</span> is regressed on <span class="InputCode">y</span> and <span class="InputCode">g</span> without any interactions. The result is biased estimates (in opposite directions) of the coefficients for both <span class="InputCode">x</span> and the interaction term:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =      10000
                                                  Average RVI     =     0.5760
                                                  Largest FMI     =     0.3048
                                                  Complete DF     =       9996
DF adjustment:   Small sample                     DF:     min     =     104.17
                                                          avg     =     130.43
                                                          max     =     182.89
Model F test:       Equal FMI                     F(   3,  205.4) =    4870.03
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         1.g |  -.0077291   .0251103    -0.31   0.759    -.0575089    .0420506
           x |   1.119861   .0182704    61.29   0.000     1.083631    1.156091
             |
       g#c.x |
          1  |   .7300249   .0239816    30.44   0.000     .6827087     .777341
             |
       _cons |  -.0148091   .0174857    -0.85   0.399    -.0494078    .0197896
------------------------------------------------------------------------------
</pre>
<p>An alternative approach is to create a variable for the interaction term, <span class="InputCode">gx</span>, and impute it separately from <span class="InputCode">x</span> (White, Royston and Wood's "Just Another Variable" approach). As in the previous example with a squared term, the obvious disadvantage is that the imputed value of <span class="InputCode">gx</span> will not in fact be <span class="InputCode">g*x</span> for any given observation. However, the results of the analysis model are much better:</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =      10000
                                                  Average RVI     =     0.4359
                                                  Largest FMI     =     0.3753
                                                  Complete DF     =       9996
DF adjustment:   Small sample                     DF:     min     =      69.58
                                                          avg     =     111.30
                                                          max     =     210.61
Model F test:       Equal FMI                     F(   3,  223.1) =    5908.62
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .9832953   .0159157    61.78   0.000     .9519209     1.01467
          xg |   1.018901   .0239421    42.56   0.000     .9713602    1.066443
           g |  -.0141319   .0247797    -0.57   0.570    -.0635346    .0352709
       _cons |  -.0087754   .0176281    -0.50   0.620    -.0439374    .0263866
------------------------------------------------------------------------------</pre>
<p>Another alternative is to impute the two groups separately. This allows <span class="InputCode">x</span> to have the proper relationship with <span class="InputCode">y</span> in both groups. However, it also allows the relationships between other variables to vary between groups. If you know that some relationships do not vary between groups you'll lose some precision by not taking advantage of this knowledge, but in the real world it's rare know such things.</p>
<pre class="InputCode">Multiple-imputation estimates                     Imputations     =         10
Linear regression                                 Number of obs   =      10000
                                                  Average RVI     =     0.3627
                                                  Largest FMI     =     0.3482
                                                  Complete DF     =       9996
DF adjustment:   Small sample                     DF:     min     =      80.48
                                                          avg     =     218.72
                                                          max     =     369.37
Model F test:       Equal FMI                     F(   3,  456.6) =    6783.26
Within VCE type:          OLS                     Prob &gt; F        =     0.0000

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         1.g |  -.0218404   .0243087    -0.90   0.372    -.0702117    .0265309
           x |   .9869436   .0156451    63.08   0.000     .9561207    1.017766
             |
       g#c.x |
          1  |   1.018534   .0214947    47.39   0.000      .976267    1.060802
             |
       _cons |  -.0076746   .0158825    -0.48   0.629    -.0390027    .0236534
------------------------------------------------------------------------------</pre>
<h3>Lessons Learned</h3>
<p>Once again, the principal lesson is that misspecification in your imputation model can lead to bias in your analysis model. Be very careful, and always check the fit of your imputation models.</p>
<p> </p>
<p>This is also an area of ongoing research, but there appears to be some agreement that imputing interaction effects passively is problematic. If your interactions are all a matter of variables having different effects for different groups, imputing each group separately is probably the obvious solution, though "Just Another Variable" also works well. If you have interactions between continuous variables then use "Just Another Variable."</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_models.htm#interact">Return to the source article</a></p>
<h2><a id="CaveatsandAcknowlegements" name="CaveatsandAcknowlegements"></a>Acknowledgements</h2>
<p>Some of these examples follow the discussion in White, Royston, and Wood. ("<a href="http://onlinelibrary.wiley.com.ezproxy.library.wisc.edu/doi/10.1002/sim.4067/pdf">Multiple imputation using chained equations: Issues and guidance for practice.</a>" Statistics in Medicine. 2011.) We highly recommend this article to anyone who is learning to use multiple imputation.</p>
<p>Next: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_readings.htm">Recommended Readings</a></p>
<p>Previous: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_estimate.htm">Estimating</a> </p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url>https://ssc.wisc.edu/sscc/pubs/mi/nonnormal1.png, https://ssc.wisc.edu/sscc/pubs/mi/nonnormal2.png, https://ssc.wisc.edu/sscc/pubs/mi/nonnormal3.png, https://ssc.wisc.edu/sscc/pubs/mi/nonnormal4.png, https://ssc.wisc.edu/sscc/pubs/mi/transform1.png, https://ssc.wisc.edu/sscc/pubs/mi/transform2.png, https://ssc.wisc.edu/sscc/pubs/mi/transform3.png, https://ssc.wisc.edu/sscc/pubs/mi/transform5.png, https://ssc.wisc.edu/sscc/pubs/mi/nonlinear.png</img_base_url>
</kb_document>
<kb_document>
<kb_title>Multiple Imputation in Stata: Imputing</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p class="intro"><em>This is part four of the Multiple Imputation in Stata series. For a list of topics covered by this series, see the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_intro.htm">Introduction</a>.</em></p>
<p>This section will talk you through the details of the imputation process. Be sure you've read at least the previous section, <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_models.htm">Creating Imputation Models</a>, so you have a sense of what issues can affect the validity of your results.</p>
<h2>Example Data</h2>
<p>To illustrate the process, we'll use a fabricated data set. Unlike those in the examples section, this data set is designed to have some resemblance to real world data. </p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/midata.do">Do file that creates this data set</a></p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/midata.dta">The data set as a Stata data file</a></p>
<p><strong>Observations</strong>: 3,000</p>
<p><strong>Variables</strong>:</p>
<ul>
<li><span class="InputCode">female</span> (binary)</li>
<li><span class="InputCode">race</span> (categorical, three values)</li>
<li><span class="InputCode">urban</span> (binary)</li>
<li><span class="InputCode">edu</span> (ordered categorical, four values)</li>
<li><span class="InputCode">exp</span> (continuous)</li>
<li><span class="InputCode">wage</span> (continuous)</li>
</ul>
<p><strong>Missingness</strong>: Each value of all the variables except <span class="InputCode">female</span> has a 10% chance of being missing completely at random, but of course in the real world we won't know that it is MCAR ahead of time. Thus we will check whether it is MCAR or MAR (MNAR cannot be checked by looking at the observed data) using the procedure outlined in <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_decide.htm#AreMyDataMCARMARorMNAR">Deciding to Impute</a>:</p>
<p class="InputCode">unab numvars: *<br/>
                  unab missvars: urban-wage<br/>
                  misstable sum, gen(miss_)<br/>
<br/>

                foreach var of local missvars {<br/>
<span class="indent3">local covars: list numvars - var</span><br/>
<span class="indent3">display _newline(3) "logit missingness of `var' on `covars'"</span><br/>
<span class="indent3">logit miss_`var' `covars'</span><br/>
<span class="indent3">foreach nvar of local covars {</span><br/>
<span class="indent6">display _newline(3) "ttest of `nvar' by missingness of `var'"</span><br/>
<span class="indent6">ttest `nvar', by(miss_`var')</span><br/>
<span class="indent3">}</span><br/>
                  }<br/>
</p>
<p>See the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_impute_log.htm#missingness">log file</a> for results.</p>
<p>Our goal is to regress wages  on sex, race, education level, and experience. To see the "right" answers, open the do file that creates the data set and examine the <span class="InputCode">gen</span> command that defines wage.</p>
<p>Complete code for the imputation process  can be found in the following do file:</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/mi/miex.do">Imputation do file</a></p>
<p>The imputation process creates a lot of output. We'll put highlights in this page, however, a complete log file including the associated graphs can be found here:</p>
<p><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_impute_log.htm">Imputation log file with graphs</a></p>
<p>Each section of this article will have links to the relevant section of the log. Click "back" in your browser to return to this page.</p>
<h2>Setting up</h2>
<p>The first step in using <span class="InputCode">mi</span> commands is to <span class="InputCode">mi set</span> your data. This is somewhat similar to <span class="InputCode">svyset</span>, <span class="InputCode">tsset</span>, or <span class="InputCode">xtset</span>. The <span class="InputCode">mi set</span> command tells Stata how it should store the additional imputations you'll create. We suggest using the <span class="InputCode">wide</span> format, as it is slightly faster.  On the other hand, <span class="InputCode">mlong</span> uses slightly less memory.</p>
<p>To have Stata use the <span class="InputCode">wide</span> data structure, type:</p>
<p class="InputCode">mi set wide</p>
<p>To have Stata use the <span class="InputCode">mlong</span> (marginal long) data structure, type:</p>
<p class="InputCode">mi set mlong</p>
<p>The wide vs. long terminology is borrowed from <span class="InputCode">reshape</span> and the structures are  similar. However, they are not equivalent and you would never use <span class="InputCode">reshape</span> to change the data structure used by <span class="InputCode">mi</span>. Instead,  type <span class="InputCode">mi convert wide</span> or <span class="InputCode">mi convert mlong</span> (add <span class="InputCode">,clear</span> if the data have not been saved since the last change).</p>
<p>Most of the time you don't need to worry about how the imputations are stored: the <span class="InputCode">mi</span> commands figure out automatically how to  apply whatever you do to each imputation. But if you need to manipulate the data in a way <span class="InputCode">mi</span> can't do for you, then you'll need to learn about the details of the structure you're using. You'll also need to be very, very careful. If you're interested in such things (including the rarely used <span class="InputCode">flong</span> and <span class="InputCode">flongsep</span> formats) run <a href="https://ssc.wisc.edu/sscc/pubs/mi/styles.do">this do file</a> and read the comments it contains while examining the data browser to see what the data look like in each form.</p>
<h3>Registering Variables</h3>
<p>The <span class="InputCode">mi</span> commands recognize three kinds of variables:</p>
<p><em>Imputed</em> variables are variables that <span class="InputCode">mi</span> is to impute or has imputed.</p>
<p><em>Regular</em> variables are variables that <span class="InputCode">mi</span> is not to impute, either by choice or because they are not missing any values.</p>
<p><em>Passive</em> variables are variables that are completely determined by other variables. For example, log wage is determined by wage, or an indicator for obesity might be determined by a function of weight and height. Interaction terms are also <em>passive</em> variables, though if you use Stata's interaction syntax you won't have to declare them as such. <em>Passive</em> variables are often problematic—the examples on <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#Transformations">transformations</a>, <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#Non-Linearity">non-linearity</a>, and <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#Interactions">interactions</a> show how using them inappropriately can lead to biased estimates.</p>
<p>If a <em>passive</em> variable is determined by <em>regular</em> variables, then it can be treated as a <em>regular</em> variable since no imputation is needed. <em>Passive</em> variables only have to be treated as such if they depend on <em>imputed</em> variables.</p>
<p>Registering a variable tells Stata what kind of variable it is. <em>Imputed</em> variables must always be registered:</p>
<p class="InputCode">mi register imputed <span class="Parameter">varlist</span></p>
<p>where <span class="InputCode"><span class="Parameter">varlist</span></span> should be replaced by the actual list of variables to be imputed.</p>
<p><em>Regular</em> variables often don't have to be registered, but it's a good idea:</p>
<p class="InputCode">mi register regular <span class="Parameter">varlist</span></p>
<p>Passive variables must be registered:</p>
<p class="InputCode">mi register passive <span class="Parameter">varlist</span></p>
<p>However,  <em>passive</em> variables are more often created after imputing. Do so with <span class="InputCode">mi passive</span> and they'll be registered as <em>passive</em> automatically.</p>
<p>In our example data, all the variables except <span class="InputCode">female</span> need to be imputed. The appropriate <span class="InputCode">mi register</span> command is:</p>
<p class="InputCode">mi register imputed race-wage</p>
<p>(Note that you cannot use <span class="InputCode">*</span> as your <em>varlist</em> even if you have to impute all your variables, because that would include the system variables added by <span class="InputCode">mi set</span> to keep track of the imputation structure.)</p>
<p>Registering <span class="InputCode">female</span> as regular is optional, but a good idea:</p>
<p class="InputCode">mi register regular female</p>
<h2>Checking the  Imputation Model</h2>
<p>Based on the types of the variables, the obvious imputation methods are:</p>
<ul>
<li><span class="InputCode">race</span> (categorical, three values): <span class="InputCode">mlogit</span></li>
<li><span class="InputCode">urban</span> (binary): <span class="InputCode">logit</span></li>
<li><span class="InputCode">edu</span> (ordered categorical, four values): <span class="InputCode">ologit</span></li>
<li><span class="InputCode">exp</span> (continuous): <span class="InputCode">regress</span></li>
<li><span class="InputCode">wage</span> (continuous): <span class="InputCode">regress</span></li>
</ul>
<p><span class="InputCode">female</span> does not need to be imputed, but should be included in the imputation models both because it is in the analysis model and because it's likely to be relevant.</p>
<p>Before proceeding to impute we will check each of the imputation models. <strong>Always run each of your imputation models individually, outside the <span class="InputCode">mi impute chained</span> context, to see if they converge and (insofar as it is possible) verify that they are specified correctly.</strong></p>
<p>Code to run each of these models is:</p>
<p class="InputCode">  mlogit race i.urban exp wage i.edu i.female<br/>
  logit urban i.race exp wage i.edu i.female<br/>
			ologit edu i.urban i.race exp wage i.female<br/>
			regress exp i.urban i.race wage i.edu i.female<br/>
                  regress wage i.urban i.race exp i.edu i.female                </p>
<p>Note that when categorical variables (ordered or not) appear as covariates <span class="InputCode">i.</span> expands them into sets of indicator variables.                </p>
<p> As we'll see later, the output of the <span class="InputCode">mi impute chained</span> command includes the commands for the individual models it runs. Thus a useful shortcut, especially if you have a lot of variables to impute, is to set up your <span class="InputCode">mi impute chained</span> command  with the <span class="InputCode">dryrun</span> option to prevent it from doing any actual imputing, run it, and then copy the commands from the output into your do file for testing.</p>
<h3>Convergence Problems</h3>
<p>The first thing to note is that all of these models run successfully. Complex models like <span class="InputCode">mlogit</span> may fail to converge if you have large numbers of categorical variables, because that often leads to small cell sizes. To pin down the cause of the problem,  remove most of the variables, make sure the model works with what's left, and then add variables back one at a time or in small groups until it stops working. With some experimentation you should be able to identify the problem variable or combination of variables. At that point you'll have to decide if you can combine categories or drop variables or make other changes in order to create a workable model.</p>
<h3>Prefect Prediction</h3>
<p>Perfect prediction is another problem to note. The imputation process cannot simply drop the perfectly predicted observations the way <span class="InputCode">logit</span> can. You could drop them before imputing, but that seems to defeat the purpose of multiple imputation. The alternative is to add the <span class="InputCode">augment</span> (or just <span class="InputCode">aug</span>) option to the affected methods. This tells <span class="InputCode">mi impute chained</span> to use the "augmented regression" approach, which adds fake observations with very low weights in such a way that they have a negligible effect on the results but prevent perfect prediction. For details see the section  "The issue of perfect prediction during imputation of categorical data" in the Stata MI documentation.</p>
<h3>Checking for Misspecification</h3>
<p>You should also try to evaluate whether the models are specified correctly. A full discussion of how to determine whether a regression model is specified correctly or not is well beyond the scope of this article, but use whatever tools you find appropriate. Here are some examples:</p>
<h4>Residual vs. Fitted Value Plots</h4>
<p>For continuous variables, residual vs. fitted value plots (easily done with <span class="InputCode">rvfplot</span>) can be useful—several of the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm">examples</a> use them to detect problems. Consider the plot for experience:</p>
<p class="InputCode">regress exp i.urban i.race wage i.edu i.female<br/>
                  rvfplot</p>
<p><img alt="rvfplot of exp" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/exp1.png" width="585"/></p>
<p>Note how a number of points are clustered along a line in the lower left, and no points are below it:</p>
<p><img alt="rvfplot plus constraint line" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/exp2.png" width="585"/></p>
<p>This reflects the constraint that experience cannot be less than zero, which means that the fitted values must always be greater than or equal to the residuals, or alternatively that the residuals must be greater than or equal to the negative of the fitted values. (If the graph had the same scale on both axes, the constraint line would be a 45 degree line.) If all the points were below a similar line rather than above it, this would tell you that there was an upper bound on the variable rather than a lower bound. The y-intercept of the constraint line tells you the limit in either case. You can also have both a lower bound and an upper bound, putting all the points in a band between them. </p>
<p>The "obvious" model, <span class="InputCode">regress</span>, is inappropriate for experience because it won't apply this constraint. It's also inappropriate for wages for the same reason. Alternatives include <span class="InputCode">truncreg, ll(0) </span>and <span class="InputCode">pmm</span> (we'll use <span class="InputCode">pmm</span>).                </p>
<p></p>
<h4>Adding Interactions</h4>
<p> In this example, it seems plausible that the relationships between variables may vary between race, gender, and urban/rural groups. Thus one way to check for misspecification is to add interaction terms to the models and see whether they turn out to be important. For example, we'll compare the obvious model:</p>
<p class="InputCode">regress exp i.race wage i.edu i.urban i.female<br/>
</p>
<p>with one that includes interactions:</p>
<p class="InputCode">regress exp (i.race i.urban i.female)##(c.wage i.edu)</p>
<p>We'll run  similar comparisons for the models of the other variables. This creates a great deal of output, so see the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_impute_log.htm#models">log file</a> for results. Interactions between <span class="InputCode">female</span> and other variables are significant in the models for <span class="InputCode">exp</span>, <span class="InputCode">wage</span>, <span class="InputCode">edu</span>, and <span class="InputCode">urban</span>. There are a few significant interactions between <span class="InputCode">race</span> or <span class="InputCode">urban</span> and other variables, but not nearly as many (and keep in mind that with this many coefficients we'd expect some false positives using a significance level of .05). We'll thus impute the men and women separately. This is an especially good option for this data set because <span class="InputCode">female</span> is never missing. If it were, we'd have to drop those observations which are missing <span class="InputCode">female</span> because they could not be placed in one group or the other.</p>
<p> In the imputation command this means adding the <span class="InputCode">by(female)</span> option. When testing models, it means starting the commands with the <span class="InputCode">by female:</span> prefix (and removing <span class="InputCode">female</span> from the lists of covariates). The improved imputation models are thus:</p>
<p class="InputCode">bysort female: reg exp i.urban i.race wage i.edu<br/>
                  by female: logit urban exp i.race wage i.edu<br/>
                  by female: mlogit race exp i.urban wage i.edu<br/>
                  by female: reg wage exp i.urban i.race i.edu<br/>
                by female: ologit edu exp i.urban i.race wage</p>
<p><span class="InputCode">pmm</span> itself cannot be run outside the imputation context, but since it's based on regression you can use regular regression to test it.</p>
<p>These models should be tested again, but we'll omit that process.</p>
<h2>Imputing</h2>
<p>The basic syntax for mi impute chained is:</p>
<p class="InputCode">mi impute chained (<span class="Parameter">method1</span>)<span class="Parameter"> varlist1 </span>(<span class="Parameter">method2</span>)<span class="Parameter"> varlist2...</span> = <span class="Parameter">regvars</span></p>
<p>Each <span class="Parameter">method</span> specifies the method to be used for imputing the following <span class="Parameter">varlist</span>  The possibilities for <span class="Parameter">method</span> are <span class="InputCode">regress</span>, <span class="InputCode">pmm</span>, <span class="InputCode">truncreg</span>, <span class="InputCode">intreg</span>, <span class="InputCode">logit</span>, <span class="InputCode">ologit</span>, <span class="InputCode">mlogit</span>, <span class="InputCode">poisson</span>, and <span class="InputCode">nbreg</span>.  <span class="Parameter">regvars</span> is a list of regular variables to be used as covariates in the imputation models but not imputed (there may not be any).</p>
<p>The basic options are:</p>
<p class="InputCode">, add(<span class="Parameter">N</span>) rseed(<span class="Parameter">R</span>) savetrace(<span class="Parameter">tracefile</span>, replace)</p>
<p><span class="Parameter">N</span> is the number of imputations to be added to the data set. <span class="Parameter">R</span> is the seed to be used for the random number generator—if you do not set this you'll get slightly different imputations each time the command is run. The <span class="Parameter">tracefile</span> is a dataset in which <span class="InputCode">mi impute chained</span> will store information about the imputation process. We'll use this dataset to check for convergence.</p>
<p> Options that are relevant to a particular method go with the method, inside the parentheses but following a comma (e.g. <span class="InputCode">(mlogit, aug)</span> ). Options that are relevant to the imputation process as a whole (like <span class="InputCode">by(female)</span> ) go at the end, after the comma.</p>
<p>For our example, the command would be:</p>
<p class="InputCode">mi impute chained (logit) urban (mlogit) race (ologit) edu (pmm) exp wage, add(5) rseed(4409) by(female)</p>
<p>Note that this does not include a <span class="InputCode">savetrace()</span> option. As of this writing, <span class="InputCode">by()</span> and <span class="InputCode">savetrace()</span> cannot be used at the same time, presumably because it would require one trace file for each <em>by</em> group. Stata is aware of this problem and we hope this will be changed soon. For purposes of this article, we'll remove the <span class="InputCode">by()</span> option when it comes time to illustrate  use of the trace file. If this problem comes up in your research, talk to us about work-arounds.</p>
<h3>Choosing the Number of Imputations</h3>
<p>There is some disagreement among authorities about how many imputations are sufficient. Some say 3-10 in almost all circumstances, the Stata documentation suggests at least 20, while White, Royston, and Wood argue that the number of imputations should be roughly equal to the percentage of cases with missing values. However, we are not aware of any argument that increasing the number of imputations ever causes problems (just that the marginal benefit of another imputation asymptotically approaches zero).</p>
<p>Increasing the number of imputations in your analysis takes essentially no work on your part. Just change the number in the <span class="InputCode">add()</span> option to something bigger. On the other hand, it can be a lot of work for the computer—multiple imputation has introduced many researchers into the world of jobs that take hours or days to run. You can generally assume that the amount of time required will be proportional to the number of imputations used (e.g. if a do file takes two hours to run with five imputations, it will probably take about four hours to run with ten imputations).  So here's our suggestion:</p>
<ol>
<li>Start with five imputations (the low end of what's broadly considered legitimate).</li>
<li>Work on  your research project until you're reasonably confident you have the analysis in its final form. Be sure to do everything with do files so you can run it again at will.</li>
<li>Note how long the process takes, from imputation to final analysis.</li>
<li>Consider how much time you have available and decide how many imputations you can afford to run, using the rule of thumb that time required is  proportional to the number of imputations. If possible, make the number of imputations  roughly equal to the percentage of cases with missing data (a high end  estimate of what's required). Allow  time to recover if things to go wrong, as they generally do.</li>
<li>Increase the number of imputations in your do file and start it.</li>
<li>Do something else while the do file runs, like write your paper. Adding imputations shouldn't change your results significantly—and in the unlikely event that they do, consider yourself lucky to have found that out before publishing.</li>
</ol>
<h3>Speeding up the Imputation Process</h3>
<p>Multiple imputation has introduced many researchers into the world of jobs that take hours, days, or even weeks to run. Usually it's not worth spending your time to make Stata code run faster, but multiple imputation can be an exception.</p>
<p>Use the fastest computer available to you. For SSCC members that means learning to run jobs on Linstat, the SSCC's Linux computing cluster. Linux is not as difficult as you may think—<a href="https://ssc.wisc.edu/sscc/pubs/linstat.htm">Using Linstat</a> has instructions.</p>
<p>Multiple imputation involves more reading and writing to disk than most Stata commands. Sometimes this includes writing temporary files in the current working directory. Use the fastest disk space available to you, both for your data set and for the working directory. In general local disk space will be faster than network disk space, and on Linstat <span class="InputCode">/ramdisk</span> (a "directory" that is actually stored in RAM) will be faster than local disk space. On the other hand, you would not want to permanently store data sets anywhere but network disk space. So consider having your do file do something like the following:</p>
<h4>Windows (Winstat or your own PC)</h4>
<p class="InputCode">copy x:\mydata\dataset c:\windows\temp\dataset<br/>
                cd c:\windows\temp<br/>
                use dataset<br/>
                {<span class="Parameter">do stuff, including saving results to the network as needed</span>}<br/>
                erase c:\windows\temp\dataset</p>
<h4>Linstat</h4>
<p class="InputCode">copy /project/mydata/dataset /ramdisk/dataset<br/>
                cd /ramdisk<br/>
                use dataset<br/>
                {<span class="Parameter">do stuff, including saving results to the network as needed</span>}<br/>
                erase /ramdisk/dataset</p>
<p>This applies when you're using imputed data as well. If your data set is large enough that working with it after imputation is slow, the above procedure may help.</p>
<h2 id="CheckingforConvergence">Checking for Convergence</h2>
<p>MICE is an iterative process. In each iteration, <span class="InputCode">mi impute chained</span> first estimates the imputation model, using both the observed data and the imputed data from the previous iteration. It then draws new imputed values from the resulting distributions. Note that as a result, each iteration has some autocorrelation with the previous imputation.</p>
<p>The first iteration must be a special case: in it, <span class="InputCode">mi impute chained</span> first estimates the imputation model for the variable with the fewest missing values based only on the observed data and draws imputed values for that variable. It then estimates the model for the variable with the next fewest missing values, using both the observed values and the imputed values of the first variable, and proceeds similarly for the rest of the variables. Thus the first iteration is often atypical, and because iterations are correlated it can make subsequent iterations atypical as well.</p>
<p>To avoid this, <span class="InputCode">mi impute chained</span> by default goes through ten iterations for each imputed data set you request, saving only the results of the tenth iteration. The first nine iterations are called the burn-in period. Normally this is plenty of time for the effects of the first iteration to become insignificant and for the process to converge to a stationary state. However, you should check for convergence and increase the number of iterations if necessary to ensure it using the <span class="InputCode">burnin()</span> option.                </p>
<p>To do so, examine the trace file saved by <span class="InputCode">mi impute chained</span>. It contains the mean and standard deviation of each imputed variable in each iteration. These will vary randomly, but they should not show any trend. An easy way to check is with <span class="InputCode">tsline</span>, but it requires reshaping the data first.</p>
<p>Our preferred imputation model uses <span class="InputCode">by()</span>, so it cannot save a trace file. Thus we'll remove <span class="InputCode">by()</span> for the moment. We'll also increase the <span class="InputCode">burnin()</span> option to 100 so it's easier to see what a stable trace looks like. We'll then use <span class="InputCode">reshape</span> and <span class="InputCode">tsline</span> to check for convergence:</p>
<p class="InputCode">preserve<br/>
                  mi impute chained (logit) urban (mlogit) race (ologit) edu (pmm) exp wage = female, add(5) rseed(88) savetrace(extrace, replace) burnin(100)<br/>
use extrace, replace<br/>
                  reshape wide *mean *sd, i(iter) j(m)<br/>
                  tsset iter<br/>
                  tsline exp_mean*, title("Mean of Imputed Values of Experience") note("Each line is for one imputation") legend(off)<br/>
                  graph export conv1.png, replace<br/>
                  tsline exp_sd*, title("Standard Deviation of Imputed Values of Experience") note("Each line is for one imputation") legend(off)<br/>
                  graph export conv2.png, replace<br/>
                  restore</p>
<p>The resulting graphs do not show any obvious problems:</p>
<p><img alt="Mean of imputed values of experience" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/conv1.png" width="585"/></p>
<p><img alt="SD of imputed values of experience" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/conv2.png" width="585"/></p>
<p>If you do see signs that the process may not have converged after the default ten iterations, increase the number of iterations performed before saving imputed values with the <span class="InputCode">burnin()</span> option. If convergence is never achieved this indicates a problem with the imputation model.</p>
<h2>Checking the Imputed Values</h2>
<p>After imputing, you should check to see if the imputed data resemble the observed data. Unfortunately there's no formal test to determine what's "close enough." Of course if the data are MAR but not MCAR, the imputed data should be systematically different from the observed data. Ironically, the fewer missing values you have to impute, the more variation you'll see between the imputed data and the observed data (and between imputations).</p>
<p>For binary and categorical variables, compare frequency tables. For continuous variables, comparing means and standard deviations is a good starting point, but you should look at the overall shape of the distribution as well. For that we suggest kernel density graphs or perhaps histograms. Look at each imputation separately rather than pooling all the imputed values so you can see if any one of them went wrong.</p>
<h3><a id="mixeq" name="mixeq"></a>mi xeq:</h3>
<p>The <span class="InputCode">mi xeq:</span> prefix tell Stata to apply the subsequent command to each imputation individually. It also applies to the original data, the "zeroth imputation." Thus:</p>
<p class="InputCode">mi xeq: tab race</p>
<p>will give you six frequency tables: one for the original data, and one for each of the five imputations.</p>
<p>However, we want to compare the observed data to just the imputed data, not the entire data set. This requires adding an <em>if</em> condition to the <span class="InputCode">tab</span> commands for the imputations, but not the observed data. Add a number or <em>numlist</em> to have <span class="InputCode">mi xeq</span> act on particular imputations:</p>
<p class="InputCode">mi xeq 0: tab race<br/>
  mi xeq 1/5: tab race if miss_race
</p>
<p>This creates frequency tables for the observed values of <span class="InputCode">race</span> and then the imputed values in all five imputations.</p>
<p>If you have a significant number of variables to examine you can easily loop over them:</p>
<p class="InputCode">foreach var of varlist urban race edu {<br/>
<span class="indent3">mi xeq 0: tab `var'</span><br/>
<span class="indent3">mi xeq 1/5: tab `var' if miss_`var'</span><br/>
}</p>
<p>For results see the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_impute_log.htm#checkcat">log file</a>.</p>
<p>Running summary statistics on continuous variables follows the same process, but creating kernel density graphs adds a complication: you need to either save the graphs or give yourself a chance to look at them. <span class="InputCode">mi xeq:</span> can carry out multiple commands for each imputation: just place them all in one line with a semicolon (<span class="InputCode">;</span>) at the end of each. (This will not work if you've changed the general end-of-command delimiter to a semicolon.) The <span class="InputCode">sleep</span> command tells Stata to pause for a specified period, measured in milliseconds.</p>
<p class="InputCode"> mi xeq 0: kdensity wage; sleep 1000<br/>
mi xeq 1/5: kdensity wage if miss_`var'; sleep 1000</p>
<p>Again, this can all be automated:</p>
<p class="InputCode">foreach var of varlist wage exp {<br/>
<span class="indent3">mi xeq 0: sum `var'</span><br/>
<span class="indent3">mi xeq 1/5: sum `var' if miss_`var'</span><br/>
<span class="indent3">mi xeq 0: kdensity `var'; sleep 1000</span><br/>
<span class="indent3">mi xeq 1/5: kdensity `var' if miss_`var'; sleep 1000</span><br/>
}</p>
<p>Saving the graphs turns out to be a bit trickier, because you need to give the graph from each imputation a different file name. Unfortunately you cannot access the imputation number within <span class="InputCode">mi xeq</span>. However, you can do a <span class="InputCode">forvalues</span> loop over imputation numbers, then have <span class="InputCode">mi xeq</span> act on each of them:</p>
<p class="InputCode">forval i=1/5 {<br/>
<span class="indent3">mi xeq `i': kdensity exp if miss_exp; graph export exp`i'.png, replace</span><br/>
}</p>
<p>Integrating this with the previous version gives:</p>
<p class="InputCode">foreach var of varlist wage exp {<br/>
<span class="indent3">mi xeq 0: sum `var'</span><br/>
<span class="indent3">mi xeq 1/5: sum `var' if miss_`var'</span><br/>
<span class="indent3">mi xeq 0: kdensity `var'; graph export chk`var'0.png, replace</span><br/>
<span class="indent3">forval i=1/5 {</span><br/>
<span class="indent6">mi xeq `i': kdensity `var' if miss_`var'; graph export chk`var'`i'.png, replace</span><br/>
<span class="indent3">}</span><br/>
}</p>
<p>For results, see the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_impute_log.htm#checkcont">log file</a>.</p>
<p>It's troublesome that in all imputations the mean of the imputed values of <span class="InputCode">wage</span> is higher than the mean of the observed values of <span class="InputCode">wage</span>, and the mean of the imputed values of <span class="InputCode">exp</span> is lower than the mean of the observed values of <span class="InputCode">exp</span>. We did not find evidence that the data is MAR but not MCAR, so we'd expect the means of the imputed data to be clustered around the means of the observed data. There is no formal test to tell us definitively whether this is a problem or not. However, it should raise suspicions, and if the final results with these imputed data are different from the results of complete cases analysis, it raises the question of whether the difference is due to problems with the imputation model.</p>
<p>Next: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_manage.htm">Managing Multiply Imputed Data</a></p>
<p>Previous: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_models.htm">Creating Imputation Models</a></p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url>https://ssc.wisc.edu/sscc/pubs/mi/exp1.png, https://ssc.wisc.edu/sscc/pubs/mi/exp2.png, https://ssc.wisc.edu/sscc/pubs/mi/conv1.png, https://ssc.wisc.edu/sscc/pubs/mi/conv2.png</img_base_url>
</kb_document>
<kb_document>
<kb_title>Multiple Imputation in Stata: Imputation Log File</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p>This web page contains the log file from the example imputation discussed in the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_impute.htm">Imputing</a> section, plus the graphics it creates.</p>
<p> </p>
<pre class="InputCode">----------------------------------------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  \sscc\pubs\mi\miex.log
  log type:  text
 opened on:  17 Aug 2012, 10:51:48

. 
. use midata

. 
. // test missingness of data
. unab numvars: *

. unab missvars: urban-wage

. misstable sum, gen(miss_)
                                                               Obs&lt;.
                                                +------------------------------
               |                                | Unique
      Variable |     Obs=.     Obs&gt;.     Obs&lt;.  | values        Min         Max
  -------------+--------------------------------+------------------------------
          race |       293               2,707  |      3          0           2
         urban |       273               2,727  |      2          0           1
           edu |       319               2,681  |      4          1           4
           exp |       293               2,707  |   &gt;500          0     47.8623
          wage |       299               2,701  |   &gt;500          0    227465.2
  -----------------------------------------------------------------------------
<a id="missingness" name="missingness"></a>
. 
. foreach var of local missvars {
  2.         local covars: list numvars - var
  3.         display _newline(3) "logit missingness of `var' on `covars'"
  4.         logit miss_`var' `covars'
  5.         foreach nvar of local covars {
  6.                 display _newline(3) "ttest of `nvar' by missingness of `var'"
  7.                 ttest `nvar', by(miss_`var')
  8.         }
  9. }



logit missingness of urban on female race edu exp wage

Iteration 0:   log likelihood = -613.04047  
Iteration 1:   log likelihood = -611.32144  
Iteration 2:   log likelihood = -611.31554  
Iteration 3:   log likelihood = -611.31554  

Logistic regression                               Number of obs   =       1964
                                                  LR chi2(5)      =       3.45
                                                  Prob &gt; chi2     =     0.6310
Log likelihood = -611.31554                       Pseudo R2       =     0.0028

------------------------------------------------------------------------------
  miss_urban |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      female |   .1505333   .1696945     0.89   0.375    -.1820618    .4831284
        race |   -.068621   .0980029    -0.70   0.484     -.260703    .1234611
         edu |   .0098348   .0973647     0.10   0.920    -.1809964     .200666
         exp |  -.0033092   .0094184    -0.35   0.725    -.0217689    .0151504
        wage |   3.68e-06   2.57e-06     1.43   0.153    -1.36e-06    8.71e-06
       _cons |  -2.513739   .2871859    -8.75   0.000    -3.076613   -1.950865
------------------------------------------------------------------------------



ttest of female by missingness of urban

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2727    .4979831    .0095764    .5000876    .4792053    .5167609
       1 |     273    .4761905    .0302826      .50035    .4165725    .5358085
---------+--------------------------------------------------------------------
combined |    3000        .496    .0091299    .5000674    .4780984    .5139016
---------+--------------------------------------------------------------------
    diff |            .0217927    .0317471               -.0404556    .0840409
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =   0.6864
Ho: diff = 0                                     degrees of freedom =     2998

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.7538         Pr(|T| &gt; |t|) = 0.4925          Pr(T &gt; t) = 0.2462



ttest of race by missingness of urban

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2456    1.014658    .0163483    .8101878    .9826002    1.046716
       1 |     251    1.055777    .0513125    .8129431     .954717    1.156837
---------+--------------------------------------------------------------------
combined |    2707    1.018471    .0155756    .8103808    .9879293    1.049012
---------+--------------------------------------------------------------------
    diff |           -.0411189    .0537051               -.1464261    .0641883
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =  -0.7656
Ho: diff = 0                                     degrees of freedom =     2705

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.2220         Pr(|T| &gt; |t|) = 0.4440          Pr(T &gt; t) = 0.7780



ttest of edu by missingness of urban

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2442    2.356675    .0184465    .9115617    2.320503    2.392847
       1 |     239    2.368201    .0595328    .9203542    2.250922    2.485479
---------+--------------------------------------------------------------------
combined |    2681    2.357702     .017617     .912182    2.323158    2.392247
---------+--------------------------------------------------------------------
    diff |            -.011526    .0618353               -.1327757    .1097237
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =  -0.1864
Ho: diff = 0                                     degrees of freedom =     2679

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.4261         Pr(|T| &gt; |t|) = 0.8521          Pr(T &gt; t) = 0.5739



ttest of exp by missingness of urban

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2450    15.56019    .1954164     9.67262    15.17699    15.94339
       1 |     257    15.69341    .5938361    9.519917    14.52399    16.86284
---------+--------------------------------------------------------------------
combined |    2707    15.57284    .1856003    9.656566    15.20891    15.93677
---------+--------------------------------------------------------------------
    diff |           -.1332234    .6332773                -1.37498    1.108533
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =  -0.2104
Ho: diff = 0                                     degrees of freedom =     2705

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.4167         Pr(|T| &gt; |t|) = 0.8334          Pr(T &gt; t) = 0.5833



ttest of wage by missingness of urban

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2458    71240.46    763.6437    37860.09    69743.01    72737.91
       1 |     243    74058.12     2597.11    40484.95    68942.29    79173.95
---------+--------------------------------------------------------------------
combined |    2701    71493.95    733.1819     38104.3     70056.3    72931.61
---------+--------------------------------------------------------------------
    diff |           -2817.665    2562.273               -7841.881    2206.551
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =  -1.0997
Ho: diff = 0                                     degrees of freedom =     2699

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.1358         Pr(|T| &gt; |t|) = 0.2716          Pr(T &gt; t) = 0.8642



logit missingness of edu on female race urban exp wage

Iteration 0:   log likelihood = -670.64062  
Iteration 1:   log likelihood = -669.91049  
Iteration 2:   log likelihood = -669.90956  
Iteration 3:   log likelihood = -669.90956  

Logistic regression                               Number of obs   =       1989
                                                  LR chi2(5)      =       1.46
                                                  Prob &gt; chi2     =     0.9174
Log likelihood = -669.90956                       Pseudo R2       =     0.0011

------------------------------------------------------------------------------
    miss_edu |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      female |  -.0194159   .1557151    -0.12   0.901    -.3246119    .2857801
        race |   .0569055   .0903871     0.63   0.529    -.1202499    .2340609
       urban |   .0476788    .157765     0.30   0.762    -.2615349    .3568925
         exp |  -.0028472   .0086668    -0.33   0.743    -.0198338    .0141393
        wage |   1.93e-06   2.25e-06     0.86   0.390    -2.47e-06    6.33e-06
       _cons |  -2.314849   .2528625    -9.15   0.000     -2.81045   -1.819248
------------------------------------------------------------------------------



ttest of female by missingness of edu

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2681    .4983215    .0096583    .5000905    .4793831      .51726
       1 |     319     .476489    .0280076    .5002316    .4213854    .5315926
---------+--------------------------------------------------------------------
combined |    3000        .496    .0091299    .5000674    .4780984    .5139016
---------+--------------------------------------------------------------------
    diff |            .0218325    .0296195               -.0362442    .0799092
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =   0.7371
Ho: diff = 0                                     degrees of freedom =     2998

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.7694         Pr(|T| &gt; |t|) = 0.4611          Pr(T &gt; t) = 0.2306



ttest of race by missingness of edu

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2416    1.016142    .0164994    .8109934    .9837879    1.048497
       1 |     291    1.037801    .0472723    .8064058    .9447603    1.130841
---------+--------------------------------------------------------------------
combined |    2707    1.018471    .0155756    .8103808    .9879293    1.049012
---------+--------------------------------------------------------------------
    diff |           -.0216583    .0502926                -.120274    .0769574
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =  -0.4306
Ho: diff = 0                                     degrees of freedom =     2705

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.3334         Pr(|T| &gt; |t|) = 0.6668          Pr(T &gt; t) = 0.6666



ttest of urban by missingness of edu

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2442    .6588862    .0095956    .4741806    .6400698    .6777025
       1 |     285    .6912281    .0274139    .4627995    .6372679    .7451882
---------+--------------------------------------------------------------------
combined |    2727    .6622662    .0090582     .473024    .6445046    .6800278
---------+--------------------------------------------------------------------
    diff |           -.0323419    .0296084               -.0903991    .0257153
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =  -1.0923
Ho: diff = 0                                     degrees of freedom =     2725

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.1374         Pr(|T| &gt; |t|) = 0.2748          Pr(T &gt; t) = 0.8626



ttest of exp by missingness of edu

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2419    15.61121    .1972722    9.702505    15.22437    15.99805
       1 |     288    15.25056    .5463414     9.27172    14.17522    16.32591
---------+--------------------------------------------------------------------
combined |    2707    15.57284    .1856003    9.656566    15.20891    15.93677
---------+--------------------------------------------------------------------
    diff |            .3606459    .6020106               -.8198013    1.541093
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =   0.5991
Ho: diff = 0                                     degrees of freedom =     2705

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.7254         Pr(|T| &gt; |t|) = 0.5492          Pr(T &gt; t) = 0.2746



ttest of wage by missingness of edu

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2412    71484.16    778.2065    38219.37    69958.14    73010.19
       1 |     289    71575.65    2187.928    37194.77     67269.3    75882.01
---------+--------------------------------------------------------------------
combined |    2701    71493.95    733.1819     38104.3     70056.3    72931.61
---------+--------------------------------------------------------------------
    diff |           -91.48891    2372.352               -4743.299    4560.321
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =  -0.0386
Ho: diff = 0                                     degrees of freedom =     2699

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.4846         Pr(|T| &gt; |t|) = 0.9692          Pr(T &gt; t) = 0.5154



logit missingness of exp on female race urban edu wage

Iteration 0:   log likelihood = -654.79701  
Iteration 1:   log likelihood = -653.43555  
Iteration 2:   log likelihood = -653.43222  
Iteration 3:   log likelihood = -653.43222  

Logistic regression                               Number of obs   =       1982
                                                  LR chi2(5)      =       2.73
                                                  Prob &gt; chi2     =     0.7416
Log likelihood = -653.43222                       Pseudo R2       =     0.0021

------------------------------------------------------------------------------
    miss_exp |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      female |   .0225336   .1628237     0.14   0.890     -.296595    .3416622
        race |  -.0595427   .0946498    -0.63   0.529    -.2450529    .1259675
       urban |   .0187602   .1634337     0.11   0.909     -.301564    .3390845
         edu |   .1058189   .0930734     1.14   0.256    -.0766016    .2882394
        wage |  -2.42e-06   2.20e-06    -1.10   0.271    -6.73e-06    1.89e-06
       _cons |  -2.216882   .2563097    -8.65   0.000     -2.71924   -1.714524
------------------------------------------------------------------------------



ttest of female by missingness of exp

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2707    .4931659    .0096109    .5000457    .4743204    .5120114
       1 |     293    .5221843    .0292315    .5003622    .4646532    .5797154
---------+--------------------------------------------------------------------
combined |    3000        .496    .0091299    .5000674    .4780984    .5139016
---------+--------------------------------------------------------------------
    diff |           -.0290184    .0307552               -.0893219    .0312851
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =  -0.9435
Ho: diff = 0                                     degrees of freedom =     2998

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.1727         Pr(|T| &gt; |t|) = 0.3455          Pr(T &gt; t) = 0.8273



ttest of race by missingness of exp

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2448    1.020425    .0163788    .8103788    .9883071    1.052543
       1 |     259           1    .0504388    .8117356    .9006758    1.099324
---------+--------------------------------------------------------------------
combined |    2707    1.018471    .0155756    .8103808    .9879293    1.049012
---------+--------------------------------------------------------------------
    diff |            .0204248    .0529598               -.0834209    .1242705
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =   0.3857
Ho: diff = 0                                     degrees of freedom =     2705

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.6501         Pr(|T| &gt; |t|) = 0.6998          Pr(T &gt; t) = 0.3499



ttest of urban by missingness of exp

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2450    .6628571    .0095526    .4728306    .6441251    .6815892
       1 |     277    .6570397    .0285735    .4755575    .6007901    .7132894
---------+--------------------------------------------------------------------
combined |    2727    .6622662    .0090582     .473024    .6445046    .6800278
---------+--------------------------------------------------------------------
    diff |            .0058174    .0299902               -.0529884    .0646233
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =   0.1940
Ho: diff = 0                                     degrees of freedom =     2725

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.5769         Pr(|T| &gt; |t|) = 0.8462          Pr(T &gt; t) = 0.4231



ttest of edu by missingness of exp

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2419    2.355105    .0185189    .9108219    2.318791     2.39142
       1 |     262    2.381679    .0572124    .9260638    2.269023    2.494336
---------+--------------------------------------------------------------------
combined |    2681    2.357702     .017617     .912182    2.323158    2.392247
---------+--------------------------------------------------------------------
    diff |            -.026574    .0593371               -.1429251    .0897771
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =  -0.4478
Ho: diff = 0                                     degrees of freedom =     2679

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.3271         Pr(|T| &gt; |t|) = 0.6543          Pr(T &gt; t) = 0.6729



ttest of wage by missingness of exp

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2432    71682.77    773.3145    38136.25    70166.35     73199.2
       1 |     269    69786.84    2307.266    37841.97    65244.17    74329.51
---------+--------------------------------------------------------------------
combined |    2701    71493.95    733.1819     38104.3     70056.3    72931.61
---------+--------------------------------------------------------------------
    diff |            1895.932    2448.559               -2905.309    6697.173
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =   0.7743
Ho: diff = 0                                     degrees of freedom =     2699

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.7806         Pr(|T| &gt; |t|) = 0.4388          Pr(T &gt; t) = 0.2194



logit missingness of wage on female race urban edu exp

Iteration 0:   log likelihood = -647.94103  
Iteration 1:   log likelihood = -645.05158  
Iteration 2:   log likelihood =  -645.0361  
Iteration 3:   log likelihood =  -645.0361  

Logistic regression                               Number of obs   =       1979
                                                  LR chi2(5)      =       5.81
                                                  Prob &gt; chi2     =     0.3252
Log likelihood =  -645.0361                       Pseudo R2       =     0.0045

------------------------------------------------------------------------------
   miss_wage |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      female |   -.191566   .1570953    -1.22   0.223    -.4994672    .1163353
        race |  -.1705262   .0959515    -1.78   0.076    -.3585876    .0175352
       urban |  -.1708259   .1599631    -1.07   0.286    -.4843478     .142696
         edu |   .0710834   .0886472     0.80   0.423     -.102662    .2448288
         exp |   .0040734   .0079491     0.51   0.608    -.0115065    .0196534
       _cons |  -2.049828   .2771956    -7.39   0.000    -2.593121   -1.506535
------------------------------------------------------------------------------



ttest of female by missingness of wage

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2701    .5012958    .0096225    .5000909    .4824277     .520164
       1 |     299    .4481605    .0288081    .4981391    .3914674    .5048537
---------+--------------------------------------------------------------------
combined |    3000        .496    .0091299    .5000674    .4780984    .5139016
---------+--------------------------------------------------------------------
    diff |            .0531353     .030468                -.006605    .1128755
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =   1.7440
Ho: diff = 0                                     degrees of freedom =     2998

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.9594         Pr(|T| &gt; |t|) = 0.0813          Pr(T &gt; t) = 0.0406



ttest of race by missingness of wage

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2442    1.020885    .0164342    .8121201    .9886582    1.053111
       1 |     265    .9962264    .0488572    .7953373    .9000271    1.092426
---------+--------------------------------------------------------------------
combined |    2707    1.018471    .0155756    .8103808    .9879293    1.049012
---------+--------------------------------------------------------------------
    diff |            .0246581    .0524204               -.0781299    .1274461
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =   0.4704
Ho: diff = 0                                     degrees of freedom =     2705

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.6809         Pr(|T| &gt; |t|) = 0.6381          Pr(T &gt; t) = 0.3191



ttest of urban by missingness of wage

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2458    .6647681    .0095237    .4721675    .6460928    .6834434
       1 |     269    .6394052    .0293312    .4810681    .5816562    .6971542
---------+--------------------------------------------------------------------
combined |    2727    .6622662    .0090582     .473024    .6445046    .6800278
---------+--------------------------------------------------------------------
    diff |            .0253629    .0303797               -.0342066    .0849324
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =   0.8349
Ho: diff = 0                                     degrees of freedom =     2725

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.7981         Pr(|T| &gt; |t|) = 0.4039          Pr(T &gt; t) = 0.2019



ttest of edu by missingness of wage

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2412    2.357794    .0185831    .9126566    2.321354    2.394235
       1 |     269    2.356877    .0554598    .9096083    2.247685     2.46607
---------+--------------------------------------------------------------------
combined |    2681    2.357702     .017617     .912182    2.323158    2.392247
---------+--------------------------------------------------------------------
    diff |             .000917     .058647                -.114081    .1159151
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =   0.0156
Ho: diff = 0                                     degrees of freedom =     2679

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.5062         Pr(|T| &gt; |t|) = 0.9875          Pr(T &gt; t) = 0.4938



ttest of exp by missingness of wage

Two-sample t test with equal variances
------------------------------------------------------------------------------
   Group |     Obs        Mean    Std. Err.   Std. Dev.   [95% Conf. Interval]
---------+--------------------------------------------------------------------
       0 |    2432    15.51836    .1952193    9.627299    15.13555    15.90117
       1 |     275    16.05461    .5979892    9.916529    14.87737    17.23184
---------+--------------------------------------------------------------------
combined |    2707    15.57284    .1856003    9.656566    15.20891    15.93677
---------+--------------------------------------------------------------------
    diff |           -.5362457    .6143811                -1.74095    .6684581
------------------------------------------------------------------------------
    diff = mean(0) - mean(1)                                      t =  -0.8728
Ho: diff = 0                                     degrees of freedom =     2705

    Ha: diff &lt; 0                 Ha: diff != 0                 Ha: diff &gt; 0
 Pr(T &lt; t) = 0.1914         Pr(|T| &gt; |t|) = 0.3828          Pr(T &gt; t) = 0.8086

. 
. 
. // set up trial imputation command just to get the individual regression commands
. mi set wide

. mi register imputed race-wage

. mi register regular female

. mi impute chained (logit) urban (mlogit) race (ologit) edu (regress) exp wage = i.female, dryrun

Conditional models:
             urban: logit urban i.race exp wage i.edu i.female
              race: mlogit race i.urban exp wage i.edu i.female
               exp: regress exp i.urban i.race wage i.edu i.female
              wage: regress wage i.urban i.race exp i.edu i.female
               edu: ologit edu i.urban i.race exp wage i.female


<a id="models" name="models"></a>. 
. // test imputation model for race
. mlogit race exp wage i.edu i.urban i.female

Iteration 0:   log likelihood =   -1953.43  
Iteration 1:   log likelihood = -1879.0566  
Iteration 2:   log likelihood = -1877.6678  
Iteration 3:   log likelihood = -1877.6668  
Iteration 4:   log likelihood = -1877.6668  

Multinomial logistic regression                   Number of obs   =       1779
                                                  LR chi2(14)     =     151.53
                                                  Prob &gt; chi2     =     0.0000
Log likelihood = -1877.6668                       Pseudo R2       =     0.0388

------------------------------------------------------------------------------
        race |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
0            |
         exp |  -.0160644   .0075448    -2.13   0.033     -.030852   -.0012767
        wage |   5.80e-06   2.07e-06     2.80   0.005     1.75e-06    9.85e-06
             |
         edu |
          2  |  -.8129621   .1829418    -4.44   0.000    -1.171521   -.4544027
          3  |  -1.593897   .1971316    -8.09   0.000    -1.980268   -1.207526
          4  |   -2.72232   .2886243    -9.43   0.000    -3.288013   -2.156626
             |
     1.urban |   .7865707   .1339259     5.87   0.000     .5240808    1.049061
    1.female |   .2893221   .1342653     2.15   0.031      .026167    .5524772
       _cons |    .226939    .225613     1.01   0.314    -.2152544    .6691324
-------------+----------------------------------------------------------------
1            |
         exp |   .0052958   .0071395     0.74   0.458    -.0086974    .0192891
        wage |   6.69e-07   1.97e-06     0.34   0.734    -3.19e-06    4.53e-06
             |
         edu |
          2  |  -.5144888   .1832349    -2.81   0.005    -.8736226    -.155355
          3  |  -1.125629   .1949919    -5.77   0.000    -1.507806    -.743452
          4  |  -1.307677   .2464598    -5.31   0.000    -1.790729   -.8246246
             |
     1.urban |   .4772458   .1266699     3.77   0.000     .2289775    .7255142
    1.female |   .1276518   .1290494     0.99   0.323    -.1252803     .380584
       _cons |    .253432    .220844     1.15   0.251    -.1794143    .6862783
-------------+----------------------------------------------------------------
2            |  (base outcome)
------------------------------------------------------------------------------

. // test for misspecification by adding interactions
. mlogit race (c.exp c.wage i.edu)##(i.female i.urban)

Iteration 0:   log likelihood =   -1953.43  
Iteration 1:   log likelihood = -1873.2138  
Iteration 2:   log likelihood = -1871.3005  
Iteration 3:   log likelihood = -1871.2474  
Iteration 4:   log likelihood = -1871.2473  

Multinomial logistic regression                   Number of obs   =       1779
                                                  LR chi2(34)     =     164.37
                                                  Prob &gt; chi2     =     0.0000
Log likelihood = -1871.2473                       Pseudo R2       =     0.0421

-------------------------------------------------------------------------------
         race |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
--------------+----------------------------------------------------------------
0             |
          exp |  -.0032793   .0152468    -0.22   0.830    -.0331624    .0266038
         wage |   7.15e-06   4.01e-06     1.78   0.075    -7.13e-07     .000015
              |
          edu |
           2  |  -.7354223   .3525541    -2.09   0.037    -1.426416    -.044429
           3  |  -1.842053   .4005341    -4.60   0.000    -2.627085    -1.05702
           4  |  -3.830948   1.095232    -3.50   0.000    -5.977563   -1.684332
              |
     1.female |   .1884419   .4158562     0.45   0.650    -.6266212    1.003505
      1.urban |   1.056967   .4258448     2.48   0.013     .2223265    1.891608
              |
 female#c.exp |
           1  |  -.0064319   .0151943    -0.42   0.672    -.0362122    .0233484
              |
  urban#c.exp |
           1  |  -.0133441   .0161867    -0.82   0.410    -.0450693    .0183812
              |
female#c.wage |
           1  |   1.91e-06   4.18e-06     0.46   0.648    -6.29e-06    .0000101
              |
 urban#c.wage |
           1  |  -3.07e-06   4.27e-06    -0.72   0.472    -.0000114    5.30e-06
              |
   edu#female |
         2 1  |  -.0181898   .3925029    -0.05   0.963    -.7874813    .7511017
         3 1  |  -.0172244   .4175976    -0.04   0.967    -.8357006    .8012518
         4 1  |   .4977631   .6962046     0.71   0.475    -.8667728    1.862299
              |
    edu#urban |
         2 1  |  -.0763182   .3951518    -0.19   0.847    -.8508015    .6981651
         3 1  |   .3914775    .440372     0.89   0.374    -.4716358    1.254591
         4 1  |   .8667237   1.155459     0.75   0.453    -1.397934    3.131381
              |
        _cons |   .0469868   .3959872     0.12   0.906    -.7291337    .8231074
--------------+----------------------------------------------------------------
1             |
          exp |   .0117503   .0139703     0.84   0.400     -.015631    .0391316
         wage |   6.92e-07   3.72e-06     0.19   0.852    -6.59e-06    7.98e-06
              |
          edu |
           2  |  -.4485059   .3458792    -1.30   0.195    -1.126417    .2294049
           3  |   -1.31316   .3798982    -3.46   0.001    -2.057747   -.5685735
           4  |  -1.904266   .5852199    -3.25   0.001    -3.051275   -.7572556
              |
     1.female |   .1574212   .4125146     0.38   0.703    -.6510925    .9659349
      1.urban |   .3765925    .421229     0.89   0.371    -.4490011    1.202186
              |
 female#c.exp |
           1  |  -.0173227   .0144194    -1.20   0.230    -.0455843    .0109389
              |
  urban#c.exp |
           1  |   .0034701    .015149     0.23   0.819    -.0262214    .0331615
              |
female#c.wage |
           1  |   3.64e-06   4.00e-06     0.91   0.363    -4.20e-06    .0000115
              |
 urban#c.wage |
           1  |  -2.70e-06   4.03e-06    -0.67   0.503    -.0000106    5.20e-06
              |
   edu#female |
         2 1  |   -.227974   .3945182    -0.58   0.563    -1.001215    .5452674
         3 1  |  -.0181844    .414751    -0.04   0.965    -.8310815    .7947127
         4 1  |   .4709082   .5673412     0.83   0.407    -.6410601    1.582876
              |
    edu#urban |
         2 1  |   .0936678   .3968078     0.24   0.813    -.6840612    .8713968
         3 1  |   .3483946   .4271216     0.82   0.415    -.4887483    1.185538
         4 1  |   .3778603   .6590117     0.57   0.566     -.913779      1.6695
              |
        _cons |   .2636095   .3807272     0.69   0.489    -.4826021    1.009821
--------------+----------------------------------------------------------------
2             |  (base outcome)
-------------------------------------------------------------------------------

. 
. // test imputation model for exp
. regress exp i.race wage i.edu i.urban i.female

      Source |       SS       df       MS              Number of obs =    1779
-------------+------------------------------           F(  8,  1770) =   93.75
       Model |  49906.8412     8  6238.35514           Prob &gt; F      =  0.0000
    Residual |   117780.77  1770  66.5428078           R-squared     =  0.2976
-------------+------------------------------           Adj R-squared =  0.2944
       Total |  167687.611  1778  94.3124921           Root MSE      =  8.1574

------------------------------------------------------------------------------
         exp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        race |
          1  |   1.391616   .4799807     2.90   0.004      .450227    2.333004
          2  |   1.031061   .4947771     2.08   0.037     .0606522     2.00147
             |
        wage |   .0001343   5.71e-06    23.50   0.000     .0001231    .0001455
             |
         edu |
          2  |   -1.96545   .5537782    -3.55   0.000    -3.051578   -.8793225
          3  |  -5.058849   .5979374    -8.46   0.000    -6.231586   -3.886111
          4  |  -7.905853   .8106653    -9.75   0.000    -9.495815   -6.315891
             |
     1.urban |  -.5730682   .4234363    -1.35   0.176    -1.403556    .2574196
    1.female |  -1.111798   .4256352    -2.61   0.009    -1.946598   -.2769972
       _cons |   9.243531    .716064    12.91   0.000     7.839111    10.64795
------------------------------------------------------------------------------

. // test for misspecification with rvfplot
. // constraint line indicates exp&gt;=0
. rvfplot, ylabel(-40 -20 0 20 40)

. graph export exp1.png, replace
(file exp1.png written in PNG format)
<img alt="rvfplot" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/exp1.png" width="585"/>
. predict exphat
(option xb assumed; fitted values)
(1018 missing values generated)

. predict expres, res
(1221 missing values generated)

. gen y=-exphat
(1018 missing values generated)

. scatter expres exphat || line y exphat, legend(order(2 "Exp&gt;=0 Constraint"))

. graph export exp2.png, replace
(file exp2.png written in PNG format)
<img alt="rvfplot with constraint line added" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/exp2.png" width="585"/>
. drop expres exphat y

. //test for misspecification by adding interactions
. regress exp (i.race i.urban i.female)##(c.wage i.edu)

      Source |       SS       df       MS              Number of obs =    1779
-------------+------------------------------           F( 24,  1754) =   32.28
       Model |  51376.4689    24   2140.6862           Prob &gt; F      =  0.0000
    Residual |  116311.142  1754  66.3119396           R-squared     =  0.3064
-------------+------------------------------           Adj R-squared =  0.2969
       Total |  167687.611  1778  94.3124921           Root MSE      =  8.1432

-------------------------------------------------------------------------------
          exp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
--------------+----------------------------------------------------------------
         race |
           1  |   .8903704   1.271895     0.70   0.484     -1.60422    3.384961
           2  |   1.418534    1.46223     0.97   0.332    -1.449363    4.286431
              |
      1.urban |  -1.905077    1.19761    -1.59   0.112     -4.25397    .4438155
     1.female |  -.0698405   1.188445    -0.06   0.953    -2.400758    2.261077
         wage |   .0001473   .0000136    10.87   0.000     .0001207    .0001739
              |
          edu |
           2  |  -3.806439   1.314344    -2.90   0.004    -6.384285   -1.228592
           3  |  -6.196198   1.456027    -4.26   0.000     -9.05193   -3.340466
           4  |  -8.003504   2.559556    -3.13   0.002    -13.02361   -2.983403
              |
  race#c.wage |
           1  |  -8.72e-06   .0000133    -0.65   0.513    -.0000349    .0000174
           2  |  -.0000135    .000013    -1.04   0.296     -.000039    .0000119
              |
     race#edu |
         1 2  |   2.488045   1.259945     1.97   0.048      .016893    4.959198
         1 3  |  -.1736131   1.376548    -0.13   0.900     -2.87346    2.526234
         1 4  |    2.29836    2.10793     1.09   0.276    -1.835959    6.432679
         2 2  |   1.029303    1.46414     0.70   0.482    -1.842342    3.900947
         2 3  |  -.0586898   1.510437    -0.04   0.969    -3.021136    2.903756
         2 4  |   2.118492   2.158164     0.98   0.326    -2.114354    6.351337
              |
 urban#c.wage |
           1  |   8.06e-06   .0000115     0.70   0.482    -.0000144    .0000306
              |
    urban#edu |
         1 2  |   .8233918   1.191193     0.69   0.490    -1.512916      3.1597
         1 3  |   1.802902    1.29168     1.40   0.163    -.7304935    4.336297
         1 4  |  -3.443128    2.15643    -1.60   0.111    -7.672571    .7863152
              |
female#c.wage |
           1  |  -.0000233   .0000115    -2.02   0.044     -.000046   -6.71e-07
              |
   female#edu |
         1 2  |   .5626791   1.188512     0.47   0.636    -1.768369    2.893728
         1 3  |   .4449296   1.252109     0.36   0.722    -2.010853    2.900712
         1 4  |   2.712876     1.8349     1.48   0.139    -.8859457    6.311698
              |
        _cons |   9.321907   1.382308     6.74   0.000     6.610763    12.03305
-------------------------------------------------------------------------------

. 
. 
. // test imputation model for wage
. regress wage i.race exp i.edu i.urban i.female

      Source |       SS       df       MS              Number of obs =    1779
-------------+------------------------------           F(  8,  1770) =  145.49
       Model |  1.0214e+12     8  1.2767e+11           Prob &gt; F      =  0.0000
    Residual |  1.5532e+12  1770   877509504           R-squared     =  0.3967
-------------+------------------------------           Adj R-squared =  0.3940
       Total |  2.5746e+12  1778  1.4480e+09           Root MSE      =   29623

------------------------------------------------------------------------------
        wage |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        race |
          1  |  -4353.207   1744.074    -2.50   0.013    -7773.869   -932.5451
          2  |  -4939.278   1795.106    -2.75   0.006    -8460.029   -1418.526
             |
         exp |   1771.135   75.35312    23.50   0.000     1623.344    1918.925
             |
         edu |
          2  |   8345.912   2008.366     4.16   0.000     4406.894    12284.93
          3  |   26875.02   2120.707    12.67   0.000     22715.66    31034.37
          4  |   44200.82   2833.404    15.60   0.000     38643.65    49757.99
             |
     1.urban |    3737.22     1535.9     2.43   0.015     724.8518    6749.588
    1.female |  -19496.65   1477.669   -13.19   0.000    -22394.81   -16598.49
       _cons |   37847.82   2566.897    14.74   0.000     32813.35    42882.29
------------------------------------------------------------------------------

. // test for misspecification with rvfplot
. // constraint line indicates wage&gt;=0
. rvfplot

. graph export wage.png, replace
<img alt="rvfplot for wage" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/wage.png" width="585"/>
(note: file wage.png not found)
(file wage.png written in PNG format)

. // test interactions
. regress wage (i.race i.urban i.female)##(c.exp i.edu) 

      Source |       SS       df       MS              Number of obs =    1779
-------------+------------------------------           F( 24,  1754) =   51.27
       Model |  1.0615e+12    24  4.4230e+10           Prob &gt; F      =  0.0000
    Residual |  1.5130e+12  1754   862625195           R-squared     =  0.4123
-------------+------------------------------           Adj R-squared =  0.4043
       Total |  2.5746e+12  1778  1.4480e+09           Root MSE      =   29370

------------------------------------------------------------------------------
        wage |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        race |
          1  |   409.0987   4761.366     0.09   0.932    -8929.451    9747.648
          2  |    40.7097     5452.8     0.01   0.994    -10653.96    10735.38
             |
     1.urban |   2955.393   4512.636     0.65   0.513     -5895.32     11806.1
    1.female |  -5986.333    4393.73    -1.36   0.173    -14603.83    2631.167
         exp |   2086.047   188.9033    11.04   0.000     1715.547    2456.546
             |
         edu |
          2  |   12596.53   4787.695     2.63   0.009     3206.339    21986.72
          3  |    33416.6   5179.464     6.45   0.000     23258.02    43575.17
          4  |   29270.41   9233.095     3.17   0.002     11161.38    47379.44
             |
  race#c.exp |
          1  |  -367.0251   180.2227    -2.04   0.042     -720.499   -13.55117
          2  |  -53.25982   182.8739    -0.29   0.771    -411.9335    305.4139
             |
    race#edu |
        1 2  |  -482.0457   4541.364    -0.11   0.915    -9389.103    8425.011
        1 3  |   1861.724   4875.674     0.38   0.703    -7701.021    11424.47
        1 4  |   7840.737   7555.631     1.04   0.300    -6978.253    22659.73
        2 2  |  -7391.542   5296.316    -1.40   0.163     -17779.3    2996.213
        2 3  |  -4044.694   5390.587    -0.75   0.453    -14617.35    6527.957
        2 4  |   3039.309   7774.405     0.39   0.696    -12208.77    18287.38
             |
 urban#c.exp |
          1  |   119.1172   157.4763     0.76   0.450    -189.7437    427.9781
             |
   urban#edu |
        1 2  |   644.4913   4304.863     0.15   0.881    -7798.712    9087.694
        1 3  |  -7540.896   4566.843    -1.65   0.099    -16497.92    1416.132
        1 4  |   25090.89   7625.385     3.29   0.001     10135.09    40046.69
             |
female#c.exp |
          1  |   -547.008   151.0542    -3.62   0.000    -843.2732   -250.7427
             |
  female#edu |
        1 2  |  -6295.529   4285.177    -1.47   0.142    -14700.12    2109.064
        1 3  |  -3410.924   4406.216    -0.77   0.439    -12052.91    5231.064
        1 4  |  -19915.24   6326.051    -3.15   0.002    -32322.64   -7507.849
             |
       _cons |   29172.73   5272.194     5.53   0.000     18832.28    39513.17
------------------------------------------------------------------------------

. 
. 
. // test imputation model for edu
. ologit edu i.race exp wage i.urban i.female

Iteration 0:   log likelihood = -2295.6305  
Iteration 1:   log likelihood =   -2021.07  
Iteration 2:   log likelihood = -2013.1176  
Iteration 3:   log likelihood = -2013.1071  
Iteration 4:   log likelihood = -2013.1071  

Ordered logistic regression                       Number of obs   =       1779
                                                  LR chi2(6)      =     565.05
                                                  Prob &gt; chi2     =     0.0000
Log likelihood = -2013.1071                       Pseudo R2       =     0.1231

------------------------------------------------------------------------------
         edu |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        race |
          1  |   .5023646   .1109327     4.53   0.000     .2849405    .7197886
          2  |   1.220383   .1136064    10.74   0.000     .9977182    1.443047
             |
         exp |  -.0604264   .0055268   -10.93   0.000    -.0712587    -.049594
        wage |   .0000253   1.50e-06    16.89   0.000     .0000224    .0000282
     1.urban |   .8322351   .0973248     8.55   0.000      .641482    1.022988
    1.female |   .9733093   .0976488     9.97   0.000     .7819211    1.164697
-------------+----------------------------------------------------------------
       /cut1 |   .6530809   .1597114                      .3400522    .9661095
       /cut2 |   2.796932   .1712768                      2.461236    3.132628
       /cut3 |    5.04024   .2008955                      4.646492    5.433988
------------------------------------------------------------------------------

. // test for misspecification by adding interactions
. ologit edu (i.race i.urban i.female)##(c.exp c.wage)

Iteration 0:   log likelihood = -2295.6305  
Iteration 1:   log likelihood =  -2013.335  
Iteration 2:   log likelihood = -2004.1162  
Iteration 3:   log likelihood = -2004.0949  
Iteration 4:   log likelihood = -2004.0949  

Ordered logistic regression                       Number of obs   =       1779
                                                  LR chi2(14)     =     583.07
                                                  Prob &gt; chi2     =     0.0000
Log likelihood = -2004.0949                       Pseudo R2       =     0.1270

-------------------------------------------------------------------------------
          edu |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
--------------+----------------------------------------------------------------
         race |
           1  |   .1327298   .2534179     0.52   0.600    -.3639601    .6294198
           2  |   1.003797   .2497778     4.02   0.000     .5142413    1.493352
              |
      1.urban |   1.366727   .2189527     6.24   0.000     .9375876    1.795866
     1.female |    .618023   .2149086     2.88   0.004     .1968099    1.039236
          exp |  -.0465445   .0139356    -3.34   0.001    -.0738578   -.0192312
         wage |   .0000226   3.55e-06     6.38   0.000     .0000157    .0000296
              |
   race#c.exp |
           1  |   .0009364   .0132133     0.07   0.944    -.0249611    .0268339
           2  |  -.0018853   .0134862    -0.14   0.889    -.0283179    .0245472
              |
  race#c.wage |
           1  |   5.02e-06   3.42e-06     1.47   0.142    -1.68e-06    .0000117
           2  |   3.62e-06   3.35e-06     1.08   0.280    -2.95e-06    .0000102
              |
  urban#c.exp |
           1  |  -.0137359   .0115411    -1.19   0.234    -.0363561    .0088844
              |
 urban#c.wage |
           1  |  -4.85e-06   2.88e-06    -1.68   0.092    -.0000105    7.92e-07
              |
 female#c.exp |
           1  |  -.0057173   .0108306    -0.53   0.598    -.0269449    .0155103
              |
female#c.wage |
           1  |   6.55e-06   2.81e-06     2.33   0.020     1.05e-06    .0000121
--------------+----------------------------------------------------------------
        /cut1 |   .6270383   .2723209                      .0932992    1.160777
        /cut2 |   2.779004   .2811126                      2.228033    3.329974
        /cut3 |   5.047488   .2980929                      4.463237     5.63174
-------------------------------------------------------------------------------

. 
. // test imputation model for urban
. logit urban i.race exp wage i.edu i.female

Iteration 0:   log likelihood = -1142.4725  
Iteration 1:   log likelihood = -1075.0707  
Iteration 2:   log likelihood = -1073.6056  
Iteration 3:   log likelihood = -1073.6034  
Iteration 4:   log likelihood = -1073.6034  

Logistic regression                               Number of obs   =       1779
                                                  LR chi2(8)      =     137.74
                                                  Prob &gt; chi2     =     0.0000
Log likelihood = -1073.6034                       Pseudo R2       =     0.0603

------------------------------------------------------------------------------
       urban |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        race |
          1  |  -.2954482   .1311706    -2.25   0.024    -.5525378   -.0383585
          2  |  -.7792647   .1334625    -5.84   0.000    -1.040846    -.517683
             |
         exp |  -.0088777   .0063875    -1.39   0.165     -.021397    .0036416
        wage |   4.29e-06   1.77e-06     2.42   0.015     8.23e-07    7.76e-06
             |
         edu |
          2  |    .606303   .1394181     4.35   0.000     .3330484    .8795575
          3  |    1.03064   .1574484     6.55   0.000     .7220471    1.339233
          4  |   1.994752   .2554113     7.81   0.000     1.494155    2.495349
             |
    1.female |  -.0520909   .1138005    -0.46   0.647    -.2751358     .170954
       _cons |   .1577303   .1863126     0.85   0.397    -.2074357    .5228963
------------------------------------------------------------------------------

. // test for misspecification by adding interactions
. logit urban (i.race i.female)##(c.exp c.wage i.edu)

Iteration 0:   log likelihood = -1142.4725  
Iteration 1:   log likelihood = -1020.9319  
Iteration 2:   log likelihood = -1015.6326  
Iteration 3:   log likelihood = -1015.3365  
Iteration 4:   log likelihood = -1015.3347  
Iteration 5:   log likelihood = -1015.3347  

Logistic regression                               Number of obs   =       1779
                                                  LR chi2(23)     =     254.28
                                                  Prob &gt; chi2     =     0.0000
Log likelihood = -1015.3347                       Pseudo R2       =     0.1113

-------------------------------------------------------------------------------
        urban |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
--------------+----------------------------------------------------------------
         race |
           1  |  -.7713392   .3795508    -2.03   0.042    -1.515245   -.0274332
           2  |  -1.156397   .4315566    -2.68   0.007    -2.002233   -.3105619
              |
     1.female |   -1.51098   .3321957    -4.55   0.000    -2.162072   -.8598889
          exp |  -.0231263   .0138578    -1.67   0.095    -.0502871    .0040344
         wage |   5.59e-06   3.70e-06     1.51   0.131    -1.66e-06    .0000129
              |
          edu |
           2  |  -.2098759   .2750943    -0.76   0.446    -.7490508     .329299
           3  |  -.0925368   .3264801    -0.28   0.777    -.7324261    .5473525
           4  |   .4987983   1.103814     0.45   0.651    -1.664638    2.662234
              |
   race#c.exp |
           1  |   .0257714   .0168647     1.53   0.126    -.0072828    .0588256
           2  |   .0187349   .0166967     1.12   0.262      -.01399    .0514598
              |
  race#c.wage |
           1  |  -5.02e-07   4.54e-06    -0.11   0.912    -9.40e-06    8.39e-06
           2  |   3.07e-06   4.36e-06     0.70   0.481    -5.47e-06    .0000116
              |
     race#edu |
         1 2  |   .1629017   .3408843     0.48   0.633    -.5052193    .8310227
         1 3  |   .0019093   .4077384     0.00   0.996    -.7972432    .8010619
         1 4  |  -.5458701   1.203649    -0.45   0.650    -2.904979    1.813238
         2 2  |   .0964406   .3983642     0.24   0.809     -.684339    .8772201
         2 3  |  -.3518724   .4347531    -0.81   0.418    -1.203973     .500228
         2 4  |  -.9799584   1.159742    -0.84   0.398     -3.25301    1.293093
              |
 female#c.exp |
           1  |     -.0047   .0134151    -0.35   0.726    -.0309931    .0215931
              |
female#c.wage |
           1  |  -4.51e-06   3.73e-06    -1.21   0.227    -.0000118    2.81e-06
              |
   female#edu |
         1 2  |   1.671398     .30319     5.51   0.000     1.077157     2.26564
         1 3  |   2.788041   .3342213     8.34   0.000      2.13298    3.443103
         1 4  |   4.461757   .5745605     7.77   0.000     3.335639    5.587875
              |
        _cons |   1.049029   .3234171     3.24   0.001     .4151428    1.682915
-------------------------------------------------------------------------------

. 
. 
. // refine models after reviewing results
. mi impute chained (logit) urban (mlogit) race (ologit) edu (pmm) exp wage, dryrun by(female)

Performing setup for each by() group:

-&gt; female = 0
Conditional models:
               exp: pmm exp i.urban i.race wage i.edu
             urban: logit urban exp i.race wage i.edu
              race: mlogit race exp i.urban wage i.edu
              wage: pmm wage exp i.urban i.race i.edu
               edu: ologit edu exp i.urban i.race wage

-&gt; female = 1
Conditional models:
             urban: logit urban wage i.race i.edu exp
              wage: pmm wage i.urban i.race i.edu exp
              race: mlogit race i.urban wage i.edu exp
               edu: ologit edu i.urban wage i.race exp
               exp: pmm exp i.urban wage i.race i.edu
. 
. // test new models for convergence
. bysort female: reg exp i.urban i.race wage i.edu

----------------------------------------------------------------------------------------------------------------------------------
-&gt; female = 0

      Source |       SS       df       MS              Number of obs =     892
-------------+------------------------------           F(  7,   884) =   52.98
       Model |   24670.002     7    3524.286           Prob &gt; F      =  0.0000
    Residual |  58807.4441   884   66.524258           R-squared     =  0.2955
-------------+------------------------------           Adj R-squared =  0.2900
       Total |  83477.4461   891   93.689614           Root MSE      =  8.1562

------------------------------------------------------------------------------
         exp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     1.urban |  -.5563624   .5784369    -0.96   0.336    -1.691632    .5789075
             |
        race |
          1  |   1.799778   .6750693     2.67   0.008     .4748526    3.124704
          2  |   .8400697   .7025656     1.20   0.232    -.5388214    2.218961
             |
        wage |   .0001445   7.61e-06    18.99   0.000     .0001295    .0001594
             |
         edu |
          2  |  -2.074689   .7327518    -2.83   0.005    -3.512825   -.6365529
          3  |  -5.124519    .816809    -6.27   0.000     -6.72763   -3.521407
          4  |  -8.313709   1.337503    -6.22   0.000    -10.93876   -5.688656
             |
       _cons |   8.402518   .9403253     8.94   0.000     6.556987    10.24805
------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------------------
-&gt; female = 1

      Source |       SS       df       MS              Number of obs =     887
-------------+------------------------------           F(  7,   879) =   29.74
       Model |  13874.8765     7  1982.12521           Prob &gt; F      =  0.0000
    Residual |  58577.2689   879  66.6408065           R-squared     =  0.1915
-------------+------------------------------           Adj R-squared =  0.1851
       Total |  72452.1454   886  81.7744305           Root MSE      =  8.1634

------------------------------------------------------------------------------
         exp |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     1.urban |  -.6693439   .6690433    -1.00   0.317    -1.982453    .6437649
             |
        race |
          1  |   .9598328   .6860308     1.40   0.162    -.3866169    2.306282
          2  |   1.164061   .7025743     1.66   0.098     -.214858     2.54298
             |
        wage |    .000121   8.71e-06    13.89   0.000     .0001039    .0001381
             |
         edu |
          2  |  -1.867343   .8689937    -2.15   0.032    -3.572888   -.1617985
          3  |   -4.84498   .9371085    -5.17   0.000    -6.684211   -3.005748
          4  |  -7.366648   1.147116    -6.42   0.000    -9.618054   -5.115242
             |
       _cons |   8.896763   .8998265     9.89   0.000     7.130704    10.66282
------------------------------------------------------------------------------

. by female: logit urban exp i.race wage i.edu

----------------------------------------------------------------------------------------------------------------------------------
-&gt; female = 0

Iteration 0:   log likelihood = -576.14858  
Iteration 1:   log likelihood = -568.10266  
Iteration 2:   log likelihood = -568.08745  
Iteration 3:   log likelihood = -568.08745  

Logistic regression                               Number of obs   =        892
                                                  LR chi2(7)      =      16.12
                                                  Prob &gt; chi2     =     0.0240
Log likelihood = -568.08745                       Pseudo R2       =     0.0140

------------------------------------------------------------------------------
       urban |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         exp |  -.0083494    .008748    -0.95   0.340    -.0254951    .0087964
             |
        race |
          1  |  -.0250462   .1793684    -0.14   0.889    -.3766018    .3265095
          2  |  -.3721612   .1813618    -2.05   0.040    -.7276238   -.0166986
             |
        wage |   6.78e-06   2.36e-06     2.88   0.004     2.16e-06    .0000114
             |
         edu |
          2  |  -.1155276   .1933929    -0.60   0.550    -.4945708    .2635156
          3  |  -.2803377   .2178799    -1.29   0.198    -.7073745    .1466991
          4  |  -.3283938   .3534695    -0.93   0.353    -1.021181    .3643936
             |
       _cons |   .5155881   .2385894     2.16   0.031     .0479615    .9832148
------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------------------
-&gt; female = 1

Iteration 0:   log likelihood = -566.19162  
Iteration 1:   log likelihood = -450.72498  
Iteration 2:   log likelihood = -445.73919  
Iteration 3:   log likelihood = -445.57881  
Iteration 4:   log likelihood = -445.57813  
Iteration 5:   log likelihood = -445.57813  

Logistic regression                               Number of obs   =        887
                                                  LR chi2(7)      =     241.23
                                                  Prob &gt; chi2     =     0.0000
Log likelihood = -445.57813                       Pseudo R2       =     0.2130

------------------------------------------------------------------------------
       urban |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         exp |  -.0107927    .010163    -1.06   0.288    -.0307119    .0091265
             |
        race |
          1  |  -.6841661   .2108969    -3.24   0.001    -1.097516   -.2708159
          2  |  -1.259647   .2157675    -5.84   0.000    -1.682544   -.8367507
             |
        wage |   1.35e-06   2.93e-06     0.46   0.645    -4.39e-06    7.09e-06
             |
         edu |
          2  |   1.619022   .2371928     6.83   0.000     1.154132    2.083911
          3  |   2.681609   .2672965    10.03   0.000     2.157717      3.2055
          4  |    4.43645   .4644515     9.55   0.000     3.526142    5.346758
             |
       _cons |   -.531204   .2617202    -2.03   0.042    -1.044166   -.0182419
------------------------------------------------------------------------------

. by female: mlogit race exp i.urban wage i.edu

----------------------------------------------------------------------------------------------------------------------------------
-&gt; female = 0

Iteration 0:   log likelihood = -979.43224  
Iteration 1:   log likelihood = -935.80472  
Iteration 2:   log likelihood = -934.98446  
Iteration 3:   log likelihood = -934.97944  
Iteration 4:   log likelihood = -934.97944  

Multinomial logistic regression                   Number of obs   =        892
                                                  LR chi2(12)     =      88.91
                                                  Prob &gt; chi2     =     0.0000
Log likelihood = -934.97944                       Pseudo R2       =     0.0454

------------------------------------------------------------------------------
        race |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
0            |
         exp |  -.0274735   .0102909    -2.67   0.008    -.0476434   -.0073037
     1.urban |   .0262569   .1794679     0.15   0.884    -.3254937    .3780074
        wage |   6.28e-06   2.77e-06     2.27   0.023     8.60e-07    .0000117
             |
         edu |
          2  |  -.4080685    .206086    -1.98   0.048    -.8119896   -.0041474
          3  |  -.5017744   .2464439    -2.04   0.042    -.9847955   -.0187533
          4  |  -1.492324   .5722452    -2.61   0.009    -2.613904   -.3707442
             |
       _cons |   .2462701   .2715063     0.91   0.364    -.2858725    .7784127
-------------+----------------------------------------------------------------
1            |  (base outcome)
-------------+----------------------------------------------------------------
2            |
         exp |  -.0143098   .0102621    -1.39   0.163    -.0344231    .0058036
     1.urban |  -.3510997   .1744316    -2.01   0.044    -.6929795     -.00922
        wage |   9.35e-07   2.75e-06     0.34   0.734    -4.46e-06    6.33e-06
             |
         edu |
          2  |   .3857799   .2450301     1.57   0.115    -.0944702      .86603
          3  |   1.092693   .2658459     4.11   0.000     .5716446    1.613741
          4  |   1.673159   .4094672     4.09   0.000     .8706185      2.4757
             |
       _cons |   -.254461   .2948447    -0.86   0.388     -.832346    .3234241
------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------------------
-&gt; female = 1

Iteration 0:   log likelihood = -973.97087  
Iteration 1:   log likelihood = -934.18474  
Iteration 2:   log likelihood = -933.62658  
Iteration 3:   log likelihood = -933.62647  
Iteration 4:   log likelihood = -933.62647  

Multinomial logistic regression                   Number of obs   =        887
                                                  LR chi2(12)     =      80.69
                                                  Prob &gt; chi2     =     0.0000
Log likelihood = -933.62647                       Pseudo R2       =     0.0414

------------------------------------------------------------------------------
        race |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
0            |
         exp |  -.0180992   .0106928    -1.69   0.091    -.0390567    .0028584
     1.urban |     1.2596   .2155262     5.84   0.000     .8371769    1.682024
        wage |   6.92e-06   3.05e-06     2.27   0.023     9.36e-07    .0000129
             |
         edu |
          2  |  -1.015529   .2869699    -3.54   0.000     -1.57798   -.4530781
          3  |  -1.873085   .3132415    -5.98   0.000    -2.487027   -1.259143
          4  |  -2.917669   .3964615    -7.36   0.000    -3.694719   -2.140619
             |
       _cons |   .3468113    .282675     1.23   0.220    -.2072216    .9008442
-------------+----------------------------------------------------------------
1            |
         exp |  -.0030983   .0100496    -0.31   0.758    -.0227951    .0165985
     1.urban |   .5720785   .1999865     2.86   0.004     .1801122    .9640448
        wage |   2.49e-06   2.89e-06     0.86   0.388    -3.17e-06    8.16e-06
             |
         edu |
          2  |  -.7094723   .2814518    -2.52   0.012    -1.261108    -.157837
          3  |  -1.229685   .3028501    -4.06   0.000    -1.823261     -.63611
          4  |  -1.319989   .3548621    -3.72   0.000    -2.015506   -.6244719
             |
       _cons |   .4289424    .276908     1.55   0.121    -.1137873     .971672
-------------+----------------------------------------------------------------
2            |  (base outcome)
------------------------------------------------------------------------------

. by female: reg wage exp i.urban i.race i.edu

----------------------------------------------------------------------------------------------------------------------------------
-&gt; female = 0

      Source |       SS       df       MS              Number of obs =     892
-------------+------------------------------           F(  7,   884) =   74.03
       Model |  4.7872e+11     7  6.8388e+10           Prob &gt; F      =  0.0000
    Residual |  8.1660e+11   884   923758881           R-squared     =  0.3696
-------------+------------------------------           Adj R-squared =  0.3646
       Total |  1.2953e+12   891  1.4538e+09           Root MSE      =   30393

------------------------------------------------------------------------------
        wage |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         exp |   2005.904   105.6246    18.99   0.000       1798.6    2213.208
     1.urban |   6217.463   2146.452     2.90   0.004     2004.727     10430.2
             |
        race |
          1  |  -5654.544     2518.5    -2.25   0.025    -10597.48   -711.6073
          2  |  -4775.141   2615.229    -1.83   0.068    -9907.923    357.6405
             |
         edu |
          2  |   10720.55   2719.075     3.94   0.000     5383.955    16057.15
          3  |   28231.17   2962.326     9.53   0.000     22417.16    34045.18
          4  |    50933.9   4794.995    10.62   0.000        41523     60344.8
             |
       _cons |   30542.89   3511.689     8.70   0.000     23650.67    37435.11
------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------------------
-&gt; female = 1

      Source |       SS       df       MS              Number of obs =     887
-------------+------------------------------           F(  7,   879) =   54.13
       Model |  3.1047e+11     7  4.4353e+10           Prob &gt; F      =  0.0000
    Residual |  7.2028e+11   879   819436657           R-squared     =  0.3012
-------------+------------------------------           Adj R-squared =  0.2956
       Total |  1.0308e+12   886  1.1634e+09           Root MSE      =   28626

------------------------------------------------------------------------------
        wage |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         exp |   1488.074   107.0972    13.89   0.000     1277.878     1698.27
     1.urban |   1266.729   2347.021     0.54   0.590     -3339.69    5873.148
             |
        race |
          1  |  -3442.619   2405.519    -1.43   0.153    -8163.851    1278.613
          2  |  -5492.975   2460.533    -2.23   0.026    -10322.18   -663.7687
             |
         edu |
          2  |   5988.045   3048.533     1.96   0.050     4.792275     11971.3
          3  |   26068.51   3217.693     8.10   0.000     19753.25    32383.77
          4  |   41106.03   3875.211    10.61   0.000     33500.29    48711.78
             |
       _cons |   25087.89   3216.737     7.80   0.000     18774.51    31401.27
------------------------------------------------------------------------------

. by female: ologit edu exp i.urban i.race wage

----------------------------------------------------------------------------------------------------------------------------------
-&gt; female = 0

Iteration 0:   log likelihood = -1092.3176  
Iteration 1:   log likelihood = -986.99706  
Iteration 2:   log likelihood =  -984.5232  
Iteration 3:   log likelihood = -984.51851  
Iteration 4:   log likelihood = -984.51851  

Ordered logistic regression                       Number of obs   =        892
                                                  LR chi2(5)      =     215.60
                                                  Prob &gt; chi2     =     0.0000
Log likelihood = -984.51851                       Pseudo R2       =     0.0987

------------------------------------------------------------------------------
         edu |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         exp |   -.059575   .0078897    -7.55   0.000    -.0750385   -.0441115
     1.urban |  -.1649053   .1336636    -1.23   0.217    -.4268811    .0970705
             |
        race |
          1  |    .420148   .1571087     2.67   0.007     .1122206    .7280753
          2  |   1.275139   .1623148     7.86   0.000     .9570078     1.59327
             |
        wage |   .0000242   2.09e-06    11.58   0.000     .0000201    .0000283
-------------+----------------------------------------------------------------
       /cut1 |  -.1678223    .205886                     -.5713513    .2357068
       /cut2 |   2.064867    .217243                      1.639079    2.490656
       /cut3 |   4.553283   .2681615                      4.027696     5.07887
------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------------------
-&gt; female = 1

Iteration 0:   log likelihood = -1172.2304  
Iteration 1:   log likelihood = -973.18412  
Iteration 2:   log likelihood = -964.74324  
Iteration 3:   log likelihood = -964.72408  
Iteration 4:   log likelihood = -964.72408  

Ordered logistic regression                       Number of obs   =        887
                                                  LR chi2(5)      =     415.01
                                                  Prob &gt; chi2     =     0.0000
Log likelihood = -964.72408                       Pseudo R2       =     0.1770

------------------------------------------------------------------------------
         edu |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         exp |  -.0555539   .0078704    -7.06   0.000    -.0709795   -.0401282
     1.urban |   1.938634   .1527146    12.69   0.000     1.639319    2.237949
             |
        race |
          1  |   .6760943   .1598405     4.23   0.000     .3628128    .9893759
          2  |   1.299797   .1621729     8.01   0.000      .981944     1.61765
             |
        wage |   .0000265   2.22e-06    11.93   0.000     .0000221    .0000309
-------------+----------------------------------------------------------------
       /cut1 |   .5531119   .1965451                      .1678906    .9383332
       /cut2 |   2.799431   .2172816                      2.373567    3.225295
       /cut3 |   5.078792   .2594613                      4.570257    5.587326
------------------------------------------------------------------------------

. // for real work you would explore misspecification of refined models as well
. 
. 
. // test convergence of imputation process
. // since by() and savetrace() don't get along right now, we'll remove by() then throw away these imputations and do them with by
&gt; () but no savetrace().
. preserve

. mi impute chained (logit) urban (mlogit) race (ologit) edu (pmm) exp wage = female, add(5) rseed(88) savetrace(extrace, replace)
&gt;  burnin(100)

Conditional models:
             urban: logit urban i.race exp wage i.edu female
              race: mlogit race i.urban exp wage i.edu female
               exp: pmm exp i.urban i.race wage i.edu female
              wage: pmm wage i.urban i.race exp i.edu female
               edu: ologit edu i.urban i.race exp wage female

Performing chained iterations ...

Multivariate imputation                     Imputations =        5
Chained equations                                 added =        5
Imputed: m=1 through m=5                        updated =        0

Initialization: monotone                     Iterations =      500
                                                burn-in =      100

             urban: logistic regression
              race: multinomial logistic regression
               edu: ordered logistic regression
               exp: predictive mean matching
              wage: predictive mean matching

------------------------------------------------------------------
                   |               Observations per m             
                   |----------------------------------------------
          Variable |   Complete   Incomplete   Imputed |     Total
-------------------+-----------------------------------+----------
             urban |       2727          273       273 |      3000
              race |       2707          293       293 |      3000
               edu |       2681          319       319 |      3000
               exp |       2707          293       293 |      3000
              wage |       2701          299       299 |      3000
------------------------------------------------------------------
(complete + incomplete = total; imputed is the minimum across m
 of the number of filled-in observations.)

. 
. use extrace, replace
(Summaries of imputed values from -mi impute chained-)

. reshape wide *mean *sd, i(iter) j(m)
(note: j = 1 2 3 4 5)

Data                               long   -&gt;   wide
-----------------------------------------------------------------------------
Number of obs.                      505   -&gt;     101
Number of variables                  12   -&gt;      51
j variable (5 values)                 m   -&gt;   (dropped)
xij variables:
                             urban_mean   -&gt;   urban_mean1 urban_mean2 ... urban_mean5
                              race_mean   -&gt;   race_mean1 race_mean2 ... race_mean5
                               exp_mean   -&gt;   exp_mean1 exp_mean2 ... exp_mean5
                              wage_mean   -&gt;   wage_mean1 wage_mean2 ... wage_mean5
                               edu_mean   -&gt;   edu_mean1 edu_mean2 ... edu_mean5
                               urban_sd   -&gt;   urban_sd1 urban_sd2 ... urban_sd5
                                race_sd   -&gt;   race_sd1 race_sd2 ... race_sd5
                                 exp_sd   -&gt;   exp_sd1 exp_sd2 ... exp_sd5
                                wage_sd   -&gt;   wage_sd1 wage_sd2 ... wage_sd5
                                 edu_sd   -&gt;   edu_sd1 edu_sd2 ... edu_sd5
-----------------------------------------------------------------------------

. tsset iter
        time variable:  iter, 0 to 100
                delta:  1 unit

. tsline exp_mean*, title("Mean of Imputed Values of Experience") note("Each line is for one imputation") legend(off)

. graph export conv1.png, replace
(file conv1.png written in PNG format)
<img alt="Convergence check, mean of exp" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/conv1.png" width="585"/>
. tsline exp_sd*, title("Standard Deviation of Imputed Values of Experience") note("Each line is for one imputation") legend(off)

. graph export conv2.png, replace
(file conv2.png written in PNG format)
<img alt="Convergence check, sd of exp" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/conv2.png" width="585"/>
. restore

. 
. 
. // "real" imputation
. mi impute chained (logit) urban (mlogit) race (ologit) edu (pmm) exp wage = i.female, add(5) rseed(88) by(female)

Performing setup for each by() group:

-&gt; female = 0
Conditional models:
               exp: pmm exp i.urban i.race wage i.edu i.female
             urban: logit urban exp i.race wage i.edu i.female
              race: mlogit race exp i.urban wage i.edu i.female
              wage: pmm wage exp i.urban i.race i.edu i.female
               edu: ologit edu exp i.urban i.race wage i.female

-&gt; female = 1
Conditional models:
             urban: logit urban wage i.race i.edu exp i.female
              wage: pmm wage i.urban i.race i.edu exp i.female
              race: mlogit race i.urban wage i.edu exp i.female
               edu: ologit edu i.urban wage i.race exp i.female
               exp: pmm exp i.urban wage i.race i.edu i.female

Performing imputation for each by() group:

-&gt; female = 0
Performing chained iterations ...

-&gt; female = 1
Performing chained iterations ...

Multivariate imputation                     Imputations =        5
Chained equations                                 added =        5
Imputed: m=1 through m=5                        updated =        0

Initialization: monotone                     Iterations =       50
                                                burn-in =       10

             urban: logistic regression
              race: multinomial logistic regression
               edu: ordered logistic regression
               exp: predictive mean matching
              wage: predictive mean matching

------------------------------------------------------------------
                   |               Observations per m             
by()               |----------------------------------------------
          Variable |   Complete   Incomplete   Imputed |     Total
-------------------+-----------------------------------+----------
female = 0         |                                   |
             urban |       1369          143       143 |      1512
              race |       1364          148       148 |      1512
               edu |       1345          167       167 |      1512
               exp |       1372          140       140 |      1512
              wage |       1347          165       165 |      1512
                   |                                   |
female = 1         |                                   |
             urban |       1358          130       130 |      1488
              race |       1343          145       145 |      1488
               edu |       1336          152       152 |      1488
               exp |       1335          153       153 |      1488
              wage |       1354          134       134 |      1488
                   |                                   |
-------------------+-----------------------------------+----------
Overall            |                                   |
             urban |       2727          273       273 |      3000
              race |       2707          293       293 |      3000
               edu |       2681          319       319 |      3000
               exp |       2707          293       293 |      3000
              wage |       2701          299       299 |      3000
------------------------------------------------------------------
(complete + incomplete = total; imputed is the minimum across m
 of the number of filled-in observations.)
<a id="checkcat" name="checkcat"></a>
. 
. // check if imputed values match observed values
. foreach var of varlist urban race edu {
  2.         mi xeq 0: tab `var'
  3.         mi xeq 1/5: tab `var' if miss_`var'
  4. }

m=0 data:
-&gt; tab urban

      urban |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |        921       33.77       33.77
          1 |      1,806       66.23      100.00
------------+-----------------------------------
      Total |      2,727      100.00

m=1 data:
-&gt; tab urban if miss_urban

      urban |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |        102       37.36       37.36
          1 |        171       62.64      100.00
------------+-----------------------------------
      Total |        273      100.00

m=2 data:
-&gt; tab urban if miss_urban

      urban |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |         97       35.53       35.53
          1 |        176       64.47      100.00
------------+-----------------------------------
      Total |        273      100.00

m=3 data:
-&gt; tab urban if miss_urban

      urban |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |        107       39.19       39.19
          1 |        166       60.81      100.00
------------+-----------------------------------
      Total |        273      100.00

m=4 data:
-&gt; tab urban if miss_urban

      urban |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |        102       37.36       37.36
          1 |        171       62.64      100.00
------------+-----------------------------------
      Total |        273      100.00

m=5 data:
-&gt; tab urban if miss_urban

      urban |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |         97       35.53       35.53
          1 |        176       64.47      100.00
------------+-----------------------------------
      Total |        273      100.00

m=0 data:
-&gt; tab race

       race |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |        864       31.92       31.92
          1 |        929       34.32       66.24
          2 |        914       33.76      100.00
------------+-----------------------------------
      Total |      2,707      100.00

m=1 data:
-&gt; tab race if miss_race

       race |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |         97       33.11       33.11
          1 |        113       38.57       71.67
          2 |         83       28.33      100.00
------------+-----------------------------------
      Total |        293      100.00

m=2 data:
-&gt; tab race if miss_race

       race |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |        107       36.52       36.52
          1 |         88       30.03       66.55
          2 |         98       33.45      100.00
------------+-----------------------------------
      Total |        293      100.00

m=3 data:
-&gt; tab race if miss_race

       race |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |        101       34.47       34.47
          1 |         98       33.45       67.92
          2 |         94       32.08      100.00
------------+-----------------------------------
      Total |        293      100.00

m=4 data:
-&gt; tab race if miss_race

       race |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |        119       40.61       40.61
          1 |         77       26.28       66.89
          2 |         97       33.11      100.00
------------+-----------------------------------
      Total |        293      100.00

m=5 data:
-&gt; tab race if miss_race

       race |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |         76       25.94       25.94
          1 |        116       39.59       65.53
          2 |        101       34.47      100.00
------------+-----------------------------------
      Total |        293      100.00

m=0 data:
-&gt; tab edu

            edu |      Freq.     Percent        Cum.
----------------+-----------------------------------
  &lt; High School |        511       19.06       19.06
    High School |        996       37.15       56.21
      Bachelors |        878       32.75       88.96
Advanced Degree |        296       11.04      100.00
----------------+-----------------------------------
          Total |      2,681      100.00

m=1 data:
-&gt; tab edu if miss_edu

            edu |      Freq.     Percent        Cum.
----------------+-----------------------------------
  &lt; High School |         50       15.67       15.67
    High School |        135       42.32       57.99
      Bachelors |         98       30.72       88.71
Advanced Degree |         36       11.29      100.00
----------------+-----------------------------------
          Total |        319      100.00

m=2 data:
-&gt; tab edu if miss_edu

            edu |      Freq.     Percent        Cum.
----------------+-----------------------------------
  &lt; High School |         53       16.61       16.61
    High School |        129       40.44       57.05
      Bachelors |        109       34.17       91.22
Advanced Degree |         28        8.78      100.00
----------------+-----------------------------------
          Total |        319      100.00

m=3 data:
-&gt; tab edu if miss_edu

            edu |      Freq.     Percent        Cum.
----------------+-----------------------------------
  &lt; High School |         60       18.81       18.81
    High School |        124       38.87       57.68
      Bachelors |        105       32.92       90.60
Advanced Degree |         30        9.40      100.00
----------------+-----------------------------------
          Total |        319      100.00

m=4 data:
-&gt; tab edu if miss_edu

            edu |      Freq.     Percent        Cum.
----------------+-----------------------------------
  &lt; High School |         62       19.44       19.44
    High School |        124       38.87       58.31
      Bachelors |         93       29.15       87.46
Advanced Degree |         40       12.54      100.00
----------------+-----------------------------------
          Total |        319      100.00

m=5 data:
-&gt; tab edu if miss_edu

            edu |      Freq.     Percent        Cum.
----------------+-----------------------------------
  &lt; High School |         55       17.24       17.24
    High School |        138       43.26       60.50
      Bachelors |         93       29.15       89.66
Advanced Degree |         33       10.34      100.00
----------------+-----------------------------------
          Total |        319      100.00

.<a id="checkcont" name="checkcont"></a>
. foreach var of varlist wage exp {
  2.         mi xeq 0: sum `var'
  3.         mi xeq 1/5: sum `var' if miss_`var'
  4.         mi xeq 0: kdensity `var'; graph export chk`var'0.png, replace
  5.         forval i=1/5 {
  6.                 mi xeq `i': kdensity `var' if miss_`var'; graph export chk`var'`i'.png, replace
  7.         }
  8. }

m=0 data:
-&gt; sum wage

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
        wage |      2701    71493.95     38104.3          0   227465.2

m=1 data:
-&gt; sum wage if miss_wage

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
        wage |       299    73701.88    38620.86          0   192810.8

m=2 data:
-&gt; sum wage if miss_wage

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
        wage |       299    75122.22    38976.49          0   193577.9

m=3 data:
-&gt; sum wage if miss_wage

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
        wage |       299    73354.54    40547.16          0   193577.9

m=4 data:
-&gt; sum wage if miss_wage

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
        wage |       299    75166.36    40163.56          0   193577.9

m=5 data:
-&gt; sum wage if miss_wage

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
        wage |       299    75681.66    41793.81          0   198598.6

m=0 data:
-&gt; kdensity wage
-&gt; graph export chkwage0.png, replace
(file chkwage0.png written in PNG format)
<img alt="kdensity of observed wages" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkwage0.png" width="585"/>
m=1 data:
-&gt; kdensity wage if miss_wage
-&gt; graph export chkwage1.png, replace
(file chkwage1.png written in PNG format)
<img alt="kdensity of imputed wages" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkwage1.png" width="585"/>
m=2 data:
-&gt; kdensity wage if miss_wage
-&gt; graph export chkwage2.png, replace
(file chkwage2.png written in PNG format)
<img alt="kdensity of imputed wages" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkwage2.png" width="585"/>
m=3 data:
-&gt; kdensity wage if miss_wage
-&gt; graph export chkwage3.png, replace
(file chkwage3.png written in PNG format)
<img alt="kdensity of imputed wages" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkwage3.png" width="585"/>
m=4 data:
-&gt; kdensity wage if miss_wage
-&gt; graph export chkwage4.png, replace
(file chkwage4.png written in PNG format)
<img alt="kdensity of imputed wages" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkwage4.png" width="585"/>
m=5 data:
-&gt; kdensity wage if miss_wage
-&gt; graph export chkwage5.png, replace
(file chkwage5.png written in PNG format)
<img alt="kdensity of imputed wages" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkwage5.png" width="585"/>
m=0 data:
-&gt; sum exp

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         exp |      2707    15.57284    9.656566          0    47.8623

m=1 data:
-&gt; sum exp if miss_exp

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         exp |       293    14.98541     10.0319          0   46.35374

m=2 data:
-&gt; sum exp if miss_exp

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         exp |       293    15.42685    10.09567          0   46.35374

m=3 data:
-&gt; sum exp if miss_exp

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         exp |       293    15.19209    9.870792          0   41.14571

m=4 data:
-&gt; sum exp if miss_exp

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         exp |       293    14.67198    10.40626          0    47.8623

m=5 data:
-&gt; sum exp if miss_exp

    Variable |       Obs        Mean    Std. Dev.       Min        Max
-------------+--------------------------------------------------------
         exp |       293    14.94231    9.530698          0   46.35374

m=0 data:
-&gt; kdensity exp
-&gt; graph export chkexp0.png, replace
(file chkexp0.png written in PNG format)
<img alt="kdensity of observed experience" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkexp0.png" width="585"/>
m=1 data:
-&gt; kdensity exp if miss_exp
-&gt; graph export chkexp1.png, replace
(file chkexp1.png written in PNG format)
<img alt="kdensity of imputed experience" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkexp1.png" width="585"/>
m=2 data:
-&gt; kdensity exp if miss_exp
-&gt; graph export chkexp2.png, replace
(file chkexp2.png written in PNG format)
<img alt="kdensity of imputed experience" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkexp2.png" width="585"/>
m=3 data:
-&gt; kdensity exp if miss_exp
-&gt; graph export chkexp3.png, replace
(file chkexp3.png written in PNG format)
<img alt="kdensity of imputed experience" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkexp3.png" width="585"/>
m=4 data:
-&gt; kdensity exp if miss_exp
-&gt; graph export chkexp4.png, replace
(file chkexp4.png written in PNG format)
<img alt="kdensity of imputed experience" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkexp4.png" width="585"/>
m=5 data:
-&gt; kdensity exp if miss_exp
-&gt; graph export chkexp5.png, replace
(file chkexp5.png written in PNG format)
<img alt="kdensity of imputed experience" height="426" src="https://ssc.wisc.edu/sscc/pubs/mi/chkexp5.png" width="585"/>
. 
. save mi1,replace
file mi1.dta saved

. log close
      name:  <unnamed>
       log:  \sscc\pubs\mi\miex.log
  log type:  text
 closed on:  17 Aug 2012, 13:11:21
----------------------------------------------------------------------------------------------------------------------------------

</unnamed></unnamed></pre>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url>https://ssc.wisc.edu/sscc/pubs/mi/exp1.png, https://ssc.wisc.edu/sscc/pubs/mi/exp2.png, https://ssc.wisc.edu/sscc/pubs/mi/wage.png, https://ssc.wisc.edu/sscc/pubs/mi/conv1.png, https://ssc.wisc.edu/sscc/pubs/mi/conv2.png, https://ssc.wisc.edu/sscc/pubs/mi/chkwage0.png, https://ssc.wisc.edu/sscc/pubs/mi/chkwage1.png, https://ssc.wisc.edu/sscc/pubs/mi/chkwage2.png, https://ssc.wisc.edu/sscc/pubs/mi/chkwage3.png, https://ssc.wisc.edu/sscc/pubs/mi/chkwage4.png, https://ssc.wisc.edu/sscc/pubs/mi/chkwage5.png, https://ssc.wisc.edu/sscc/pubs/mi/chkexp0.png, https://ssc.wisc.edu/sscc/pubs/mi/chkexp1.png, https://ssc.wisc.edu/sscc/pubs/mi/chkexp2.png, https://ssc.wisc.edu/sscc/pubs/mi/chkexp3.png, https://ssc.wisc.edu/sscc/pubs/mi/chkexp4.png, https://ssc.wisc.edu/sscc/pubs/mi/chkexp5.png</img_base_url>
</kb_document>
<kb_document>
<kb_title>Multiple Imputation in Stata</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p>Many SSCC members are eager to use multiple imputation in their research, or have been told they should be by reviewers or advisors. This series is intended to be a practical guide to the technique and its implementation in Stata, based on the questions SSCC members are asking the SSCC's statistical computing consultants.</p>
<p>The series assumes you are already familiar with the basic concepts of multiple imputation and is not intended as a substitute for a study of the literature on it. White, Royston and Wood's article <a href="http://onlinelibrary.wiley.com.ezproxy.library.wisc.edu/doi/10.1002/sim.4067/pdf">Multiple Imputation using chained equations: Issues and guidance for practice</a> (Statistics in Medicine, November 2010) is a good starting point for that study.</p>
<p>The article  also assumes you are familiar with Stata usage and syntax. If you are not, we suggest working through our <a href="https://ssc.wisc.edu/sscc/pubs/sfr-intro.htm">Stata for Researchers</a> series and (optionally but usefully) <a href="https://ssc.wisc.edu/sscc/pubs/stata_prog1.htm">Stata Programming Essentials</a>. We also note that the Stata documentation on the <span class="InputCode">mi</span> commands used for multiple imputation is very good and much broader than this series, which will focus on the most common scenarios among SSCC members.</p>
<p>One of the best ways to get an intuitive sense for how multiple imputation works is to run examples using constructed data sets where the right answers are known. We've created a <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm">set of examples</a> to go along with this series. For the sake of narrative continuity they have been placed in a separate article, but you'll find links at the appropriate places. We strongly suggest you read them, but ideally you'll run the provided code yourself and experiment with changing it.</p>
<p>This series currently includes the following sections:</p>
<ol>
<li><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_intro.htm">Introduction</a></li>
<li><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_decide.htm">Deciding to Impute</a></li>
<li><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_models.htm">Creating Imputation Models</a></li>
<li><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_impute.htm">Imputing</a></li>
<li><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_manage.htm">Managing Multiply Imputed Data</a></li>
<li><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_estimate.htm">Estimating</a></li>
<li><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm">Examples</a></li>
<li><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_readings.htm">Recommended Readings</a></li>
</ol>
<p>Next: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_decide.htm">Deciding to Impute</a></p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Multiple Imputation in Stata: Managing Multiply Imputed Data</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p class="intro"><em>This is part five of the Multiple Imputation in Stata series. For a list of topics covered by this series, see the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_intro.htm">Introduction</a>.</em></p>
<p>In many cases you can avoid managing multiply imputed data completely. <strong>Wherever possible, do any needed data cleaning, recoding, restructuring, variable creation, or other data management tasks before imputing. </strong>Because this is not always possible, the <span class="InputCode">mi</span> framework includes tools for managing multiply imputed data. However, in practice we rarely see them used. (This section may be expanded in the future as issues arise.)</p>
<ul>
<li><a href="#miver">mi Versions of Data Management Commands</a>
<ul>
<li><a href="#set">Setting mi data</a></li>
</ul>
</li>
<li><a href="#miupdate">mi update</a></li>
<li><a href="#mixeq">mi xeq</a></li>
<li><a href="#vars">Creating or Changing Variables
                  </a>
<ul>
<li><a href="#regular">Regular Variables</a></li>
<li><a href="#passive">Passive Variables</a></li>
<li><a href="#supervarying">Super-varying Variables</a></li>
</ul>
</li>
<li><a href="#extract">mi extract</a></li>
<li><a href="#ice">mi import ice</a></li>
</ul>
<h2><a id="miver" name="miver"></a>mi Versions of Data Management Commands</h2>
<p>Once you <span class="InputCode">mi set</span> your data and add imputations to it, the imputed values are added to the data set as either additional observations or additional variables, depending on which structure you chose. Commands which do not take that into account may or may not give correct results. The <span class="InputCode">mi</span> versions of basic data management commands do take the <span class="InputCode">mi</span> structure into account, ensuring that the changes you make are applied to all the imputations properly. These include <span class="InputCode">mi merge</span>, <span class="InputCode">mi append</span>, <span class="InputCode">mi expand</span>, <span class="InputCode">mi rename</span>, and many others (see the <span class="InputCode">mi</span> documentation). For all these commands the syntax for the <span class="InputCode">mi</span> version is identical to that of the regular version other than having some additional options available which are related to multiply imputed data.</p>
<p>The command that's most commonly needed is <span class="InputCode">mi reshape</span>. Panel data (subjects observed over time) should be imputed in wide form where there is one observation per subject rather than one observation per subject per time period. That way the imputation model for a given variable in a given period can include values of the same variable in other periods, which are likely to be good predictors. However, analysis often requires the long form, where there is one observation per subject per period. Before imputing, switch to the wide form with:</p>
<p class="InputCode"> reshape wide...</p>
<p>After imputing, switch back with:</p>
<p class="InputCode"> mi reshape long...</p>
<p>The two commands will be identical except for adding <span class="InputCode">mi</span> and changing <span class="InputCode">wide</span> to <span class="InputCode">long</span>.</p>
<p>See the <a href="https://ssc.wisc.edu/sscc/pubs/sfr-hier.htm">Hierarchical Data</a> section of <a href="https://ssc.wisc.edu/sscc/pubs/sfr-intro.htm">Stata for Researchers</a> for more discussion of <span class="InputCode">reshape</span> and long vs. wide forms.</p>
<h3><a id="set" name="set"></a>Setting mi Data</h3>
<p>Commands like <span class="InputCode">svyset</span>, <span class="InputCode">tsset</span>, and <span class="InputCode">xtset</span> also have <span class="InputCode">mi</span> versions: <span class="InputCode">mi svyset</span>, <span class="InputCode">mi tsset</span>, <span class="InputCode">mi xtset</span>, etc. If you set your data before imputing (using the regular version of the command) it will still be set  after imputing. If you need to set it after imputing, use the <span class="InputCode">mi</span> version.</p>
<p>Keep in mind that <span class="InputCode">mi impute chained</span> cannot correct for survey structure. See the section on Survey Data in <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_models.htm#SurveyData">Creating Imputation Models</a> for more discussion.</p>
<h2><a id="miupdate" name="miupdate"></a>mi update</h2>
<p>Certain things should always be true in multiply imputed data sets. For example, <em>regular</em> (unimputed) variables should have the same values in all imputations. The <span class="InputCode">mi update</span> command will check that this is so and fix any problems it finds (in this case, by setting the value in all imputations to the value in the observed data).</p>
<p>You may never need to run an <span class="InputCode">mi update</span> yourself, because each <span class="InputCode">mi</span> command that changes your data also runs  <span class="InputCode">mi update</span> afterwards. While this is automatic, you should be aware of it for two reasons. First, if you're running a string of data management commands there's no need to do an <span class="InputCode">mi update</span> after each one. If your data set is big enough that the process is taking significant time, consider adding the <span class="InputCode">noupdate</span> option to all but the last command. Second, if you introduce <em>super-varying</em> variables or make other changes that <span class="InputCode">mi update</span> could find problematic, you need to be sure that <span class="InputCode">mi update</span> won't change your data inappropriately. For <em>super-varying</em> variables, that just means you shouldn't register them (see the section on <a href="#supervarying"><em>super-varying</em> variables</a>). But if you're making complicated data changes you should read the <span class="InputCode">mi update</span> documentation carefully so that you know what it will do.</p>
<p> On the other hand, some data management commands do not have an <span class="InputCode">mi</span> version (<span class="InputCode">drop</span> is an example). You should  run <span class="InputCode">mi update</span> yourself after using one of them.</p>
<h2><a id="mixeq" name="mixeq"></a>mi xeq</h2>
<p>The <span class="InputCode">mi xeq</span> command allows you to act on your imputations one at a time. Since we needed this capability to check on our imputation results it was introduced in <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_impute.htm#mixeq">Imputing</a>. Be sure you're familiar with it before continuing.</p>
<h2><a id="vars" name="vars"></a>Creating or Changing Variables</h2>
<p>The process for creating or changing a variable depends on whether the variable is a <em>regular</em> variable, a <em>passive</em> variable, or a <em>super-varying</em> variable.</p>
<h3><a id="regular" name="regular"></a>Regular Variables</h3>
<p>New or changed variables that are functions of existing <em>regular</em> variables are also <em>regular</em> variables. They will have the same value in every imputation. You can create new <em>regular</em> variables or change the values of existing <em>regular</em> variables using <span class="InputCode">mi xeq</span> plus the standard <span class="InputCode">gen</span>, <span class="InputCode">egen</span>, and <span class="InputCode">replace</span> commands. You should register new <em>regular</em> variables as such, though it's not required.</p>
<p class="InputCode">mi xeq: gen lnIncome=ln(income)<br/>
                  mi register regular lnIncome
                </p>
<p>(Assuming <span class="InputCode">income</span> is <em>regular</em>, not <em>imputed</em>.)</p>
<h3><a id="passive" name="passive"></a>Passive Variables</h3>
<p><em>Passive</em> variables are functions of <em>imputed</em> variables. Thus they will have different values in different imputations. They can be created or changed using <span class="InputCode">mi passive</span> followed by the standard <span class="InputCode">gen</span> or <span class="InputCode">replace</span> commands.  However, <span class="InputCode">mi passive</span> should only be used with <span class="InputCode">egen</span> for functions that  depend solely on the current observation, like <span class="InputCode">rowtotal()</span>. Functions like <span class="InputCode">total()</span> or <span class="InputCode">mean()</span> create <em>super-varying</em> variables. Using <span class="InputCode">mi passive</span> automatically registers new variables as <em>passive</em>.</p>
<p class="InputCode">mi passive: gen lnIncome=ln(income)</p>
<p>(Now assuming <span class="InputCode">income</span> is <em>imputed</em>.)                </p>
<p><em>Passive</em> variables are not automatically changed if the variables they are based on change. If you need to update <em>passive</em> variables, the easiest way is probably to drop the existing versions and then rerun the commands that created them in the first place.</p>
<p><em>Passive</em> variables are often problematic—the examples on <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#Transformations">transformations</a>, <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#Non-Linearity">non-linearity</a>, and <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#Interactions">interactions</a> show how using them inappropriately can lead to biased estimates.                </p>
<h3><a id="supervarying" name="supervarying"></a>Super-varying Variables</h3>
<p>Normally, if a case is complete (has no missing values) it will be identical in all imputations. But consider a household income variable which is the total of all the individual incomes in the household: if one person's income is missing and must be imputed, then  household income for everyone in that person's household will be different in each imputation, <em>even  people who are complete cases</em>. Variables with the property that they vary between imputations even for complete cases are known as <em>super-varying</em> variables. Variables that are functions of the values of imputed variables for other observations are likely to be <em>super-varying</em>. Functions that depend on values in other observations include most <span class="InputCode">egen</span> functions, but also <span class="InputCode">gen</span> or <span class="InputCode">replace</span> expressions <span class="InputCode"></span>that use square brackets (<span class="InputCode">x[1]</span> or <span class="InputCode">x[_n+1]</span>, for example).</p>
<p>If you need to create <em>super-varying</em> variables, switch to the <span class="InputCode">flong</span> format (or <span class="InputCode">flongsep</span> if you don't have enough memory for <span class="InputCode">flong</span>). In <span class="InputCode">flong</span>, there is one copy of each observation for each imputation, even for complete cases. Thus <em>super-varying</em> variables can have different values in each copy. The <span class="InputCode">mlong</span> and <span class="InputCode">wide</span> formats save memory by storing just one copy of complete cases, but this makes them unable to store <em>super-varying</em> variables.</p>
<p><em>Super-varying</em> variables should be created or changed  using <span class="InputCode">mi xeq</span> and the standard <span class="InputCode">gen</span>, <span class="InputCode">egen</span>, or <span class="InputCode">replace</span> commands (most likely <span class="InputCode">egen</span>).</p>
<p class="InputCode">mi convert flong, clear<br/>
                mi xeq: egen householdIncome=total(income)</p>
<p><strong>Super-varying variables must not be registered</strong>. While they are theoretically passive variables, registering them as either <em>passive</em> or <em>regular</em> will prompt <span class="InputCode">mi update</span> to apply the (normally true) constraint that complete cases do not vary between imputations and "correct" their values. Leaving <em>super-varying</em> variables unregistered makes <span class="InputCode">mi update</span> leave them alone, but they'll still work in estimation commands.</p>
<p>All the statistical concerns raised by <em>passive</em> variables also apply to <em>super-varying</em> variables.</p>
<h2><a id="extract" name="extract"></a>mi extract</h2>
<p>Sometimes you need to work with an individual imputation as a regular data set, ignoring the fact that it was imputed. Also, some special purpose software like HLM can work with multiply imputed data but expects that you will put each imputation in a separate file.</p>
<p>The tool for doing selecting a single imputation and turning it into a regular data set is <span class="InputCode">mi extract</span>, and the syntax is very simple:</p>
<p class="InputCode">mi extract <span class="Parameter">n</span></p>
<p> where <span class="Parameter">n</span> is the number of the imputation you want to work with. After extraction, the data will not be <span class="InputCode">mi set</span> and there will be no indication it was ever imputed. <span class="Parameter">n</span> can be 0, in which case <span class="InputCode">mi extract</span>  gives you the observed data, missing values and all.</p>
<p>HLM reads SPSS files, not Stata files, but you can call on Stat/Transfer to convert your data sets to SPSS format. If you have 10 imputations, the following code will extract each imputation, save it as a separate data set, then have Stat/Transfer convert it to SPSS format:</p>
<p class="InputCode">forval i=1/10 {<br/>
<span class="indent3">preserve</span><br/>
<span class="indent3">mi extract `i'</span><br/>
<span class="indent3">save hlm`i',replace</span><br/>
<span class="indent3">! "c:\program files (x86)\stattransfer11\st.exe" "hlm`i'.dta" "hlm`i'.sav"</span><br/>
<span class="indent3">restore</span><br/>
}<br/>
</p>
<p>The command to call Stat/Transfer is written to work on Winstat. If Stat/Transfer is located in a different directory on your computer, you will need to modify that line accordingly. On Linstat it can be simply:</p>
<p class="InputCode"><span class="indent3">! st hlm`i'.dta hlm`i'.sav</span></p>
<h2><a id="ice" name="ice"></a>mi import ice</h2>
<p>If you have imputed data using <span class="InputCode">ice</span>, <span class="InputCode">mi import ice</span> will convert it from <span class="InputCode">ice</span> format to <span class="InputCode">mi</span> format, allowing you to use <span class="InputCode">mi</span> commands. The <span class="InputCode">automatic</span> option tells <span class="InputCode">mi import ice</span> to  register all the variables and is highly recommended. It can tell which variables are  <em>regular</em>  by noting which ones never change between imputations, but it cannot distinguish between <em>imputed</em> and <em>passive</em> variables. If you have <em>passive</em> variables, use the <span class="InputCode">passive()</span> option and list them in the parentheses. If <span class="InputCode">mi import ice</span> finds that a variable is not <em>regular</em> and it is not listed in a <span class="InputCode">passive()</span> option, then it will mark the variable as <em>imputed</em>.</p>
<p>Example:</p>
<p class="InputCode">mi import ice, automatic passive(passiveVar1 passiveVar2)</p>
<p>Data imported from <span class="InputCode">ice</span> will be placed in the <span class="InputCode">flong</span> form, since that's essentially what <span class="InputCode">ice</span> uses. We suggest converting to <span class="InputCode">wide</span> or perhaps <span class="InputCode">mlong</span> (unless you have <em>super-varying</em> variables, which require <span class="InputCode">flong</span>):</p>
<p class="InputCode">mi convert wide, clear</p>
<p>Next: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_estimate.htm">Estimating</a></p>
<p>Previous: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_impute.htm">Imputing</a></p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Multiple Imputation in Stata</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p class="intro"><em>This is part three of the Multiple Imputation in Stata series. For a list of topics covered by this series, see the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_intro.htm">Introduction</a>.</em></p>
<p>In theory, an imputation model estimates the joint distribution of all the variables it contains. MICE breaks this problem  into a series of estimations that regress one variable  on all the other variables in the model. (The downside is that a series of models of the distributions of individual variables does not necessarily add up to a consistent model of the joint distribution.)</p>
<p> The <span class="InputCode">mi impute chained</span> command does not require you to specify the model for each variable separately: you just list the variables to be imputed along with information about how they should be imputed, and <span class="InputCode">mi impute chained</span> will form the individual models automatically. However, the success of the overall imputation model depends on the success of all the individual models. If a single model fails to converge, the imputation process as a whole will fail. If a single model is misspecified, it may bias the results of your analysis model. <strong>We  strongly recommend that you run each of the individual models on its own, outside the context of <span class="InputCode">mi impute chained</span>, to test for convergence and misspecification.</strong> We'll discuss the details of doing so in the next section. This section will focus on  issues you must consider in creating your imputation models.</p>
<ul>
<li><a href="#ChoosingVariables">Choosing Variables</a>
<ul>
<li><a href="#CustomizingImputationModels">Customizing Imputation Models</a></li>
<li><a href="#PanelLongitudinalData">Panel/Longitudinal Data</a></li>
<li><a href="#SurveyData">Survey Data </a></li>
</ul>
</li>
<li><a href="#ChoosingMethods">Choosing Methods</a>
<ul>
<li><a href="#ContinuousButNonNormalVariables">Continuous But Non-Normal Variables</a></li>
<li><a href="#Transformations">Transformations</a></li>
<li><a href="#BoundedVariables">Bounded Variables</a></li>
<li><a href="#NonLinearTerms">Non-Linear Terms</a></li>
<li><a href="#InteractionTerms">Interaction Terms</a></li>
<li><a href="#SetsofIndicatorVariables">Sets of Indicator Variables </a></li>
</ul>
</li>
</ul>
<h3><a id="ChoosingVariables" name="ChoosingVariables"></a>Choosing Variables</h3>
<p>The first step in creating an imputation model is deciding which variables to impute. The imputation model should always include all the variables in the analysis model. This includes the dependent variable of your analysis model, though there is some debate about whether the imputed values of the dependent variable should be used. Even if you don't plan to use the imputed values of the dependent variable, the observed values of the dependent variable provide information about the other variables, and the information available from those observations which are missing the dependent variable should be used in the imputation model as well.</p>
<p><a id="depvar" name="depvar"></a><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#ImputingtheDependentVariable">Example: Imputing the Dependent Variable</a></p>
<p>The imputation model should include any other variables that provide information either about the true values of the missing data or about their probability of being missing. Avoid creating a "kitchen sink" model however. Large numbers of variables, especially categorical variables, can lead to models that fail to converge. Use theory to guide you in choosing appropriate variables.</p>
<p>You can add variables to the imputation model that do not need to be (or shouldn't be) imputed by putting them at the end of the variable list following an equals sign.</p>
<h4><a id="CustomizingImputationModels" name="CustomizingImputationModels"></a>Customizing Imputation Models</h4>
<p>You can add variables to or remove variables from the imputation model for an individual variable or group of variables using the <span class="InputCode">include()</span> or <span class="InputCode">omit()</span> options. The <span class="InputCode">include()</span> option  even allows you add expressions to a model such as <span class="InputCode">(x^2)</span>, but they have to go inside an additional set of parentheses (e.g. <span class="InputCode">include((x^2)) </span>). These options go with the imputation method for a variable or variables (e.g. <span class="InputCode">(regress, include(x))</span> ) rather than at the end of the <span class="InputCode">mi impute chained</span> command.</p>
<p>Be cautious about adding expressions to imputation models: if <span class="InputCode">y</span> depends on some function of <span class="InputCode">x</span>, then <span class="InputCode">x</span> should depend on the inverse function of <span class="InputCode">y</span> and failing to model both  can bias your results. See <a href="#NonLinearTerms">Non-Linear Terms</a> for further discussion.</p>
<h4><a id="PanelLongitudinalData" name="PanelLongitudinalData"></a>Panel/Longitudinal Data</h4>
<p>If you have data where units are observed over time, the best predictors of a missing value in one period are likely the values of that variable in the previous and subsequent periods. However, the imputation model can only take advantage of this information if the data set is in wide form (one observation per unit, not one observation per unit per time period). You can convert back to long form after imputing if needed.</p>
<p>To convert the data to wide form before imputing, use <span class="InputCode">reshape</span>. To convert back to long form after imputing, use <span class="InputCode">mi reshape</span>. This has the same syntax as <span class="InputCode">reshape</span>, but makes sure the imputations are handled properly. If you're not familiar with <span class="InputCode">reshape</span>, see the <a href="https://ssc.wisc.edu/sscc/pubs/sfr-hier.htm">Hierarchical Data</a> section of <a href="https://ssc.wisc.edu/sscc/pubs/sfr-intro.htm">Stata for Researchers</a>.</p>
<h4><a id="SurveyData" name="SurveyData"></a>Survey Data</h4>
<p>The <span class="InputCode">mi estimate:</span> and <span class="InputCode">svy:</span> prefix commands can be used together (in that order) to run models on survey data that have been multiple imputed. However, <span class="InputCode">svy:</span> cannot be used with <span class="InputCode">mi impute chained</span>. You can apply weights (e.g.<span class="InputCode"> [pweight=weight]</span>) but not correct for other elements of survey structure like strata or PSU. The current recommendation is to include survey structure variables like strata and PSU in the imputation models as sets of indicator variables (e.g. <span class="InputCode">i.psu</span>). This is an area of ongoing research.</p>
<p></p>
<p> When you test your individual imputation models, we suggest running them first with the <span class="InputCode">svy:</span> prefix and then without it but with weights applied and  survey structure variables added to the model. If the two give very different results, try adding interactions between the survey structure variables or additional variables related to survey structure. If they continue to give very different results despite your best efforts, be wary about using multiple imputation.</p>
<h3><a id="ChoosingMethods" name="ChoosingMethods"></a>Choosing Methods</h3>
<p>There are nine  methods available for imputing a variable: <span class="InputCode">regress</span>, <span class="InputCode">pmm</span>, <span class="InputCode">truncreg</span>, <span class="InputCode">intreg</span>, <span class="InputCode">logit</span>, <span class="InputCode">ologit</span>, <span class="InputCode">mlogit</span>, <span class="InputCode">poisson</span> and <span class="InputCode">nbreg</span>. In most cases you'll choose the same imputation method you'd choose if you were going to model the variable normally: <span class="InputCode">regress</span> for most continuous variables, <span class="InputCode">logit</span> for binary variables, <span class="InputCode">mlogit</span> for unordered categorical variables, etc.</p>
<h4><a id="ContinuousButNonNormalVariables" name="ContinuousButNonNormalVariables"></a>Continuous But Non-Normal Variables</h4>
<p>Keep in mind that the standard <span class="InputCode">regress</span> implies a normal error term after controlling for the covariates. If you have a continuous variable that is not normal, <span class="InputCode">regress</span> may not give you a distribution of imputed values that matches the observed values very well.</p>
<p>An alternative is Predictive Mean Matching (PMM). PMM  is an ad hoc technique with little theory behind it, but it seems to work quite well in practice. PMM starts out by regressing the variable to be imputed on the covariates, and then drawing a set of coefficients from the results, taking into accout both the estimated coefficients and the uncertainty about them. Those coefficients are used to calculate a predicted value for all missing values. However, it then uses the predicted value for a given observation to identify those observations whose observed value of the variable are close to the predicted value and chooses one of them randomly to be the imputed value. If the observed values of a variable are not normal, PMM will usually produce a distribution of imputed values that matches the distribution of the observed values more closely than  regression.</p>
<p>The <span class="InputCode">knn()</span> option controls how many observations are considered as matches (based on their observed values of the variable being close to the predicted value for the observation being imputed). Recent work by <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4051964/">Morris, White and Royston</a> indicates that larger numbers of observations should be used than was standard practice in the past. They suggest at least 10, and more if your data set is very large (tens of thousands of observations or more).</p>
<p>Because PMM draws its imputed values from the observed values, it has the property that the imputed values will never be outside the range of the observed values. This makes it very useful for bounded variables  (discussed below). It can also be used for some non-continuous distributions. However, PMM is not appropriate if you have reason to believe the unobserved values are outside the range of the observed values.                </p>
<p><a id="nonnorm" name="nonnorm"></a><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#Non-NormalData">Example: Non-Normal Data</a></p>
<h4><a id="Transformations" name="Transformations"></a>Transformations</h4>
<p>Skewed variables may be made more normal by transformations such as taking the log. However, you should consider how this affects the relationships between variables. For example, if you have variables for "income" and "spending on entertainment" and you believe the relationship between the two is linear, replacing "income" with "log income" makes the imputation model for both variables misspecified.</p>
<p><a id="transform" name="transform"></a><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#Transformations">Example: Transformations</a></p>
<h4><a id="BoundedVariables" name="BoundedVariables"></a>Bounded Variables</h4>
<p>Another common situation is bounded variables. For example, "hours worked" cannot go below zero, and percentages must be between zero and 100. Such variables can be imputed using <span class="InputCode">truncreg</span>. The <span class="InputCode">ll()</span> and <span class="InputCode">ul()</span> options contain the lower limit and upper limit for the variable, which can be either numbers or variables. You are not required to specify both (e.g. hours worked probably only needs <span class="InputCode">ll(0)</span>, unless you're worried that the model might try to have someone work more than 168 hours per week). Unfortunately, in our experience it's not unusual for <span class="InputCode">truncreg</span> to have convergence problems  in imputation models with many variables.</p>
<p>PMM is a good alternative to <span class="InputCode">truncreg</span> because it naturally honors any bounds that exist in the observed data.</p>
<h4><a id="NonLinearTerms" name="NonLinearTerms"></a>Non-Linear Terms</h4>
<p>If your analysis model contains non-linear terms, most likely variables squared, then this must be taken into account when creating your imputation model. Suppose your analysis model regresses <span class="InputCode">y</span> on <span class="InputCode">x</span> and <span class="InputCode">x^2</span>. If you just impute <span class="InputCode">y</span> and <span class="InputCode">x</span>, creating <span class="InputCode">x^2</span> later (either with <span class="InputCode">mi passive</span> or <span class="InputCode">c.x#c.x</span>), then the imputed values of <span class="InputCode">y</span> will only depend on <span class="InputCode">x</span> and the imputed values of <span class="InputCode">x</span> will depend linearly on <span class="InputCode">y</span>. When you run your analysis model, the coefficient on the squared term will be biased towards zero because for observations where either <span class="InputCode">y</span> or <span class="InputCode">x</span> is imputed, <span class="InputCode">y</span> really is unrelated to <span class="InputCode">x^2</span>. (Never forget that when you write your <span class="InputCode">mi impute chained</span> command you are building models, not just listing variables to impute.)</p>
<p>The best alternative appears to be what White, Royston and Wood  call the "Just Another Variable" approach. Create new variables to store the non-linear terms (e.g. <span class="InputCode">gen x2=x^2</span>) and then impute them as if they were just another variable, unrelated to the linear terms. The imputed values of the non-linear terms won't have the proper relationship to the linear terms (i.e. the imputed values <span class="InputCode">x2</span> will not in fact be <span class="InputCode">x^2</span>) but as long as they are distributed properly this does not appear to affect the results of the analysis model. This is an area of ongoing research.</p>
<p><a id="nonlin" name="nonlin"></a><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#Non-Linearity">Example: Non-Linearity</a></p>
<h4><a id="InteractionTerms" name="InteractionTerms"></a>Interaction Terms</h4>
<p>Interaction terms raise issues very similar to those raised by non-linear terms: if the interaction term isn't included in the imputation model, the coefficient on the interaction term will be biased towards zero in the analysis model. The "Just Another Variable" approach also works well for interaction terms: create variables storing the interaction effects (e.g. <span class="InputCode">gen gx=g*x</span>) and then impute them separately.</p>
<p> If, however, the interactions  involve binary or categorical variables that represent groups, consider instead using the <span class="InputCode">by()</span> option to impute each group separately. This allows coefficients to vary between groups without the problem of imputed interaction terms not actually matching the variables being interacted.</p>
<p>For example, suppose you're regressing <span class="InputCode">income</span> on <span class="InputCode">education</span>, <span class="InputCode">experience</span>, and <span class="InputCode">black</span> (an indicator for "subject is black"), but think the returns to education vary by race and thus include <span class="InputCode">black##c.education</span> in the regression. The just another variable approach would create a variable <span class="InputCode">edblack=black*race</span> and impute it, but it's possible for the model to impute a zero for <span class="InputCode">black</span> and a non-zero value for <span class="InputCode">edblack</span>. There's no indication this would cause problems in the analysis model, however.</p>
<p>An alternative would be to add the <span class="InputCode">by(black)</span> option to the imputation command, so that whites and blacks are imputed separately. This would allow you to use <span class="InputCode">black##c.education</span> in your analysis model without bias (and it would always correspond to the actual values of <span class="InputCode">black</span> and <span class="InputCode">education</span>). However, running two separate imputation models allows the returns to experience to vary by race in the imputation model, not just education. If you had strong theoretical reasons to believe that was not the case (which is unlikely) that would be a specification problem. A far more more common problem is small sample size: make sure each of your <span class="InputCode">by()</span> groups is big enough for reasonable regressions.</p>
<p>Trying to use "Just Another Variable" for interactions between categorical variables and imputing them with <span class="InputCode">logit</span> is problematic. Use <span class="InputCode">by()</span> instead.</p>
<p><a id="interact" name="interact"></a><a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm#Interactions">Example: Interactions</a></p>
<h4><a id="SetsofIndicatorVariables" name="SetsofIndicatorVariables"></a>Sets of Indicator Variables</h4>
<p>If you have a set of mutually exclusive indicator variables, use them to create a single categorical variable and then impute it using <span class="InputCode">mlogit</span>. For example, combine <span class="InputCode">white</span>, <span class="InputCode">black</span>, <span class="InputCode">hispanic</span>, <span class="InputCode">other</span> into <span class="InputCode">race</span>, or <span class="InputCode">highSchool</span>, <span class="InputCode">someCollege</span>, <span class="InputCode">bachelors</span>, <span class="InputCode">advanced</span> into <span class="InputCode">education</span>. You can recreate the indicator variables after imputing, either with <span class="InputCode">mi passive</span> or by simply using <span class="InputCode">i.race</span> or <span class="InputCode">i.education</span> in your models.</p>
<p>If you impute the indicator variables themselves using <span class="InputCode">logit</span>, the imputation model will not impose the constraint that only one of them can be one. Thus you'll likely get people with more than one race or more than one education level. By converting the indicators to a categorical variable and imputing the categorical variable using <span class="InputCode">mlogit</span> you force the model to choose just one category.</p>
<p>Next: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_impute.htm">Imputing</a></p>
<p>Previous: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_decide.htm">Deciding to Impute</a></p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Multiple Imputation in Stata: Recommended Readings</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p><em>This article is part of the Multiple Imputation in Stata series. For a list of topics covered by this series, see the <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_intro.htm">Introduction</a>.</em></p>
<p>Multiple Imputation is an ongoing research area, so be sure to pay attention to when papers on the topic were published. You'll find that advice in some areas changes, and older discussions of software are often completely out-of-date. You'll also need to keep current on the topic. That said, here is what we'd suggest reading to learn more about multiple imputation:</p>
<p> </p>
<ul>
<li>White, Royston, and Wood. "<a href="http://onlinelibrary.wiley.com.ezproxy.library.wisc.edu/doi/10.1002/sim.4067/pdf">Multiple imputation using chained equations: Issues and guidance for practice.</a>" Statistics in Medicine. 2011</li>
<li> Stata MI Documentation.  Type "help mi" in Stata, click on the link at the top.</li>
<li> UCLA Statistical Computing Seminars. "<a href="http://www.ats.ucla.edu/stat/stata/seminars/missing_data/mi_in_stata_pt1.htm">Multiple Imputation in Stata.</a>"</li>
<li> Van Buuren,  "<a href="http://web.ebscohost.com.ezproxy.library.wisc.edu/ehost/pdfviewer/pdfviewer?sid=6ac98e9f-3480-4900-a2ad-4f61ce07ac28%40sessionmgr12&amp;vid=2&amp;hid=17">Multiple imputation of discrete and continuous data by fully conditional specification</a>." Statistical Methods in Medical Research, 2007 </li>
<li>Van Buuren, Brand, Groothuis-Oudshoorn, and Rubin.  "Fully conditional specification in multivariate imputation." Journal of Statistical Computation and Simulation. 2006</li>
<li> Paul Allison. "Missing Data." Sage University Papers Series: Quantitative Applications in the Social Sciences. 2001 Joseph Shafer.</li>
<li> White, Royston, and Wood. "<a href="http://onlinelibrary.wiley.com.ezproxy.library.wisc.edu/doi/10.1002/sim.4067/pdf">Multiple imputation using chained equations: Issues and guidance for practice.</a>" Statistics in Medicine. 2011 Yes, read it again.</li>
<li>Any of the  references in the Stata MI Documentation</li>
<li>Any useful article published since this list was made (<a href="mailto:helpdesk@ssc.wisc.edu">tell us about it</a>!)</li>
</ul>
<p>Note that some of these links will only work if you are connected to the UW-Madison network.</p>
<p>Previous: <a href="https://ssc.wisc.edu/sscc/pubs/stata_mi_ex.htm">Examples</a> </p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Running Stata MP at the SSCC</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p>The SSCC has two varieties of Stata. Standard Stata uses just one processor, but Stata/MP can take advantage of up to 32 processors for a huge increase in performance. Use it whenever you have a Stata job that's taking a long time due to computations, such as mixed models or multiple imputation (unfortunately it can't help with loading large data sets). No need to change your do files, just how you run them.</p>
<p>Stata/MP32 licenses are very expensive, so we have a limited number of them. If you are  told all the licenses are in use you'll need to wait until one becomes available.</p>
<h2>Winstat</h2>
<p>On Winstat, right-click on a do file and choose <span class="MenuOutput">Execute StataMP Batch Mode</span>. Stata will not start its usual graphical user interface: it will simply run your do file and then quit. A log file with the same name as your do file will be created in the same directory as your do file automatically, but any <span class="InputCode">log using</span> commands will be honored as well. Please note that in accordance with the SSCC’s <a href="https://ssc.wisc.edu/sscc/policies/server_usage.htm">Server Usage Policy</a>, Stata/MP on Winstat will actually use 8 cores.</p>
<p>Unfortunately this method will not work if the name of your do file or any of the folders it is located in contain spaces. Rename things like <span class="InputCode">U:\My Dissertation\My Do Files\My Do File.do</span> to <span class="InputCode">U:\MyDissertation\MyDoFiles\MyDoFile.do</span>.</p>
<h2>Linstat</h2>
<p>On Linstat, do files run using the <span class="InputCode">stata, xstata,</span> and <span class="InputCode">condor_stata</span> commands will be run using Stata/MP, though some HTCondor servers only have 8 or 16 cores and Stata/MP will adapt accordingly. If you use <span class="InputCode">xstata</span> to get a graphical user interface, be sure to close Stata as soon as your done actively using it so the license becomes available to others.</p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Stata Programming Techniques for Panel Data in Stata</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p>Panel data, where subjects are observed repeatedly over time, is a very common data structure in the social sciences. This article will teach you some  programming techniques used to prepare panel data for analysis. They include:</p>
<ul>
<li><a href="#ever">Did a subject ever experience this event?</a></li>
<li><a href="#how_many">How many times did the subject experience this event?</a></li>
<li><a href="#yet">Has a subject experienced this event yet?</a></li>
<li><a href="#how_many_thus_far">How many times has the subject experienced the event thus far?</a></li>
<li><a href="#sequence">Events in Sequence</a></li>
<li><a href="#time">Changing Time Periods</a></li>
</ul>
<h2>Background</h2>
<p>Panel data is a particular kind of hierarchical data, where the level 2 unit is a subject and the level 1 unit is a subject observed in a particular period. (If you're not familiar with this vocabulary for describing hierarchical data, <a href="https://ssc.wisc.edu/sscc/pubs/sfr-hier.htm">here's an introduction to it</a>.) Panel data normally includes both variables that change over time (level 1 variables) and variables that do not (level 2 or subject-level variables). It's very important that you know the type of each of your variables.</p>
<p>While this article will describe the problems to be solved in terms of subjects observed over time, some of these techniques apply to any hierarchical data structure. In fact the code for some of these examples is essentially identical to code used in the <a href="https://ssc.wisc.edu/sscc/pubs/sfr-groups.htm">Working with Groups</a> section of <a href="https://ssc.wisc.edu/sscc/pubs/sfr-intro.htm">Stata for Researchers</a> to identify characteristics of households composed of individuals.</p>
<p> This article will assume that the data set is in the long form, i.e. there is one observation per person per period. (The kind of work described here is usually much easier in long form.) It will also assume that the data set is sorted by subject and period, so the observations for each subject are in chronological order.</p>
<p>A note on variable names: with this sort of data you could easily have variables for "was incarcerated in this month," "was incarcerated at some point," or "had been incarcerated by this period."  Use clear variable names and/or labels to help you distinguish between them.</p>
<h2>Identifying an Event</h2>
<p>An event  could be anything from "was incarcerated" to "was diagnosed with diabetes" to "attended a four-year college." Before doing anything else, you need to identify an <em>if</em> condition which will be true if the event happened in a given time period and false otherwise. For example, if you have an <span class="InputCode">inc</span> variable which is 1 if the subject was incarcerated in a given month and 0 otherwise, the condition is simply <span class="InputCode">inc==1</span>. (If you have no missing data, just  <span class="InputCode">inc</span> will do because to Stata 1 is true and 0 is false—just keep in mind that any other value, including missing, is also treated as true.) Other examples might be <span class="InputCode">diag==146</span> (assuming 146 is the code for diabetes in your data set) or <span class="InputCode">enrolled &amp; type==4</span>.</p>
<h2 id="ever">Did a subject ever experience this event?</h2>
<p>Often you need to be able to identify those subjects for whom the event occurred at any point. At this level it does not matter when the event occurred or how many times. The result will be a subject-level binary variable which will be 1 in all time periods if subject ever experienced the event and 0 in all time periods otherwise.                </p>
<p>Use <span class="InputCode">max()</span> function in the <span class="InputCode">egen</span> library to identify whether the event condition identified above is ever true:</p>
<p class="InputCode">by subject: egen everInc=max(inc)</p>
<p class="InputCode"> by subject: egen everDiabetes=max(diag==146)</p>
<p class="InputCode"> by subject: egen ever4yr=max(enrolled &amp; type==4) </p>
<p>Recall that true is 1 and false is 0. Thus if the condition is true (1) for any observation  <span class="InputCode">max()</span> will return 1. If it always false (0), <span class="InputCode">max()</span> will return 0.</p>
<h2 id="how_many">How many times did the subject experience this event?</h2>
<p>If you need to know how many times the subject experienced the event during the study period, use the <span class="InputCode">total()</span> function:</p>
<p class="InputCode">by subject: egen timesInc=total(inc)</p>
<p class="InputCode">by subject: egen timesDiag=total(diag==146)</p>
<p class="InputCode"> by subject: egen semesters4yr=total(enrolled &amp; type==4)</p>
<h2 id="how_many_thus_far">How many times has the subject experienced the event thus far?</h2>
<p>If you need to know how many times the subject has experienced the event up to and including the current period, use the <span class="InputCode">sum()</span> function. It calculates a running sum over all the observations it has seen thus far (which is why it is a regular function and not part of the <span class="InputCode">egen</span> library).</p>
<p class="InputCode">by subject: gen timesIncThusFar=sum(inc)</p>
<p class="InputCode">by subject: gen timesDiagThusFar=sum(diag==146)</p>
<p class="InputCode"> by subject: gen semesters4yrThusFar=sum(enrolled &amp; type==4)</p>
<h2 id="yet">Has a subject experienced this event yet?</h2>
<p>Sometimes you need to identify which periods come before  the subject first experienced an event and which come after. The result will be a binary variable which changes over time, but only from 0 to 1 (i.e. it will be some number of zeros followed by some number of ones). The easy way to do this is to first create a variable counting how many times the subject has experienced the event thus far using <span class="InputCode">sum()</span> as described above. If that number is greater than zero, the subject has experienced the event.</p>
<p class="InputCode">gen hasBeenInc=(timesIncThusFar&gt;0)</p>
<p class="InputCode">gen hasDiabetes=(timesDiagThusFar&gt;0)</p>
<p class="InputCode">gen has4yr=(semesters4yrThusFar&gt;0)</p>
<h2 id="sequence">Events in Sequence</h2>
<p>Sometimes the order in which events happen matters. For example, you may be interested in events where a given drug is prescribed before the patient has received a diabetes diagnosis, or in people who attend a two-year college after attending a four-year college. We'll call the first event event A and the second event B.</p>
<p>The first step is to create a variable for "Has a subject experienced event A yet?" as described in the previous section. Then you can take whatever condition identifies event B, add the condition that the subject either has or has not experienced event A yet, and use any of the techniques described above. Two examples:</p>
<p class="InputCode">gen prescribeBefore=(drug==216 &amp; !hasDiabetes)</p>
<p class="InputCode">by subject: egen everPrescribedBefore=max(drug==216 &amp; !hasDiabetes)</p>
<p>After running this code, <span class="InputCode">prescribeBefore</span> is an indicator for "in this period the subject was prescribed drug 216 before having received a diagnosis of diabetes" and <span class="InputCode">everPrescribedBefore </span>is a subject-level indicator for "the subject was prescribed drug 216 before receiving a diagnosis of diabetes at some point."</p>
<p>If you were interested in "after" rather than "before" you'd simply change <span class="InputCode">&amp; !hasDiabetes</span> to <span class="InputCode">&amp; hasDiabetes</span>.</p>
<h2 id="time">Changing Time Periods</h2>
<p>Sometimes the "periods" in panel data represent different amounts of time for different subjects. For example, the time between survey interviews may vary, or semesters start and end at different times at different institutions. In that case it may be convenient to break up the original time periods into smaller standard periods,  such as months, especially if the data for each period can be assumed not to change during the period.</p>
<p>Load the following example data set:</p>
<p class="InputCode">use http://ssc.wisc.edu/sscc/pubs/files/semester_panel.dta</p>
<p>This contains (fictional) data about students in college. There is one row per student per semester, and the <span class="InputCode">level</span> variable tells us whether they attended a two-year or four-year institution in that semester. The variables <span class="InputCode">startYear</span>, <span class="InputCode">startMonth</span>, <span class="InputCode">endYear</span>, and <span class="InputCode">endMonth</span> tell us when each semester began and ended. Our goal is to restructure the data such that we have one observation per student per month.</p>
<p>Begin by creating a semester identifier, numbering them in chronological order:</p>
<p class="InputCode">sort id startYear startMonth<br/>
by id: gen semester=_n</p>
<p>Next convert the date variables into Stata's date format, using months as the base unit:</p>
<p class="InputCode">gen start=ym(startYear,startMonth)<br/>
                  gen end=ym(endYear,endMonth)<br/>
                format start end %tm</p>
<p>(If you're not familiar with Stata dates, you can learn about them <a href="https://ssc.wisc.edu/sscc/pubs/stata_dates.htm">here</a>.)</p>
<p>Since the dates are now stored as "number of months since January 1960" (with a format that makes them readable by humans when printed in output) the duration of a semester can be calculated by subtracting the start date from the end date and adding 1:</p>
<p class="InputCode">gen duration=end-start+1</p>
<p>Next, use the <span class="InputCode">expand</span> command to create one copy of each semester for each month the semester lasted:</p>
<p class="InputCode">expand duration</p>
<p>We now have one observation per student per month, but there's no indication which month a particular observation represents. Since they're all the same (the student is assumed to have attended the same institution for the duration of the semester) we will simply assign them in order: the first observation for a given semester will represent the first month of the semester, the second observation the second month, etc.</p>
<p>Sometimes relative times (first month, second month, etc.) are adequate. In that case you can simply set an observation's month to its observation number within the semester:</p>
<p class="InputCode">bysort id semester: gen month=_n</p>
<p>But in this case it's probably much more useful to have a time variable that tells us in absolute terms which month the observation represents. The month the semester started is stored in the <span class="InputCode">start</span> variable, so all you have to do is add the observation number minus one:</p>
<p class="InputCode">bysort id semester: gen time=start+_n-1<br/>
                  format time %tm<br/>
</p>
<p><span class="InputCode">time</span> is now the only time variable you need, so you can drop all others plus the variables used to get this far:</p>
<p class="InputCode">drop start* end* dur semester</p>
<p>You now have a data set with one observation per student per month <em>for the months the student was enrolled in school</em>. You probably want observations for all of the months in the study period where the student was not enrolled in school as well.</p>
<p>The <span class="InputCode">fillin</span> command ensures there's an observation for each unique combination of the variables it's given, creating new observations as needed. Newly created observations will have missing values for all the other variables and will have a <span class="InputCode">_fillin</span> variable set to 1, but we won't need that variable for this task. Running:</p>
<p class="InputCode">fillin id time<br/>
                drop _fillin                </p>
<p>will ensure there's an observation for each student for  each month in the data set. If at least one student is enrolled  in every month during the study period, this will create a full set of observations.</p>
<p>At this point months in which the student was not enrolled have a missing value for <span class="InputCode">level</span> (the level of the institution attended, two-year or four-year). You could change that to 0 if you prefer:</p>
<p class="InputCode">replace level=0 if level==.</p>
<p></p>
<p>However, if a month is entirely missing from the data set (because no one is enrolled during that month) <span class="InputCode">fillin</span> will not create observations for that month. In that case, if you want observations for every month in the study period you'll need to  add the missing months.</p>
<p>The following code calculates the length of the study period, creates one observation for every month in that period for a fake student, then combines that with the original data, runs <span class="InputCode">fillin</span>, and drops the fake student. The result is a data set with an observation for every month for all students.</p>
<p class="InputCode">// save what you've done thus far<br/>
                save sem_panel2, replace<br/>
<br/>
                // keep only the time variable<br/>
keep time<br/>
<br/>
// calculate the duration of the study period<br/>
// sort by time, then subtract first time from last time<br/>
sort time<br/>
gen dur=time[_N]-time[1]<br/>
<br/>
// cut to one observation<br/>
keep if _n==1<br/>
<br/>
// expand it to one observation per month in the study period
<br/>
expand dur<br/>
<br/>
// set time for each observation<br/>
replace time=time+_n-1<br/>
<br/>
// create id for fake student <br/>
gen id=-9<br/>
<br/>
// add back to full data<br/>
append using sem_panel2<br/>
<br/>
// create observations using fillin<br/>
fillin id time<br/>
drop _fillin<br/>
<br/>
// drop the fake student<br/>
drop if id==-9</p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Stata Programming Essentials</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p>Ever needed to do the same thing to ten different variables and wished that you didn't have to write it out ten times? If so, then this article is for you. If not, someday you will—so you might as well keep reading anyway.</p>
<p>Stata has all the tools required to write very sophisticated programs, but knowing just a few of them allows you to make everyday do files shorter and more efficient. This article will focus on those programming tools that, in our experience, anyone who uses Stata heavily will eventually want to learn. To benefit from this article you'll need a solid understanding of basic Stata syntax, such as you can get from our <a href="https://ssc.wisc.edu/sscc/pubs/sfr-intro.htm">Stata for Researchers</a> series. The primary intended audience is Stata users with no other programming experience. If you've done a lot of Stata programming already and are looking to expand your "bag of tricks" check out <a href="https://ssc.wisc.edu/sscc/pubs/stata_prog2.htm">Stata Programming Tools</a>.                </p>
<p>This article is best read at the computer with Stata running. Typing the commands in the examples yourself will help you notice and retain all the details, and prepare you to write your own code.</p>
<h2><a id="Macros" name="Macros"></a>Macros</h2>
<p>A Stata macro is a box you  put  text in. You then use what's in the box in subsequent commands. (The real trick is getting a single command to run multiple times with a different bit of text in the box each time--we'll get there).</p>
<p>The macros we'll use are "local" macros. If you're familiar with global and local variables from other languages, Stata's local macros are local in the same way. If not, just trust us that local macros are the right ones to use.</p>
<p>The command to define a local macro is:</p>
<p class="InputCode">local <span class="Parameter">name</span> <span class="Parameter">value</span></p>
<p>For example:</p>
<p class="InputCode">local x 1</p>
<p>This creates a local macro called <span class="InputCode">x</span> and puts the character '<span class="InputCode">1</span>' in it (not the value 1 as in "one unit to the right of zero on the number line"). To use a macro, you put its name in a command, surrounded by a particular set of quotation marks:</p>
<p class="InputCode">display `x'</p>
<p>The quote before the <span class="InputCode">x</span> is the left single quote. It is found in the upper left corner of the keyboard, under the tilde (<span class="InputCode">~</span>). The quote after the <span class="InputCode">x</span> is the right single quote. It is found under the double quotation mark (<span class="InputCode">"</span>) on the right side of the keyboard.</p>
<p>Macros are handled by a  macro processor that examines commands before passing them to Stata proper. When it sees a macro (denoted by that particular set of quotation marks) it replaces the  macro with its table. Thus what Stata proper saw was:</p>
<p class="InputCode">display 1</p>
<p>Now try a slightly more complicated macro:</p>
<p class="InputCode">local x 2+2<br/>
                  display `x'
                </p>
<p>The result is <span class="MenuOutput">4</span>, but that's because the display command acts like a calculator. The command Stata saw was:</p>
<p class="InputCode">display 2+2</p>
<p>so it evaluated <span class="InputCode">2+2</span> and gave you the answer. If you want <span class="InputCode">display</span> to put something on the screen without evaluating it,  put it in quotes. Then <span class="InputCode">display</span> will treat it like a string.</p>
<p class="InputCode">display "`x'"</p>
<p>gives the result <span class="MenuOutput">2+2</span>. But consider what happened before you put it in quotes: your macro contained a working bit of Stata code which Stata happily executed when you used it. In fact Stata proper didn't know or care that <span class="InputCode">2+2</span> came from a macro. This feature allows you to use macros absolutely anywhere, even in macro definitions.</p>
<h3><a id="StoringResultsinMacros" name="StoringResultsinMacros"></a>Storing Results in Macros</h3>
<p>If you want to put the result of a calculation in a macro, put an equals sign after the macro name:</p>
<p class="InputCode">local x=2+2<br/>
                display "`x'"                </p>
<p>If the <span class="InputCode">local</span> command contains an equals sign, Stata will evaluate what follows before putting it in the macro. Now <span class="InputCode">x</span> really does contain <span class="InputCode">4</span> and not <span class="InputCode">2+2</span> no matter how you display it.</p>
<h3><a id="MacroExpressions" name="MacroExpressions"></a>Macro Expressions</h3>
<p>Stata's macro processor can evaluate Stata expressions; i.e. any formula you could put after the equals sign in a <span class="InputCode">generate</span> or <span class="InputCode">replace</span> command (but not <span class="InputCode">egen</span>). The syntax is:</p>
<p class="InputCode">`=<span class="Parameter">expression</span>'</p>
<p>where <span class="Parameter">expression</span> is the expression to be evaluated. Try:</p>
<p class="InputCode">display "`=2+2'"</p>
<p>The result is <span class="MenuOutput">4</span>, but <span class="InputCode">display</span> didn't calculate it (the quotes prevent that). Instead, the equals sign before <span class="InputCode">2+2</span> told the macro processor to evaluate that expression and put the result in the code, so what Stata proper saw was <span class="InputCode">display "4"</span>. Another common use is <span class="InputCode">`=_N'</span>, which will be the number of observations in the current data set (and can be used in places where <span class="InputCode">_N</span> by itself can't).</p>
<p> Macro expressions--and macros in general--can contain other macros. Try:</p>
<p class="InputCode">display "`=`x'-1'"</p>
<p>This tells the macro processor to subtract one from the value of the macro x and then place the result in the code. This can be extremely useful: for example, if you had a macro <span class="InputCode">`year'</span> containing the current year, <span class="InputCode">`=`year'-1'</span> would be the year before the current year.</p>
<h3><a id="UndefinedMacros" name="UndefinedMacros"></a>Undefined Macros</h3>
<p>Unfortunately, using a macro you haven't defined doesn't generate an error message. Stata's macro processor just replaces it with nothing:</p>
<p class="InputCode">display `y'</p>
<p>Gives the same result as:</p>
<p class="InputCode">display </p>
<p>This can cause headaches: if you mistype a macro's name you'll probably get a generic syntax error with no indication that a macro is the cause of the problem. Even worse, in some circumstances the command will still work but give incorrect results. Be very careful to type the names of macros properly.</p>
<h3><a id="SomeUsesforMacrosOutsideofLoops" name="SomeUsesforMacrosOutsideofLoops"></a>Some Uses for Macros Outside of Loops</h3>
<p>The main reason for learning about macros is so you can use them in loops. But there are times when using them all by themselves can make complex code easier to read.</p>
<p>Suppose you need to run a large number of regressions of various types, but they all include a fixed set of control variables. Consider putting the list of control variables in a macro:</p>
<p class="InputCode">local controlVars age sex occupation location maritalStatus hasChildren</p>
<p>This will make the regression commands shorter:</p>
<p class="InputCode">reg income education `controlVars'<br/>
                  logit 
                employed education `controlVars'</p>
<p>Now suppose you frequently work with  subsamples of your data set. You can define macros for them as well:</p>
<p class="InputCode">local blackWoman race==1 &amp; female<br/>
                local hispMan race==2 &amp; !female<br/>
                reg income education `controlVars' if `blackWoman'<br/>
				logit employed education `controlVars' if `hispMan'</p>
<p>The point here is not to save keystrokes, but to make the code more clear. Using macros hides the details of what the control variables are or how a black woman can be identified in this data set and helps you focus on what you're trying to do. Not having to type out those details every time also removes an opportunity for error. You can  make changes more quickly too: if you need to add a control variable you only have to add it to the definition of the <span class="InputCode">controlVars</span> macro rather than adding it to each regression command.</p>
<p> Saving keystrokes is a nice side effect, but resist the temptation to make your code less clear in the name of making it shorter. Taking a few minutes to type out clear code is far more efficient than spending hours debugging code that's short but hard to understand.</p>
<h2><a id="ForLoops" name="ForLoops"></a>For Loops</h2>
<p>A <span class="InputCode">foreach</span> loop takes a list and then executes a command or set of commands for each element of the list. The element currently being worked on is stored in a macro so you can refer to it in the commands. The list to be looped over can be a generic list  containing  text, or there are several kinds of  structured lists (we'll only discuss <em>varlists</em>).</p>
<p>The syntax for a <span class="InputCode">foreach</span> loop with a generic list is:</p>
<p class="InputCode">foreach <span class="Parameter">macro</span> in <span class="Parameter">list</span> {<br/>
<span class="indent3"><span class="Parameter">command(s)</span></span><br/>
                  }</p>
<p>As a very simple example:</p>
<p class="InputCode">foreach color in red blue green {<br/>
<span class="indent3">display "`color'"</span><br/>}</p>
<p>Here, <span class="InputCode">color</span> is the name of the macro that will contain the list elements. <span class="InputCode">red blue green</span> is the list itself. Stata breaks the list into elements wherever it sees spaces, so this list contains three elements: <span class="InputCode">red</span>, <span class="InputCode">blue,</span> and <span class="InputCode">green</span>. The left curly bracket (<span class="InputCode">{</span>) marks the beginning of the loop and must be at the end of the <span class="InputCode">foreach</span> command. The right curly bracket (<span class="InputCode">}</span>) marks the end of the loop and must go on its own line. If you type this in interactive Stata the Results window adds line numbers for the commands inside the loop, but you do not need to type them. Note how nothing is actually executed until you type the right curly bracket, and then Stata runs the whole thing. When it does you'll get the following output:</p>
<p class="MenuOutput">red<br/>
                  blue<br/>
                green</p>
<p></p>
<p>Stata begins by analyzing your list and identifying the elements it contains. It then puts the first element (<span class="InputCode">red</span>) in the loop's macro (<span class="InputCode">color</span>) and executes the command in the loop. Given the tables of <span class="InputCode">color</span>, the command becomes <span class="InputCode">display "red"</span> and <span class="MenuOutput">red</span> is printed on the screen. Stata then puts the second element in the macro and runs the command again, printing <span class="MenuOutput">blue</span> on the screen. It then repeats the process for <span class="MenuOutput">green</span>, and when that's done Stata realizes the list is out of elements and the <span class="InputCode">foreach</span> loop is  complete.</p>
<p>Throughout this article you'll see that commands which are inside a loop are indented. This makes the loop's structure visually obvious and we highly recommend you do the same when writing do files. All you need to do is press <span class="InputCode">Tab</span> before you begin the first line of the loop. Stata's do file editor and any other text editor suitable for programming will indent subsequent lines automatically. (There's no need to worry about indenting when working interactively, but in real work it's very rare to use loops interactively.)</p>
<p>You can use a generic list to loop over many different kinds of things: variables, values, files, subsamples, subscripts, anything you can describe using text. If an element needs to contain spaces, put it in quotes.</p>
<h3><a id="LoopingoverVariables" name="LoopingoverVariables"></a>Looping over Variables</h3>
<p>The most common thing to loop over is variables. For example, suppose you wanted to regress several different dependant variables on the same independent variables. The following code does so, using the automobile example data set that comes with Stata:</p>
<p class="InputCode">sysuse auto<br/>
                foreach yvar in mpg price displacement {<br/>
<span class="indent3">reg `yvar' foreign weight</span><br/>
                }
                </p>
<h3><a id="LoopingoverPartsofVariableNames" name="LoopingoverPartsofVariableNames"></a>Looping over Parts of Variable Names</h3>
<p>Consider the following data set:</p>
<p class="InputCode">use http://www.ssc.wisc.edu/sscc/pubs/files/stata_prog/months.dta</p>
<p>It contains a fictitious (and not terribly plausible) data set of people and their incomes over twelve months. This is panel data in the wide form, so there are twelve income variables: <span class="InputCode">incJan</span>, <span class="InputCode">incFeb</span>, <span class="InputCode">incMar</span>, etc. Suppose you want to create a corresponding set of indicator variables for whether the person had any income in that month. Creating one of them is straightforward:</p>
<p class="InputCode">gen hadIncJan=(incJan&gt;0) if incJan&lt;.</p>
<p>but creating all twelve in the same way would be tedious.</p>
<p> (If you checked, you'd find that this data set does not have any missing values so excluding them with <span class="InputCode">if incJan&lt;.</span> is not strictly necessary. Consider it a reminder to always think about missing values when creating such indicator variables.)</p>
<p>You can create all twelve indicator variables quickly and easily with a <span class="InputCode">foreach</span> loop:</p>
<p class="InputCode">foreach month in Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec {<br/>
<span class="indent3">gen hadInc`month'=(inc`month'&gt;0) if inc`month'&lt;.</span><br/>
}</p>
<p>This sets up a generic list containing the months, and then uses those months as parts of variable names.</p>
<p>Note the process we used to create this loop: first we figured out the command we'd use for a single element of the list and then changed it to use macros. This is a good habit whenever you need to write non-trivial code involving macros.</p>
<h3><a id="LoopingoverVarlists" name="LoopingoverVarlists"></a>Looping over Varlists</h3>
<p>While generic lists can contain variable names, you have to type out all the names individually. If you tell Stata that the list you want to loop over is an official Stata <em>varlist</em> you can use standard <em>varlist</em> shortcuts, like <span class="InputCode">x*</span> for all variables that begin with <span class="InputCode">x</span> and <span class="InputCode">x-z</span> for all the variables from <span class="InputCode">x</span> to <span class="InputCode">z</span>. To review <em>varlist</em> syntax, see the appropriate section in  <a href="https://ssc.wisc.edu/sscc/pubs/sfr-syntax.htm#Varlists">Stata for Researchers</a>.</p>
<p>The syntax for a <span class="InputCode">foreach</span> loop over a <em>varlist</em> is as follows:</p>
<p class="InputCode">foreach <span class="Parameter">macro</span> of varlist <span class="Parameter">vars</span> {</p>
<p>Note that while the <span class="InputCode">foreach</span> syntax for a generic list contains <span class="InputCode">in</span>, the syntax for a structured list has <span class="InputCode">of</span>. Stata uses the <span class="InputCode">in</span> or <span class="InputCode">of</span> to determine whether the next word is the first element of the list or a type of list.</p>
<p>Researchers occasionally receive data sets created in other programs where the variable names are  in upper case letters. Since Stata actually cares about case, upper case variable names can be tiresome to work with. Stata recently gave the <span class="InputCode">rename</span> command the ability to convert names to lower case:</p>
<p class="InputCode">rename *, lower</p>
<p>But this such a great example that let's do it with a <span class="InputCode">foreach</span> loop over a <em>varlist</em> anyway:</p>
<p class="InputCode">foreach oldname of varlist * {<br/>
<span class="indent3">local newname=lower("`oldname'")</span><br/>
<span class="indent3">rename `oldname' `newname'</span><br/>
                }</p>
<p>The asterisk (<span class="InputCode">*</span>) all by itself matches all variables, so  the list <span class="InputCode">foreach</span> is to loop over contains all the variables in the current data set. The <span class="InputCode">lower()</span> function takes a string, in this case the tables of the macro <span class="InputCode">oldname</span>, and converts it to lower case. Note the use of the equals sign in the <span class="InputCode">local</span> command that defines <span class="InputCode">newname</span>, so that <span class="InputCode">lower("`oldname'")</span> is evaluated and the result is stored.</p>
<h3><a id="LoopingoverNumbers" name="LoopingoverNumbers"></a>Looping over Numbers</h3>
<p>A <span class="InputCode">forvalues</span> loop (frequently abbreviated <span class="InputCode">forval</span>) loops over numbers. Rather than defining a list, you define a range of numbers.</p>
<p>By far the most common range consists of a starting number and an ending number, and Stata assumes it should count by ones between them. The syntax is simply:</p>
<p class="InputCode">forvalues <span class="Parameter">macro</span>=<span class="Parameter">start</span>/<span class="Parameter">end</span> {</p>
<p>For example:</p>
<p class="InputCode">forvalues i=1/5 {<br/>
<span class="indent3">display `i'</span><br/>
                  }
                </p>
<p>gives the output:</p>
<p class="MenuOutput">1<br/>
                  2<br/>
                  3<br/>
                  4<br/>
                  5                </p>
<p>If you need to count in a different way, type <span class="InputCode">help forvalues</span> to see more options.</p>
<p>Consider the following data set:</p>
<p class="InputCode">use http://www.ssc.wisc.edu/sscc/pubs/files/stata_prog/years.dta</p>
<p>This data set is very similar to the data set of monthly incomes we examined earlier, but it contains yearly incomes from 1990 to 2010. Your task is again to create an indicator for whether a person had any income in a given year. Using <span class="InputCode">forvalues</span> this is very easy to do:</p>
<p class="InputCode">forvalues year=1990/2010 {<br/>
<span class="indent3">gen hadInc`year'=(inc`year'&gt;0) if inc`year'&lt;.</span><br/>
                }</p>
<p>This would be more difficult if the years did not include the century (i.e. <span class="InputCode">90</span> instead of <span class="InputCode">1990</span>) because Stata thinks <span class="InputCode">100</span> should come after <span class="InputCode">99</span> and not <span class="InputCode">00</span>. If your data include such years, consider adding the century before doing any serious work with it.</p>
<h3><a id="LoopingoverValues" name="LoopingoverValues"></a>Looping over Values and levelsof</h3>
<p>Sometimes you need to loop over the values a particular variable takes on. Consider the following data set:</p>
<p class="InputCode">use http://www.ssc.wisc.edu/sscc/pubs/files/stata_prog/vals.dta</p>
<p>This contains  data on the race, income, age and education category of  a set of fictional people. Suppose you want to regress <span class="InputCode">income</span> on <span class="InputCode">age</span> and <span class="InputCode">education</span>, but believe that the effects of age and education may be different for people of different races. One approach (probably not the best one) would be to run a separate regression for the people of each race. Normally you could do that with:</p>
<p class="InputCode">by race: regress income age i.education</p>
<p>(The construction <span class="InputCode">i.education</span> tells Stata that <span class="InputCode">education</span> is a factor or categorical variable and should be converted into a set of indicators. See the <a href="https://www.ssc.wisc.edu/sscc/pubs/sfr-syntax.htm#FactorVariables">section on factor variables in Stata for Researchers</a> if you'd like to review factor variable syntax.)</p>
<p>However, this is fictional survey data and you need to correct for the survey design in running regressions. If you're not familiar with Stata's survey commands, that means the following:</p>
<ol>
<li>The survey design is described using the <span class="InputCode">svyset</span> (survey set) command. This data set has primary sampling units given by the variable <span class="InputCode">psu</span> and probability weights given by the variable <span class="InputCode">weight</span>. The corresponding command <span class="InputCode">svyset</span> command (which has already been run so you don't need to) is:<br/>
<span class="InputCode">svyset psu [pweight=weight]</span></li>
<li>To have Stata correct for those weights in estimation commands, add the <span class="InputCode">svy:</span> prefix, for example:<br/>
<span class="InputCode">svy: regress income age i.education</span></li>
<li>You can't use the standard <em>if</em> syntax with survey data or the weights may not be applied correctly. Instead, use the <span class="InputCode">subpop()</span> option of <span class="InputCode">svy:</span>, for example:<br/>
<span class="InputCode">svy, subpop(if race==1): regress income age i.education</span></li>
<li><span class="InputCode">by:</span> can't be used with <span class="InputCode">svy:</span></li>
</ol>
<p>Point #4 means you can't run your regression for all races using <span class="InputCode">by:</span>, but you can do it with a loop. All <span class="InputCode">by:</span> does is identify the values of <span class="InputCode">race</span> and then loop over them, and at this point you know how to do that yourself (though <span class="InputCode">by:</span> is faster when you can use it). The <span class="InputCode">race</span> variable takes on the values one, two and three, so an appropriate loop is:</p>
<p class="InputCode">forvalues race=1/3 {<br/>
<span class="indent3">svy, subpop(if race==`race'): reg income age i.education</span><br/>
                  }</p>
<p>What if you had a fourth race, and its number were nine ("Other") rather than four? You could simply recode it and make it four. But if that's not a good idea for your project, you'll have to switch to the less structured <span class="InputCode">foreach</span> loop:</p>
<p class="InputCode">foreach race in 1 2 3 9 {<br/>
<span class="indent3">svy, subpop(if race==`race'): reg income age i.education</span><br/>
                  }</p>
<p>On the other hand, it's not unusual to have to loop over dozens or even hundreds of values, or not to know ahead of time what values a variable takes on. In that case you can let the <span class="InputCode">levelsof</span> command identify them for you and put them in a macro. The syntax is:</p>
<p class="InputCode">levelsof <span class="Parameter">variable</span>, local(<span class="Parameter">macro</span>)</p>
<p>For example,</p>
<p class="InputCode">levelsof race, local(races)</p>
<p>will list all the values of the variable <span class="InputCode">race</span> and  store them in a macro called <span class="InputCode">races</span>. You can then loop over all of them with:</p>
<p class="InputCode">foreach race in `races' {<br/>
<span class="indent3">svy, subpop(if race==`race'): reg income age i.education</span><br/>
                  }</p>
<p>However, this situation is common enough that Stata wrote special code for parsing macros into lists for looping. The syntax is:</p>
<p class="InputCode">foreach race of local races {<br/>
<span class="indent3">svy, subpop(if race==`race'): reg income age i.education</span><br/>
                  }</p>
<p>Note that <span class="InputCode">races</span> is not in the usual macro quotes: the whole point of this construction is to bypass the regular macro processor in favor of code that's faster in the context of loops. It makes a very small difference, but if you do enough looping it will add up.</p>
<p>One feature you'll miss from <span class="InputCode">by:</span> is the text in the output telling you which by group is currently being worked on, but you can add it yourself. The following version of the loop adds a <span class="InputCode">display</span> command that inserts two blank lines and then prints the current value of the <span class="InputCode">race</span> macro before running the regression:</p>
<p class="InputCode">foreach race of local races {<br/>
<span class="indent3">display _newline(2) "Race=`race'"</span><br/>
<span class="indent3">svy, subpop(if race==`race'): reg income age i.education</span><br/>
                  }</p>
<p>Using <span class="InputCode">display</span> to print out the value of a macro at a given point in your program is also a very useful tool for debugging.</p>
<p>Keep in mind that this was just an example. A better way to examine the effect of <span class="InputCode">race</span> would probably be to interact race with  the other variables. The <a href="https://www.ssc.wisc.edu/sscc/pubs/sfr-syntax.htm#FactorVariables">new syntax for factor variables and interactions</a> makes this very easy:</p>
<p class="InputCode">svy: regress income i.race##(c.age i.education)</p>
<p>This model contains all the previous models--if you're new to regressions that include interactions, figuring out why that is might be a good exercise.</p>
<h2><a id="NestedLoops" name="NestedLoops"></a>Nested Loops</h2>
<p>The commands contained in a loop can include other loops:</p>
<p class="InputCode">forval i=1/3 {<br/>
<span class="indent3">forval j=1/3 {</span><br/>
<span class="indent3"><span class="indent3">display "`i',`j'"</span></span><br/>
<span class="indent3">}</span><br/>
                }</p>
<p>This code creates the following output:</p>
<p><span class="MenuOutput">1,1<br/>
                1,2<br/>
                1,3<br/>
                2,1<br/>
                2,2<br/>
                2,3<br/>
                3,1<br/>
                3,2<br/>
                3,3</span><br/>
</p>
<p>The inner loop (the one that uses <span class="InputCode">j</span>) is executed three times, once for each value of <span class="InputCode">i</span>. Thus the <span class="InputCode">display</span> command runs a total of nine times. Note how the <span class="InputCode">display</span> command is indented twice: once because it is part of the <span class="InputCode">i</span> loop and once because it is part of the <span class="InputCode">j</span> loop. When you start working with nested loops it's even more important that you can easily tell what each loop contains.</p>
<p>Consider one final data set:</p>
<p class="InputCode">use http://www.ssc.wisc.edu/sscc/pubs/files/stata_prog/monthyear.dta</p>
<p>This contains monthly income data, but for the period 1990-2010. The variable names are in the form <span class="InputCode">incJan1990</span>, <span class="InputCode">incFeb1990</span>, etc. To generate a set of corresponding indicators you need to loop over both the months and the years:</p>
<p class="InputCode">forval year=1990/2010 {<br/>
<span class="indent3">foreach month in Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec {</span><br/>
<span class="indent3"><span class="indent3">gen hadInc`month'`year'=(inc`month'`year'&gt;0) if inc`month'`year'&lt;.</span></span><br/>
<span class="indent3">}</span><br/>
}</p>
<p>This is certainly workable, but somewhat cumbersome. It would be especially awkward if you were interested in lags, leads, or changes over time: you'd need code to tell Stata that the month before January 1991 is December 1990. For most purposes it's easier if time periods are simply numbered sequentially. In this case January 1990 would be period 1, December 1990 would be period 12 and January 1991 period 13. Fortunately it's fairly easy to switch:</p>
<p class="InputCode">local period 1<br/>
                  forval year=1990/2010 {<br/>
<span class="indent3">foreach month in Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec {</span><br/>
<span class="indent3"><span class="indent3">rename inc`month'`year' inc`period'</span></span><br/>
<span class="indent3"><span class="indent3">rename hadInc`month'`year' hadInc`period'</span></span><br/>
<span class="indent3"><span class="indent3">local period=`period'+1</span></span><br/>
<span class="indent3">}</span><br/>
}</p>
<p>The macro <span class="InputCode">period</span> is used as a counter. It starts out set to <span class="InputCode">1</span>, and thus as the nested loops begin <span class="InputCode">incJan1990</span> is renamed <span class="InputCode">inc1</span> (and similarly <span class="InputCode">hadIncJan1990</span> to <span class="InputCode">hadInc1</span>). The command <span class="InputCode">local period=`period'+1</span> increases <span class="InputCode">period</span> by one: once the macro processor is done with it Stata proper sees <span class="InputCode">local period=1+1</span>. That completes the inner loop, so <span class="InputCode">month</span> is changed to <span class="InputCode">Feb</span>, and <span class="InputCode">incFeb1990</span> is renamed to <span class="InputCode">inc2</span>. The <span class="InputCode">period</span> macro is increased again (Stata proper now sees <span class="InputCode">local period=2+1</span>), <span class="InputCode">month</span> is set to <span class="InputCode">Mar</span>, <span class="InputCode">incMar1990</span> is renamed to <span class="InputCode">inc3</span>, and so forth until all 252 months are converted. (Note that 1990 to 2010 inclusive is 21 years.)</p>
<p>In making this conversion you lose the ability to look at a variable and know immediately what calendar month it describes. But it's much easier to loop over. The nested loops can be replaced with:</p>
<p class="InputCode">forvalues period=1/252 {</p>
<h2><a id="TheImportanceofNamingConventions" name="TheImportanceofNamingConventions"></a>The Importance of Naming Conventions</h2>
<p>The variable name <span class="InputCode">incJan1990</span> contains three components: the thing being observed (income) and  the month and year in which it is observed. The loops we wrote depend on the variable names describing all three in a consistent way: they would fail if the data set contained <span class="InputCode">incJan1990</span> along with <span class="InputCode">incomeJan1991</span>, <span class="InputCode">incjan1992</span>, <span class="InputCode">incJanuary1993</span> or <span class="InputCode">incJan94</span>. In the real world such things are not unusual. Data sets from surveys are a particular challenge because their variable names often come from the form of the questionnaire rather than the information they contain. Taking the time to rename your variables in a way that makes sense to you is a good idea at the beginning of any project, but if you'll be using loops it's vital that you create and apply a consistent naming convention for variables.</p>
<h2>Take Advantage of Stata's Automatic Loops</h2>
<p>Now that you've learned how to use loops, it can be tempting to use them for everything. Keep in mind that most Stata commands are already loops (do something to observation one, then do it to observation two, etc.) and those loops are much faster than any <span class="InputCode">foreach</span> or <span class="InputCode">forvalues</span> loop. For example, the following uses <span class="InputCode">forvalues</span> to loop over all the observations in the data set and set the value of <span class="InputCode">y</span> for each observation to the value of <span class="InputCode">x</span> for that observation:</p>
<p class="InputCode">                gen y=.<br/>
                forvalues i=1/`=_N' {<br/>
<span class="indent3">replace y=x[`i'] if _n==`i'</span><br/>
                }</p>
<p>but you'll get the exact same result far more quickly and easily with:</p>
<p class="InputCode">gen y=x</p>
<p>Occasionally someone finds a task that really does requires explicit looping over observations, but it's rare.</p>
<p>Clever programming can sometimes turn other loops into the standard loop over observations, making <span class="InputCode">foreach</span> or <span class="InputCode">forvalues</span> unnecessary. For example, reshaping wide form panel data into long form will eliminate the need for many loops.</p>
<p>Go back to the original 12 months of income data:</p>
<p class="InputCode">use http://www.ssc.wisc.edu/sscc/pubs/files/stata_prog/months.dta</p>
<p>Recall that we created <span class="InputCode">hadInc</span> indicator variables with the following loop:</p>
<p class="InputCode">foreach month in Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec {<br/>
<span class="indent3">gen hadInc`month'=(inc`month'&gt;0) if inc`month'&lt;.</span><br/>
}</p>
<p>However, you'll get the  same results with the following:</p>
<p class="InputCode">reshape long inc, i(id) j(month) string<br/>
                  gen hadInc=(inc&gt;0) if inc&lt;.<br/>
                reshape wide inc hadInc, i(id) j(month) string</p>
<p>(Take a moment to examine the data after each step.)</p>
<p>Reshaping a large data set is time consuming, so  don't switch between wide form and long form lightly. But if you can identify a block of things you need to do that would be easier to do in long form, it may be worth reshaping at the beginning and end of that block.                </p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Stata Programming Tools</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p>This article will introduce you to many Stata programming tools that are not  needed by everyone but are very useful in certain circumstances. The intended audience is Stata veterans who are already familiar with and comfortable using  Stata syntax and fundamental programming tools like macros, <span class="InputCode">foreach</span> and <span class="InputCode">forvalues</span>.</p>
<p>If you are new to Stata, our <a href="https://ssc.wisc.edu/sscc/pubs/sfr-intro.htm">Stata for Researchers</a> will teach you basic Stata syntax, and <a href="https://ssc.wisc.edu/sscc/pubs/stata_prog1.htm">Stata Programming Essentials</a> will teach you the fundamental programming tools. Unlike this article, they do not assume any Stata or general programming experience.</p>
<p>Topics discussed in this article include:</p>
<ul>
<li><a href="#CompoundDoubleQuotes">Compound Double Quotes</a></li>
<li><a href="#Capture">Capture</a></li>
<li><a href="#VariablesinScalarContexts">Variables in Scalar Contexts</a></li>
<li><a href="#AdvancedMacros">Advanced Macros</a></li>
<li><a href="#BranchingIf">Branching If</a></li>
<li><a href="#WhileLoops">While Loops</a></li>
<li><a href="#Programs">Programs</a></li>
</ul>
<p>If you need to learn about a specific topic, feel free to skip to it. Some of the examples use material  covered previously (i.e. a program containing a branching <em>if</em>) but the explanations are self-contained.</p>
<h2><a id="CompoundDoubleQuotes" name="CompoundDoubleQuotes"></a>Compound Double Quotes</h2>
<p>The beginning and ending of a string are normally denoted by double quotes ("string"). This causes problems when the string itself contains double quotes. For example, if you wanted to display the string <span class="InputCode">Hamlet said "To be, or not to be."</span> you could not use the code:</p>
<p class="InputCode">display "Hamlet said "To be, or not to be.""</p>
<p> The solution is what Stata calls compound double quotes. When Stata sees <span class="InputCode">`"</span> (left single quote followed by a double quote) it treats what follows as a string until it sees <span class="InputCode">"'</span> (double quote followed by a right single quote). Thus:</p>
<p class="InputCode">display `"Hamlet said "To be, or not to be.""'</p>
<h2><a id="Capture" name="Capture"></a>Capture</h2>
<p> On occasion you may expect a command to generate an error under certain circumstances  but choose to ignore it. For example, putting <span class="InputCode">log close</span> at the beginning of a do file prevents a previously opened log from interfering with your do file, but generates an error if no log  is open--even though not having a log open is exactly the situation you want to create.</p>
<p> The <span class="InputCode">capture</span> prefix prevents Stata from halting your do file if the ensuing command generates an error. The error is "captured" first. Thus:</p>
<p class="InputCode">capture log close</p>
<p>will close any open log but not crash a do file if no log file is open.</p>
<p>The <span class="InputCode">capture</span> prefix should only be used when you fully understand the error the command sometimes generates and why it does so--and you are very confident that that error can be ignored.</p>
<p>When a command is complete it stores a "return code" in <span class="InputCode">_rc</span>. A return code of zero generally means the command executed successfully. You can use <span class="InputCode">capture</span> followed by a <a href="#BranchingIf">branching if</a> based on the return code to have Stata do different things depending on whether an error occurred or not.</p>
<h2><a id="VariablesinScalarContexts" name="VariablesinScalarContexts"></a>Variables in Scalar Contexts</h2>
<p>A Stata variable is a vector: it has many values. If you type <span class="InputCode">list x</span> you get a list of all the values of <span class="InputCode">x</span>. However, some contexts call for a scalar. For example, the <span class="InputCode">display</span> command displays just one thing. If a variable is used in a scalar context,   the  value of the variable for the first observation is used. Thus if you type:</p>
<p class="InputCode">display x</p>
<p>you'll get just the first value of x, as if you'd typed:</p>
<p class="InputCode">display x[1]</p>
<p>We suggest not taking advantage of this behavior, because it makes for confusing code. If a command calls for a scalar and you want that scalar to be the first value of <span class="InputCode">x</span>, type <span class="InputCode">x[1]</span> rather than just <span class="InputCode">x</span>.</p>
<p>This behavior can cause real problems if you don't realize a particular context calls for a scalar, as you'll see in the section on <a href="#BranchingIf">branching if</a>. </p>
<h2><a id="AdvancedMacros" name="AdvancedMacros"></a>Advanced Macros</h2>
<p>This section will discuss many tricks you can use to define macros. Type <span class="InputCode">help local</span> for even more.</p>
<h3><a id="StoringListsofValuesWithlevelsof" name="StoringListsofValuesWithlevelsof"></a>Storing Lists of Values with levelsof</h3>
<p>The <span class="InputCode">levelsof</span> command lists the valid values of a variable and stores them in a local macro. The syntax is:</p>
<p class="InputCode">levelsof <span class="Parameter">variable</span>, local(<span class="Parameter">macro</span>)</p>
<p>It is frequently used with an <em>if</em> condition to store those values of a variable that appear in a given subsample.</p>
<p>The following example (taken from <a href="https://ssc.wisc.edu/sscc/pubs/stata_prog1.htm">Stata Programming Essentials</a>) uses <span class="InputCode">levelsof</span> and a <span class="InputCode">foreach</span> loop to run a survey-weighted regression separately for each race. This recreates the functionality of <span class="InputCode">by:</span> even though <span class="InputCode">by:</span> can't be used with <span class="InputCode">svy</span>:</p>
<p class="InputCode">levelsof race, local(races)<br/>
                  foreach race of local races {<br/>
<span class="indent3">display _newline(2) "Race=`race'"</span><br/>
<span class="indent3">svy, subpop(if race==`race'): reg income age i.education</span><br/>
}</p>
<p>Since <span class="InputCode">levelsof</span> doesn't include missing values in its list, the above code will not run a regression for observations with a missing value for <span class="InputCode">race</span>. This is different from <span class="InputCode">by:</span>, which treats observations with a missing value of the <em>by</em> variable as just another group to act on.</p>
<p>Suppose you asked individuals to list the social groups they belong to. The groups are  encoded as numbers and stored in variables <span class="InputCode">group_</span><span class="Parameter">i</span> where <span class="Parameter">i</span> is an integer. Thus if a person said she belongs to group five and group three (in that order)  her value of <span class="InputCode">group_1</span> would be <span class="InputCode">5</span> and her value of <span class="InputCode">group_2</span> would be <span class="InputCode">3</span>. (If she lists fewer groups than the maximum number allowed, she will have missing values for some <span class="InputCode">group_</span><span class="Parameter">i</span> variables.) You then want to create an indicator variable  for each group which has a value of 1 if the person said she is a member of that group and zero otherwise:</p>
<p class="InputCode">foreach groupvar of varlist group_* {<br/>
<span class="indent3">levelsof `groupvar', local(groups)</span><br/>
<span class="indent3">foreach group of local groups {</span><br/>
<span class="indent6">capture gen mem_`group'=0</span><br/>
<span class="indent6">replace mem_`group'=1 if `groupvar'==`group'</span><br/>
<span class="indent3">}</span><br/>
                  }</p>
<p>This loops over the <span class="InputCode">group_</span><span class="Parameter">i</span> variables, figures out which groups were listed in each one, loops over them, creates an indicator variable for each group, and sets the indicator to 1 if the person listed that group.</p>
<p>Note that you don't need to know ahead of time how many groups a person can list or the numbers of the groups that can appear. The <span class="InputCode">gen</span> command has <span class="InputCode">capture</span> in front of it because the indicator for a given group may have already been created: if someone listed group 5 as his first group and someone else listed group 5 as his second group, <span class="InputCode">mem_5</span> will be created when processing <span class="InputCode">group_1</span> and trying to create <span class="InputCode">mem_5</span> again when processing <span class="InputCode">group_2</span> gives an error--but you know it's an error you can ignore.</p>
<h3><a id="ExpandingVariableListswithunab" name="ExpandingVariableListswithunab"></a>Expanding Variable Lists with unab</h3>
<p>The <span class="InputCode">unab</span> command "unabbreviates"  a <em>varlist</em> ("expand" was taken) and puts the results in a macro. The syntax is:</p>
<p class="InputCode">unab <span class="Parameter">macro</span>: <span class="Parameter">varlist</span></p>
<p>For example, if you had variables <span class="InputCode">x1</span>, <span class="InputCode">x2</span> and <span class="InputCode">x3</span>, then:</p>
<p class="InputCode">unab vars: x*</p>
<p>would create a local macro called <span class="InputCode">vars</span> and place in it <span class="InputCode">x1 x2 x3</span>. This can be  useful for commands that require a list of variables but cannot use <em>varlist</em> syntax, like <span class="InputCode">reshape</span> when going from long to wide.</p>
<h3><a id="ListsofFiles" name="ListsofFiles"></a>Lists of Files</h3>
<p>You can place a list of files in a macro by sending <span class="InputCode">local</span> the output of a <span class="InputCode">dir</span> command. The syntax is:</p>
<p class="InputCode">local <span class="Parameter">macroname</span>: dir <span class="Parameter">directory</span> files "<span class="Parameter">pattern they must match</span>"</p>
<p>Here <span class="Parameter">macroname</span> is the name of the macro you want to create, <span class="Parameter">directory</span> is the directory where the files are located and <span class="Parameter">pattern they must match</span> is something like <span class="InputCode">"*"</span> for all files or <span class="InputCode">"*.dta"</span> for all Stata data sets. For example, the following would put a list of all Stata data sets in the current working directory in a macro called <span class="InputCode">datafiles</span>:</p>
<p class="InputCode">local datafiles: dir . files "*.dta"</p>
<p>One complication is that  the file names are placed in quotes (so it can handle file names with spaces in them). Thus to display them you have to use compound double quotes:</p>
<p class="InputCode">di `"`datafiles'"'</p>
<p>However, this doesn't cause problems if you want to loop over the list of files:</p>
<p class="InputCode">foreach file of local datafiles {<br/>
<span class="indent3">use `file', clear</span><br/>
<span class="indent3">// do something with the file</span><br/>
                  }</p>
<h3><a id="FormattingtheContentsofaMacro" name="FormattingtheContentsofaMacro"></a>Formatting the Contents of a Macro</h3>
<p>You can apply a format to a number before storing it in a macro by sending <span class="InputCode">local</span> the output of a <span class="InputCode">display</span> command that includes a format. For example, the following command stores the R-squared of the most recently run regression, <span class="InputCode">e(r2)</span>, in a macro called <span class="InputCode">r2</span>, but using the format <span class="InputCode">%5.4f</span> so it has four digits rather than sixteen.</p>
<p class="InputCode">local r2: display %5.4f e(r2)</p>
<p>See <a href="https://ssc.wisc.edu/sscc/pubs/4-25.htm">Including Calculated Results In Stata Graphs</a> for an example that uses this tool.</p>
<h2></h2>
<h3><a id="IncrementingandDecrementingMacros" name="IncrementingandDecrementingMacros"></a>Incrementing and Decrementing Macros</h3>
<p>Macros frequently need to be increased by one and less frequently decreased by one. You can do so with the <span class="InputCode">++</span> and <span class="InputCode">--</span> operators. These go inside the macro quotes either before or after the macro name. If they are placed before,  the macro is incremented (or decremented) and then the result placed in the command. If they are placed after, the current value of the macro is placed in the command and then the macro is incremented (or decremented). Try the following:</p>
<p class="InputCode">local x 1<br/>
                  display `++x'<br/>
                  display `x--'<br/>
                display `x'</p>
<p>You can use the increment or decrement operators in a <span class="InputCode">local</span> command to change a macro without doing anything else, but be sure to put the operator before the macro's name. For example the following does <strong>not</strong> increase <span class="InputCode">x</span>:</p>
<p class="InputCode">                local x `x++'<br/>
</p>
<p> The macro processor replaces the macro <span class="InputCode">`x++'</span> with <span class="InputCode">1</span>. It then  increases <span class="InputCode">x</span> to <span class="InputCode">2</span>, but then when Stata proper executes the command it sees <span class="InputCode">local x 1</span> and sets <span class="InputCode">x</span> back to <span class="InputCode">1</span>. The following does increase <span class="InputCode">x</span>:</p>
<p class="InputCode">                  local x `++x'<br/>
</p>
<h2><a id="BranchingIf" name="BranchingIf"></a>Branching If</h2>
<p>You're familiar with  <em>if</em> conditions at the end of commands, meaning "only carry out this command for the observations where this condition is true." This is a subsetting <em>if</em>. When <em>if</em> starts a command, it is a branching <em>if</em> and  has a very different meaning: "don't execute the following command or commands at all unless this condition is true." The syntax is for a single command is:</p>
<p class="InputCode">if <span class="Parameter">condition</span> <span class="Parameter">command</span></p>
<p>For a block of commands, it's:</p>
<p class="InputCode">if <span class="Parameter">condition</span> {<br/>
<span class="indent3"><span class="Parameter">commands<br/></span></span>
}</p>
<p>An <span class="InputCode">if</span> block can be followed by an <span class="InputCode">else</span> block, meaning "commands to be executed if the condition is not true." The <span class="InputCode">else</span> can precede another <span class="InputCode">if</span>, allowing for <span class="InputCode">else if</span> chains of any length:</p>
<p class="InputCode">if <span class="Parameter">condition1</span> {<br/>
<span class="indent3"><span class="Parameter">commands to execute if condition1 is true</span></span><br/>
}<br/>
else if <span class="Parameter">condition2</span> {<br/>
<span class="indent3"><span class="Parameter">commands to execute if condition one is false and 
	condition2 is true</span></span><br/>
}<br/>
else {<br/>
<span class="indent3"><span class="Parameter">commands to execute if both condition1 and condition2 
	are false</span></span><br/>
}</p>
<p>Consider trying to demean a  list of variables, where the list is contained in a macro called <span class="InputCode">varlist</span> which was generated elsewhere and could contain string variables:</p>
<p class="InputCode">foreach var of  local varlist {<br/>
<span class="indent3">capture confirm numeric variable `var'</span><br/>
<span class="indent3">if _rc==0 {</span><br/>
<span class="indent3"><span class="indent3">sum `var', meanonly</span></span><br/>
<span class="indent3"><span class="indent3">replace `var'=`var'-r(mean)</span></span><br/>
<span class="indent3">} </span><br/>
<span class="indent3">else display as error "`var' is not a numeric variable and cannot be demeaned."</span><br/>}</p>
<p>The command <span class="InputCode">confirm numeric variable `var'</span> checks that <span class="InputCode">`var'</span> is a numeric variable, but when preceded by <span class="InputCode">capture</span> it does not crash the program if the variable is a string. Instead, <span class="InputCode">if _rc==0 {</span> checks whether the <span class="InputCode">confirm</span> command succeeded (implying <span class="InputCode">`var'</span> is in fact numeric). If it did, the program proceeds to demean <span class="InputCode">`var'</span>. If not, the program displays an error message (<span class="InputCode">as error</span> makes the message red) and then proceeds to the next variable.</p>
<p>The condition for a branching <em>if</em> is only evaluated once, so a branching <em>if</em> is a scalar context and the rule that only the first value of a variable will be used applies. In particular, a branching <em>if</em> cannot be used for subsetting. Imagine a data set made up of observations from multiple census years where the data from 1960 is coded in a different way and thus has to be handled differently. What you should <strong>not</strong> write is something like:</p>
<p class="InputCode">if year==1960 { //don't do this!<br/>
<span class="indent3"><span class="Parameter">code 
                for handling observations from 1960</span></span><br/>
                }<br/>
                else {<br/>
<span class="indent3"><span class="Parameter">code for handling observations from other year</span></span>s<br/>
                }</p>
<p>Since this is a scalar context, the condition <span class="InputCode">year==1960</span><span class="InputCode"></span> is only evaluated once, and the only value of <span class="InputCode">year</span> Stata will look at is that of the first observation. Thus if the first observation happens to be from 1960, the entire data set will be coded as if it came from 1960. If not, the entire data set will be coded as if it came from other years. This is not a job for branching <em>if</em>, it is a job for a standard subsetting <em>if</em> at the end of each command.</p>
<p>The above code might make sense if it were embedded in a loop that processed multiple data sets, where each data set came from a single year. Then <span class="InputCode">year</span> would be the same for all the observations in a given data set, and the first observation could stand in for all of them. But in that case we'd  suggest writing something like:</p>
<p class="InputCode">if year[1]==1960 { // if  year for the first observation is 1960  this is a 1960 data set</p>
<p>Conditions for branching <em>if</em> frequently involve macros. If the macros contain text rather than numbers,  the values on both sides of the equals sign need to be placed in quotes:</p>
<p class="InputCode">foreach school in West East Memorial {<br/>
<span class="indent3">if "`school'"=="West" {</span><br/>
<span class="indent6"><span class="Parameter">commands that should only be carried out for West High School</span></span><br/>
<span class="indent3">}</span><br/>
<span class="indent3"><span class="Parameter">commands that should be carried out for all schools</span></span><br/>
                  }
                </p>
<h2><a id="WhileLoops" name="WhileLoops"></a>While Loops</h2>
<p><span class="InputCode">foreach</span> and <span class="InputCode">forvalues</span> loops repeat a block of commands a set number of times, but <span class="InputCode">while</span> loops repeat them until a given condition is no longer true. For example:</p>
<p class="InputCode">local i 1<br/>
                while `i'&lt;=5 {<br/>
<span class="indent3">display `i++'</span><br/>
                }</p>
<p>is equivalent to:</p>
<p class="InputCode"> forval i=1/5 {<br/>
<span class="indent3">display `i'</span><br/>
                }</p>
<p>Note that <span class="InputCode">i</span> is increased by 1 each time through the <span class="InputCode">while</span> loop--if you left out that step the loop would never end.</p>
<p>The following code is a more typical use of while:</p>
<p class="InputCode">local xnew 1<br/>
                  local xold 0<br/>
                  local iteration 1<br/>
                  while abs(`xnew'-`xold')&gt;.001 &amp; `iteration'&lt;100 {<br/>
<span class="indent3">local xold `xnew'</span><br/>
<span class="indent3">local xnew=`xold'-(3-`xold'^3)/(-3*`xold'^2)</span><br/>
<span class="indent3">display "Iteration: `iteration++', x: `xnew'"</span><br/>
}</p>
<p>The above uses the Newton-Raphson method to solve the equation 3-x^3=0. The algorithm proceeds until the result from the current iteration differs from that of the last iteration by less than .001, or  it completes 100 iterations. The second condition acts as a failsafe in case the algorithm does not converge. Note that the initial value of <span class="InputCode">xold</span> is not used, but if it were the same as the initial value of <span class="InputCode">xnew</span> then the <span class="InputCode">while</span> condition would be false immediately and the loop would never be executed.</p>
<h2><a id="Programs" name="Programs"></a>Programs</h2>
<p>A Stata program is a block of code which can be executed at any time by invoking the program's name. They are useful when you need to perform a task repeatedly, but not all at once. To begin defining a program, type:</p>
<p class="InputCode">program define <span class="Parameter">name</span></p>
<p>where <span class="Parameter">name</span> is replaced by the name of the program to be defined. Subsequent commands are considered part of the program, until you type</p>
<p class="InputCode">end</p>
<p>Thus a basic "Hello World" program is:</p>
<p class="InputCode">program define hello<br/>
<span class="indent3">display "Hello World"</span><br/>
                end</p>
<p>To run this program, type <span class="InputCode">hello</span>.</p>
<p>A program cannot be modified after it is defined; to change it you must first drop the existing version with <span class="InputCode">program drop</span> and then define it again. Since a do file run in an interactive session can't be  sure what's been  defined previously, it's best to <span class="InputCode">capture program drop</span> a program before you define it:</p>
<p class="InputCode">capture program drop hello<br/>
                program define hello<br/>
<span class="indent3">display "Hello World Again"</span><br/>
                  end</p>
<h3>Arguments</h3>
<p>Programs can be controlled by passing in <em>arguments</em>. An argument can be anything you can put in a macro: numbers, text, names of variables, etc. You pass arguments into a program by typing them after it's program name. Thus:</p>
<p class="InputCode">hello Russell Dimond</p>
<p>runs the <span class="InputCode">hello</span> program with two arguments: <span class="InputCode">Russell</span> and <span class="InputCode">Dimond</span>. But arguments only matter if the program does something with them--the current version of <span class="InputCode">hello</span>  will completely ignore them.</p>
<p>Programs that use arguments  should first use the <span class="InputCode">args</span> command to assign them to local macros. The command:</p>
<p class="InputCode">args fname lname</p>
<p>puts the first argument the program received in the macro <span class="InputCode">fname</span> and the second in the macro <span class="InputCode">lname</span>. You can then use those macros in subsequent commands:</p>
<p class="InputCode">capture program drop hello<br/>
program define hello<br/>
<span class="indent3">args fname lname</span><br/>
<span class="indent3">display "Hello `fname' `lname'</span>"<br/>
end
                </p>
<p>If you then type:</p>
<p class="InputCode">hello Russell Dimond</p>
<p>the output will be:</p>
<p class="InputCode">Hello Russell Dimond</p>
<p>Of course if you type:</p>
<p class="InputCode">hello Dimond Russell</p>
<p>the output will be:</p>
<p class="InputCode">Hello Dimond Russell</p>
<p>It's up to you to make sure the arguments you pass in match what the program is expecting.</p>
<p>The macro <span class="InputCode">0</span> is always defined, and contains a list of all the arguments that were passed into the program. This is useful for handling lists of unknown length. For example, you could take the code you wrote earlier for demeaning lists of variables and turn it into a program:</p>
<p class="InputCode">program define demean<br/>
<span class="indent3">foreach var of  local 0 {</span><br/><span class="indent3">
<span class="indent3">capture confirm numeric variable `var'</span></span><br/><span class="indent3">
<span class="indent3">if _rc==0 {</span></span><br/>
<span class="indent3">
<span class="indent3"><span class="indent3">sum `var', meanonly</span></span></span><br/><span class="indent3">
<span class="indent3"><span class="indent3">replace `var'=`var'-r(mean)</span></span></span><br/><span class="indent3">
<span class="indent3">} </span></span><br/><span class="indent3">
<span class="indent3">else display as error "`var' is not a numeric variable and cannot be demeaned."</span></span><br/><span class="indent3">
                }<br/>
                end</span></p>
<p>To run this program, you'd type <span class="InputCode">demean</span> and then a list of variables to be demeaned:</p>
<p class="InputCode">demean x y z</p>
<p>You might also want to look at the <span class="InputCode">syntax</span> command, which makes it relatively easy to write a program that understands standard Stata syntax, but <span class="InputCode">syntax</span> is beyond the scope of this article.</p>
<h3>Returning Values</h3>
<p>Your program can  return values in the <span class="InputCode">r()</span> or <span class="InputCode">e()</span> vectors, just like official Stata commands. This is critical if your program is intended for use with <span class="InputCode">bootstrap</span> or <span class="InputCode">simulate</span>. To do so, first declare in your <span class="InputCode">program define</span> command that the program is either <span class="InputCode">rclass</span> (puts results in the <span class="InputCode">r()</span> vector) or <span class="InputCode">eclass</span> (puts results in the <span class="InputCode">e()</span> vector):</p>
<p class="InputCode">program define myprogram, rclass</p>
<p>When you have a result to return, use the <span class="InputCode">return</span> command. The general syntax is:</p>
<p class="InputCode">return <span class="Parameter">type</span> <span class="Parameter">name</span>=<span class="Parameter">value</span></p>
<p>where <span class="Parameter">type</span> can be <span class="InputCode">scalar</span>, <span class="InputCode">local</span> or <span class="InputCode">matrix</span>, <span class="Parameter">value</span> is what you want to return, and <span class="Parameter">name</span> is what you want it to be called. As a trivial example:</p>
<p class="InputCode">return scalar x=3</p>
<p>When the program is complete, you can refer to the result as <span class="InputCode">r(</span><span class="Parameter">name</span><span class="InputCode">)</span> or <span class="InputCode">e(</span><span class="Parameter">name</span><span class="InputCode">)</span>. Thus if you've just run  a program containing the above <span class="InputCode">return</span> command, typing:</p>
<p class="InputCode">gen var=r(x)</p>
<p>creates a variable <span class="InputCode">var</span> with the value <span class="InputCode">3</span>.</p>
<p>For a more realistic example, see the last section of <a href="https://ssc.wisc.edu/sscc/pubs/4-27.htm#BootstrappingResultsYouveCalculated">Bootstrapping in Stata</a>.</p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Propensity Score Matching in Stata using teffects</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>

<!-- InstanceBeginEditable name="Text" -->
<p>For many years, the standard tool for propensity score matching in Stata has been the <span class="InputCode">psmatch2</span> command, written by Edwin Leuven and Barbara Sianesi. However, Stata 13 introduced a new <span class="InputCode">teffects</span> command for estimating treatments effects in a variety of ways, including propensity score matching. The <span class="InputCode">teffects psmatch</span> command has one very important advantage over <span class="InputCode">psmatch2</span>: it takes into account the fact that propensity scores are estimated rather than known when calculating standard errors. This often turns out to make a significant difference, and sometimes in surprising ways. We thus strongly recommend switching from <span class="InputCode">psmatch2</span> to <span class="InputCode">teffects psmatch</span>, and this article will help you make the transition.</p>
<h2><a id="AnExampleofPropensityScoreMatching" name="AnExampleofPropensityScoreMatching"></a>An Example of Propensity Score Matching</h2>
<p>Run the following command in Stata to load an example data set:                </p>
<p class="InputCode">use http://ssc.wisc.edu/sscc/pubs/files/psm</p>
<p>It consists of four variables: a treatment indicator <span class="InputCode">t</span>, covariates <span class="InputCode">x1</span> and <span class="InputCode">x2</span>, and an outcome <span class="InputCode">y</span>. This is constructed data, and the effect of the treatment is in fact a one unit increase in <span class="InputCode">y</span>. However, the probability of treatment is positively correlated with <span class="InputCode">x1</span> and <span class="InputCode">x2</span>, and both <span class="InputCode">x1</span> and <span class="InputCode">x2</span> are positively correlated with <span class="InputCode">y</span>. Thus simply comparing the mean value of <span class="InputCode">y</span> for the treated and untreated groups badly overestimates the effect of treatment:</p>
<p class="InputCode">ttest y, by(t)</p>
<p>(Regressing <span class="InputCode">y</span> on <span class="InputCode">t</span>, <span class="InputCode">x1</span>, and <span class="InputCode">x2</span> will give you a pretty good picture of the situation.)</p>
<p>The <span class="InputCode">psmatch2</span> command will give you a  much better estimate of the treatment effect:</p>
<p class="InputCode">psmatch2 t x1 x2, out(y)</p>
<pre class="InputCode">----------------------------------------------------------------------------------------
        Variable     Sample |    Treated     Controls   Difference         S.E.   T-stat
----------------------------+-----------------------------------------------------------
               y  Unmatched |  1.8910736  -.423243358   2.31431696   .109094342    21.21
                        ATT |  1.8910736   .871388246   <span class="Blue"><span class="InputCode">1.01968536</span></span>   <span class="Red"><span class="InputCode">.173034999</span></span>     5.89
----------------------------+-----------------------------------------------------------
Note: S.E. does not take into account that the propensity score is estimated.</pre>
<h2><a id="TheteffectsCommand" name="TheteffectsCommand"></a>The teffects Command</h2>
<p>You can carry out the same estimation with <span class="InputCode">teffects</span>. The basic syntax of the <span class="InputCode">teffects</span> command when used for propensity score matching is:</p>
<p class="InputCode">teffects psmatch (<span class="Parameter">outcome</span>) (<span class="Parameter">treatment</span> <span class="Parameter">covariates</span>)</p>
<p>In this case the basic command would be:</p>
<p class="InputCode">teffects psmatch (y) (t x1 x2)</p>
<p>However, the default behavior of <span class="InputCode">teffects</span> is not the same as <span class="InputCode">psmatch2</span> so we'll need to use some options to get the same results. First, <span class="InputCode">psmatch2</span> by default reports the average treatment effect on the treated (which it refers to as ATT). The <span class="InputCode">teffects</span> command by default reports the average treatment effect (ATE) but will calculate the average treatment effect on the treated (which it refers to as ATET) if  given the <span class="InputCode">atet</span> option. Second, <span class="InputCode">psmatch2</span> by default uses a probit model for the probability of treatment. The <span class="InputCode">teffects</span> command uses a logit model by default, but will use probit if the <span class="InputCode">probit</span> option is applied to the treatment equation. So to run the same model using <span class="InputCode">teffects</span> type:</p>
<p class="InputCode">teffects psmatch (y) (t x1 x2, probit), atet </p>
<pre class="InputCode">Treatment-effects estimation                    Number of obs      =      1000
Estimator      : propensity-score matching      Matches: requested =         1
Outcome model  : matching                                      min =         1
Treatment model: probit                                        max =         1
------------------------------------------------------------------------------
             |              AI Robust
           y |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
ATET         |
           t |
   (1 vs 0)  |   <span class="Blue"><span class="InputCode">1.019685</span></span>   <span class="Red"><span class="InputCode">.1227801</span></span>     8.30   0.000     .7790407     1.26033
------------------------------------------------------------------------------</pre>
<p>The average treatment effect on the treated is identical, other than being rounded at a different place. But note that <span class="InputCode">teffects</span> reports a very different standard error (we'll discuss why that is <a href="#StandardErrors">shortly</a>), plus a Z-statistic,  p-value, and  95% confidence interval rather than just a T-statistic.</p>
<p>Running <span class="InputCode">teffects</span> with the default options gives the following:</p>
<p class="InputCode">teffects psmatch (y) (t x1 x2)</p>
<pre class="InputCode">Treatment-effects estimation                    Number of obs      =      1000
Estimator      : propensity-score matching      Matches: requested =         1
Outcome model  : matching                                      min =         1
Treatment model: logit                                         max =         1
------------------------------------------------------------------------------
             |              AI Robust
           y |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
ATE          |
           t |
   (1 vs 0)  |   <span class="Blue"><span class="InputCode">1.019367</span></span>   .1164694     8.75   0.000     .7910912    1.247643
------------------------------------------------------------------------------</pre>
<p>This is equivalent to:</p>
<p class="InputCode">psmatch2 t x1 x2, out(y) logit ate</p>
<pre class="InputCode">----------------------------------------------------------------------------------------
        Variable     Sample |    Treated     Controls   Difference         S.E.   T-stat
----------------------------+-----------------------------------------------------------
               y  Unmatched |  1.8910736  -.423243358   2.31431696   .109094342    21.21
                        ATT |  1.8910736   .930722886   <span class="Red"><span class="InputCode">.960350715</span></span>   .168252917     5.71
                        ATU |-.423243358   .625587554   1.04883091            .        .
                        ATE |                           <span class="Blue"><span class="InputCode">1.01936701</span></span>            .        .
----------------------------+-----------------------------------------------------------
Note: S.E. does not take into account that the propensity score is estimated.</pre>
<p>The ATE from this model is very similar to the ATT/ATET from the previous model. But note that <span class="InputCode">psmatch2</span> is reporting a somewhat different ATT in this model. The <span class="InputCode">teffects</span> command reports the same ATET if asked:</p>
<p class="InputCode">teffects psmatch (y) (t x1 x2), atet</p>
<p></p>
<pre class="InputCode">Treatment-effects estimation                    Number of obs      =      1000
Estimator      : propensity-score matching      Matches: requested =         1
Outcome model  : matching                                      min =         1
Treatment model: logit                                         max =         1
------------------------------------------------------------------------------
             |              AI Robust
           y |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
ATET         |
           t |
   (1 vs 0)  |   <span class="Red"><span class="InputCode">.9603507</span></span>   .1204748     7.97   0.000     .7242245    1.196477
------------------------------------------------------------------------------</pre>
<h2><a id="StandardErrors" name="StandardErrors"></a>Standard Errors</h2>
<p>The output of <span class="InputCode">psmatch2</span> includes the following caveat:</p>
<p><span class="InputCode">Note: S.E. does not take into account that the propensity score is estimated.</span></p>
<p>A recent paper by Abadie and Imbens (2012. <a href="http://www.hks.harvard.edu/fs/aabadie/pscore.pdf">Matching on the estimated propensity score</a>. Harvard University and National Bureau of Economic Research) established how to take into account that propensity scores are estimated, and <span class="InputCode">teffects psmatch</span> relies on their work. Interestingly, the adjustment for ATE is always negative, leading to smaller standard errors: matching based on estimated propensity scores turns out to be more efficient than matching based on true propensity scores. However, for ATET the adjustment can be positive or negative, so the standard errors reported by <span class="InputCode">psmatch2</span> may be too large or to small.</p>
<h2> <a id="HandlingTies" name="HandlingTies"></a>Handling Ties</h2>
<p>Thus far we've used <span class="InputCode">psmatch2</span> and <span class="InputCode">teffects psmatch</span> to do simple nearest-neighbor matching with one neighbor (and no caliper). However, this raises the question of what to do when two observations have the same propensity score and are thus tied for "nearest neighbor." Ties are common if the covariates in the treatment model are categorical or even integers.</p>
<p>The <span class="InputCode">psmatch2</span> command by default matches with one of the tied observations, but with the <span class="InputCode">ties</span> option it matches with all tied observations. The <span class="InputCode">teffects psmatch</span> command always matches with all ties. If your data set has multiple observations with the same propensity score, you won't get exactly the same results from <span class="InputCode">teffects psmatch</span> as you were getting from <span class="InputCode">psmatch2</span> unless you go back and add the <span class="InputCode">ties</span> option to your <span class="InputCode">psmatch2</span> commands. (At this time we are not aware of any clear guidance as to whether it is better to match with ties or not.)</p>
<h2><a id="MatchingWithMultipleNeighbors" name="MatchingWithMultipleNeighbors"></a>Matching With Multiple Neighbors</h2>
<p>By default <span class="InputCode">teffects psmatch</span> matches each observation with one other observation. You can change this with the <span class="InputCode">nneighbor()</span> (or just <span class="InputCode">nn()</span>) option. For example, you could match each observation with its three nearest neighbors with:</p>
<p class="InputCode">teffects psmatch (y) (t x1 x2), nn(3)</p>
<h2><a id="Postestimation" name="Postestimation"></a>Postestimation</h2>
<p>By default <span class="InputCode">teffects psmatch</span> does not add any new variables to the data set. However, there are a variety of useful variables that can be created with options and post-estimation <span class="InputCode">predict</span> commands.  The following table lists the 1st and 467th observations of the example data set after some of these variables have been created. We'll refer to it as we explain the commands that created the new variables. Reviewing these variables is also a good way to make sure you understand exactly how propensity score matching works.</p>
<pre class="InputCode" style="font-size: 80%">      
      +-------------------------------------------------------------------------------------------------------+
      |        x1          x2   t          y   match1        ps0        ps1          y0         y1         te |
      |-------------------------------------------------------------------------------------------------------|
   1. |  .0152526   -1.793022   0   -1.79457      467   .9081651   .0918349    -1.79457   2.231719   4.026289 |
 467. | -2.057838    .5360286   1   2.231719      781    .907606    .092394   -.6012772   2.231719   2.832996 |
      +-------------------------------------------------------------------------------------------------------+
</pre>
<p>Start with a clean slate by typing:</p>
<p class="InputCode">                use http://ssc.wisc.edu/sscc/pubs/files/psm, replace<br/>
</p>
<p>The <span class="InputCode">gen()</span> option tells <span class="InputCode">teffects psmatch</span> to create a new variable (or variables). For each observation, this new variable will contain the number of the observation that observation was matched with. If there are ties or you told <span class="InputCode">teffects psmatch</span> to use multiple neighbors, then <span class="InputCode">gen()</span> will need to create multiple variables. Thus you supply the stem of the variable name, and <span class="InputCode">teffects psmatch</span> will add suffixes as needed.</p>
<p class="InputCode">teffects psmatch (y) (t x1 x2), gen(match)</p>
<p>In this case each observation is only matched with one other, so <span class="InputCode">gen(match</span>) only creates <span class="InputCode">match1</span>. Referring to the example output, the match of observation 1 is observation 467 (which is why those two are listed).</p>
<p>Note that these observation numbers are only valid in the current sort order, so make sure you can recreate that order if needed. If necessary, run:</p>
<p class="InputCode">gen ob=_n</p>
<p>and then:</p>
<p class="InputCode">sort ob</p>
<p>to restore the current sort order.</p>
<p>The <span class="InputCode">predict</span> command with the <span class="InputCode">ps</span> option creates two variables containing the propensity scores, or that observation's predicted probability of being in either the control group or the treated group:</p>
<p class="InputCode">predict ps0 ps1, ps</p>
<p>Here <span class="InputCode">ps0</span> is the predicted probability of being in the control group (<span class="InputCode">t=0</span>) and <span class="InputCode">ps1</span> is the predicted probability of being in the treated group (<span class="InputCode">t=1</span>). Observations 1 and 467 were matched because their propensity scores are very similar.</p>
<p>The <span class="InputCode">po</span> option creates variables containing the potential outcomes for each observation:</p>
<p class="InputCode">predict y0 y1, po</p>
<p>Because observation 1 is in the control group, <span class="InputCode">y0</span> contains its observed value of <span class="InputCode">y</span>. <span class="InputCode">y1</span> is the observed value of <span class="InputCode">y</span> for observation 1's match, observation 467. The propensity score matching estimator assumes that if observation 1 had been in the treated group its value of y would have been that of the observation in the treated group most similar to it (where "similarity" is measured by the difference in their propensity scores).</p>
<p>Observation 467 is in the treated group, so its value for <span class="InputCode">y1</span> is its observed value of <span class="InputCode">y</span> while its value for <span class="InputCode">y0</span> is the observed value of <span class="InputCode">y</span> for its match, observation 781.</p>
<p>Running the predict command with no options gives the treatment effect itself:</p>
<p class="InputCode">predict te</p>
<p>The treatment effect is simply the difference between <span class="InputCode">y1</span> and <span class="InputCode">y0</span>. You could calculate the ATE yourself (but emphatically not its standard error) with:</p>
<p class="InputCode">sum te</p>
<p>and the ATET with:</p>
<p class="InputCode">sum te if t</p>
<h2>Regression on the "Matched Sample"</h2>
<p>Another way to conceptualize propensity score matching is to think of it as choosing a sample from the control group that "matches" the treatment group. Any differences between the treatment and matched control groups are then assumed to be a result of the treatment. Note that this gives the average treatment effect on the treated—to calculate the ATE you'd  create a sample of the treated group that matches the controls. Mathematically this is all equivalent to using matching to estimate what an observation's outcome would have been if it had been in the other group, as described above.</p>
<p>Sometimes researchers then want to run regressions on the "matched sample," defined as the observations in the treated group plus the observations in the control group which were matched to them. The problem with this approach is that the matched sample is based on propensity scores which are estimated, not known. Thus the matching scheme is an estimate as well. Running regressions after matching is essentially a two stage regression model, and the standard errors from the second stage must take the first stage into account, something standard regression commands do not do. This is an area of ongoing research.</p>
<p>We will discuss how to run regressions on a matched sample because it remains a popular technique, but we cannot recommend it.</p>
<p><span class="InputCode">psmatch2</span> makes it easy by creating a <span class="InputCode">_weight</span> variable automatically. For observations in the treated group, <span class="InputCode">_weight</span> is 1. For observations in the control group it is the number of observations from the treated group for which the observation is a match. If the observation is not a match, <span class="InputCode">_weight</span> is missing. <span class="InputCode">_weight</span> thus acts as a frequency weight (<span class="InputCode">fweight</span>) and can be used with Stata's standard weighting syntax. For example (starting with a clean slate again):</p>
<p class="InputCode">use http://ssc.wisc.edu/sscc/pubs/files/psm, replace<br/>
				psmatch2 t x1 x2, out(y) logit<br>
                  reg y x1 x2 t [fweight=_weight]                  <br/>
</br></p>
<p>Observations with a missing value for <span class="InputCode">_weight</span> are omitted from the regression, so it is automatically limited to the matched sample. Again, keep in mind that the standard errors given by the <span class="InputCode">reg</span> command are incorrect because they do not take into account the matching stage.</p>
<p><span class="InputCode">teffects psmatch</span> does not create a <span class="InputCode">_weight</span> variable, but it is possible to create one based on the <span class="InputCode">match1</span> variable. Here is example code, with comments:</p>
<p class="InputCode">gen ob=_n //store the observation numbers for future use<br/>
                save fulldata,replace // save the complete data set<br/>
<br/>
                keep if t // keep just the treated group<br/>
                keep match1 // keep just the match1 variable (the observation numbers of their matches)<br/>
                bysort match1: gen weight=_N // count how many times each control observation is a match<br/>
                by match1: keep if _n==1 // keep just one row per control observation<br/>
                ren match1 ob //rename for merging purposes<br/>
<br/>
                merge 1:m ob using fulldata // merge back into the full data<br/>
                replace weight=1 if t // set weight to 1 for treated observations</p>
<p>The resulting <span class="InputCode">weight</span> variable will be identical to the <span class="InputCode">_weight</span> variable created by <span class="InputCode">psmatch2</span>, as can be verified with:</p>
<p class="InputCode">assert weight==_weight</p>
<p>It is used in the same way and will give exactly the same results:</p>
<p class="InputCode">reg y x1 x2 t [fweight=weight]</p>
<p>Obviously this is a good bit more work than using <span class="InputCode">psmatch2</span>. If your propensity score matching model can be done using both <span class="InputCode">teffects psmatch</span> and <span class="InputCode">psmatch2</span>, you may want to run <span class="InputCode">teffects psmatch</span> to get the correct standard error and then <span class="InputCode">psmatch2</span> if you need a <span class="InputCode">_weight</span> variable.</p>
<p>This regression has an N of 666, 333 from the treated group and 333 from the control group. However, it only uses 189 different observations from the control group. About 1/3 of them are the matches for more than one observation from the treated group and are thus duplicated in the regression (run <span class="InputCode">tab weight if !t</span> for details). Researchers sometimes use the <span class="InputCode">norepl</span> (no replacement) option in <span class="InputCode">psmatch2</span> to ensure each observation is  used just once, even though this generally makes the matching worse. To the best of our knowledge there is no equivalent with <span class="InputCode">teffects psmatch</span>.</p>
<p>The results of this regression leave somewhat to be desired:</p>
<pre class="InputCode">------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          x1 |    1.11891   .0440323    25.41   0.000      1.03245    1.205369
          x2 |    1.05594   .0417253    25.31   0.000       .97401     1.13787
           t |   .9563751   .0802273    11.92   0.000     .7988445    1.113906
       _cons |   .0180986   .0632538     0.29   0.775    -.1061036    .1423008
------------------------------------------------------------------------------</pre>
<p>By construction all the coefficients should be 1. Regression using all the observations (<span class="InputCode">reg y x1 x2 t</span> rather than <span class="InputCode">reg y x1 x2 t [fweight=weight]</span>) does better in this case:</p>
<pre class="InputCode">------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          x1 |   1.031167   .0346941    29.72   0.000     .9630853    1.099249
          x2 |   .9927759   .0333297    29.79   0.000     .9273715     1.05818
           t |   .9791484   .0769067    12.73   0.000     .8282306    1.130066
       _cons |   .0591595   .0416008     1.42   0.155    -.0224758    .1407948
------------------------------------------------------------------------------</pre>
<h2>Other Methods of Estimating Treatment Effects</h2>
<p>While propensity score matching is the most common method of estimating treatment effects at the SSCC, <span class="InputCode">teffects</span> also implements Regression Adjustment (<span class="InputCode">teffects ra</span>), Inverse Probability Weighting (<span class="InputCode">teffects ipw</span>), Augmented Inverse Probability Weighting (<span class="InputCode">teffects aipw</span>), Inverse Probability Weighted Regression Adjustment <span class="InputCode">(teffects ipwra</span>), and Nearest Neighbor Matching (<span class="InputCode">teffects nnmatch</span>). The syntax is similar, though it varies whether you need to specify variables for the outcome model, the treatment model, or both:</p>
<p class="InputCode">teffects ra (y x1 x2) (t)<br/>
teffects ipw (y) (t x1 x2)<br/>
teffects aipw (y x1 x2) (t x1 x2)<br/>
teffects ipwra (y x1 x2) (t x1 x2)<br/>
teffects nnmatch (y x1 x2) (t)</p>
<h2><a id="CompleteExampleCode" name="CompleteExampleCode"></a>Complete Example Code</h2>
<p>The following is the complete code for the examples in this article.</p>
<p class="InputCode">clear all<br/>
use http://www.ssc.wisc.edu/sscc/pubs/files/psm<br/>
<br/>


ttest y, by(t)<br/>

reg y x1 x2 t<br/>
<br/>


psmatch2 t x1 x2, out(y)<br/>

teffects psmatch (y) (t x1 x2, probit), atet <br/>
<br/>


teffects psmatch (y) (t x1 x2)<br/>

psmatch2 t x1 x2, out(y) logit ate<br/>

teffects psmatch (y) (t x1 x2), atet<br/>
<br/>


use http://www.ssc.wisc.edu/sscc/pubs/files/psm, replace<br/>
<br/>


teffects psmatch (y) (t x1 x2), gen(match)<br/>
<br/>


predict ps0 ps1, ps<br/>

predict y0 y1, po<br/>

predict te<br/>

l if _n==1 | _n==467<br/>
<br/>


use http://www.ssc.wisc.edu/sscc/pubs/files/psm, replace<br/>
<br/>


psmatch2 t x1 x2, out(y) logit<br/>

reg y x1 x2 t [fweight=_weight]<br/>
<br/>


gen ob=_n<br/>

save fulldata,replace<br/>
<br/>


teffects psmatch (y) (t x1 x2), gen(match)<br/>

keep if t<br/>

keep match1<br/>

bysort match1: gen weight=_N<br/>

by match1: keep if _n==1<br/>

ren match1 ob<br/>
<br/>


merge 1:m ob using fulldata<br/>

replace weight=1 if t<br/>
<br/>


assert weight==_weight<br/>
<br/>


reg y x1 x2 t [fweight=weight]<br/>

reg y x1 x2 t<br/>
<br/>


teffects ra (y x1 x2) (t)<br/>

teffects ipw (y) (t x1 x2)<br/>

teffects aipw (y x1 x2) (t x1 x2)<br/>

teffects ipwra (y x1 x2) (t x1 x2)<br/>

teffects nnmatch (y x1 x2) (t) <br/>
</p>
<p></p>
<p></p>
<p></p>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
<kb_document>
<kb_title>Search Stata's Online Help</kb_title>
<kb_keywords>.</kb_keywords>
<kb_summary>.</kb_summary>
<kb_body>
<!-- InstanceBeginEditable name="Content" -->
<form action="http://stata.com/search.cgi" id="form1" method="get" name="form1">
<p>
<label>Command:
                        <input id="query" name="query" type="text"/>
<br/>
<input id="button" name="button" type="submit" value="Submit"/>
<br/>
</label>
</p>
</form>
<!-- InstanceEndEditable -->

</kb_body>
<img_base_url></img_base_url>
</kb_document>
</kb_documents>